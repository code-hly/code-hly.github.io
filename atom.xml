<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>龙场驿站</title>
  
  <subtitle>无善无恶心之体，有善有恶意之动；知善知恶是良知，为善去恶是格物。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-11-16T04:52:45.811Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>建木</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>分布式算法6-一致性协议之3PC</title>
    <link href="http://yoursite.com/2019/10/01/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%956-%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%B9%8B3PC/"/>
    <id>http://yoursite.com/2019/10/01/分布式算法6-一致性协议之3PC/</id>
    <published>2019-10-01T09:11:02.000Z</published>
    <updated>2020-11-16T04:52:45.811Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>​    我们说为了实现 <code>BASE</code> 理论，需要在可用性和一致性之间找到一个合适的一致性理论，于是，我们在上篇文章中了解了 <code>2PC</code> 理论，也就是两阶段提交，二阶段提交原理简单，实现方便，但是缺点则是同步阻塞，单点问题，数据不一致，过于保守。</p><p>​    而为了弥补二阶段提交的缺点，研究者们在他的基础上，提出了三阶段提交。</p><a id="more"></a><h2 id="什么是三阶段提交"><a href="#什么是三阶段提交" class="headerlink" title="什么是三阶段提交"></a>什么是三阶段提交</h2><p>​    <code>3PC</code>，全称 “<code>three phase commit</code>”，是 <code>2PC</code> 的改进版，其将 <code>2PC</code> 的 “提交事务请求” 过程一分为二。</p><p>​    回忆一下 <code>2PC</code> 的过程：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185827.png" alt="image.png"></p><p>也就是说，<code>3PC</code> 将阶段一 “提交事务请求” 分成了<code>2</code>部分，总共形成了 <code>3</code> 个部分：</p><ol><li><code>CanCommit</code></li><li><code>PreCommit</code></li><li><code>doCommit</code></li></ol><p>如下图所示：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185816.png" alt="3PC"></p><h2 id="阶段一：CanCommit"><a href="#阶段一：CanCommit" class="headerlink" title="阶段一：CanCommit"></a>阶段一：<code>CanCommit</code></h2><h3 id="事务询问"><a href="#事务询问" class="headerlink" title="事务询问"></a>事务询问</h3><p>​    协调者向所有的参与者发送一个包含事务内容的 <code>canCommit</code> 请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应。</p><h3 id="各参与者向协调者反馈事务询问的响应"><a href="#各参与者向协调者反馈事务询问的响应" class="headerlink" title="各参与者向协调者反馈事务询问的响应"></a>各参与者向协调者反馈事务询问的响应</h3><p>​    参与者接收来自协调者的 <code>canCommit</code> 请求，如果参与者认为自己可以顺利执行事务，就返回 <code>Yes</code>，否则反馈 <code>No</code> 响应。</p><h2 id="阶段-二：PreCommit"><a href="#阶段-二：PreCommit" class="headerlink" title="阶段 二：PreCommit"></a>阶段 二：<code>PreCommit</code></h2><p>​    协调者在得到所有参与者的响应之后，会根据结果执行<code>2</code>种操作：<strong>执行事务预提交</strong>，或者<strong>中断事务</strong>。</p><h3 id="1-执行事务预提交"><a href="#1-执行事务预提交" class="headerlink" title="1. 执行事务预提交"></a>1. 执行事务预提交</h3><h4 id="发送预提交请求："><a href="#发送预提交请求：" class="headerlink" title="发送预提交请求："></a>发送预提交请求：</h4><p>​    协调者向所有参与者节点发出 <code>preCommit</code> 的请求，并进入 <code>prepared</code> 状态。</p><h4 id="事务预提交："><a href="#事务预提交：" class="headerlink" title="事务预提交："></a>事务预提交：</h4><p>​    参与者受到 <code>preCommit</code> 请求后，会执行事务操作，对应 <code>2PC</code> 中的 “执行事务”，也会 <code>Undo</code> 和 <code>Redo</code> 信息记录到事务日志中。</p><h4 id="各参与者向协调者反馈事务执行的结果："><a href="#各参与者向协调者反馈事务执行的结果：" class="headerlink" title="各参与者向协调者反馈事务执行的结果："></a>各参与者向协调者反馈事务执行的结果：</h4><p>​    如果参与者成功执行了事务，就反馈 <code>Ack</code> 响应，同时等待指令：提交（<code>commit</code>） 或终止（<code>abort</code>）。</p><h3 id="2-中断事务"><a href="#2-中断事务" class="headerlink" title="2. 中断事务"></a>2. 中断事务</h3><h4 id="发送中断请求"><a href="#发送中断请求" class="headerlink" title="发送中断请求"></a>发送中断请求</h4><p>​    协调者向所有参与者节点发出 <code>abort</code> 请求 。</p><h4 id="中断事务"><a href="#中断事务" class="headerlink" title="中断事务"></a>中断事务</h4><p>​    参与者如果收到 <code>abort</code> 请求或者超时了，都会中断事务。</p><h2 id="阶段三：doCommit"><a href="#阶段三：doCommit" class="headerlink" title="阶段三：doCommit"></a>阶段三：<code>doCommit</code></h2><p>​    该阶段做真正的提交，同样也会出现两种情况：</p><h3 id="1-执行提交"><a href="#1-执行提交" class="headerlink" title="1. 执行提交"></a>1. 执行提交</h3><h4 id="发送提交请求"><a href="#发送提交请求" class="headerlink" title="发送提交请求"></a>发送提交请求</h4><p>​    进入这一阶段，如果协调者正常工作，并且接收到了所有协调者的 <code>Ack</code> 响应，那么协调者将从 “预提交” 状态变为 “提交” 状态，并向所有的参与者发送 <code>doCommit</code> 请求 。</p><h4 id="事务提交"><a href="#事务提交" class="headerlink" title="事务提交"></a>事务提交</h4><p>​    参与者收到 <code>doCommit</code> 请求后，会正式执行事务提交操作，并在完成之后释放在整个事务执行期间占用的事务资源。</p><h4 id="反馈事务提交结果"><a href="#反馈事务提交结果" class="headerlink" title="反馈事务提交结果"></a>反馈事务提交结果</h4><p>​    参与者完成事务提交后，向协调者发送 <code>Ack</code> 消息。</p><h4 id="完成事务"><a href="#完成事务" class="headerlink" title="完成事务"></a>完成事务</h4><p>​    协调者接收到所有参与者反馈的 <code>Ack</code> 消息后，完成事务。</p><h3 id="2-中断事务-1"><a href="#2-中断事务-1" class="headerlink" title="2. 中断事务"></a>2. 中断事务</h3><p>​    假设有任何参与者反馈了 <code>no</code>  响应，或者超时了，就中断事务。</p><h4 id="发送中断请求-1"><a href="#发送中断请求-1" class="headerlink" title="发送中断请求"></a>发送中断请求</h4><p>​    协调者向所有的参与者节点发送 <code>abort</code> 请求。</p><h4 id="事务回滚"><a href="#事务回滚" class="headerlink" title="事务回滚"></a>事务回滚</h4><p>​    参与者接收到 <code>abort</code> 请求后，会利用其在二阶段记录的 <code>undo</code> 信息来执行事务回滚操作，并在完成回滚之后释放整个事务执行期间占用的资源。</p><h4 id="反馈事务回滚结果"><a href="#反馈事务回滚结果" class="headerlink" title="反馈事务回滚结果"></a>反馈事务回滚结果</h4><p>​    参与者在完成事务回滚之后，想协调者发送 <code>Ack</code> 消息。</p><h4 id="中断事务-1"><a href="#中断事务-1" class="headerlink" title="中断事务"></a>中断事务</h4><p>​    协调者接收到所有的 Ack 消息后，中断事务。</p><blockquote><p> 注意：一旦进入阶段三，可能会出现 <code>2</code> 种故障：</p><ol><li><strong>协调者出现问题</strong></li><li><strong>协调者和参与者之间的网络故障</strong></li></ol><p>一段出现了任意一种情况，最终都会导致参与者无法收到 <code>doCommit</code> 请求或者 <code>abort</code> 请求，针对这种情况，参与者都会在等待超时之后，继续进行事务提交。</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>​    相比较 <code>2PC</code>，最大的优点是减少了参与者的阻塞范围（第一个阶段是不阻塞的），并且能够在单点故障后继续达成一致（<code>2PC</code> 在提交阶段会出现此问题，而 <code>3PC</code> 会根据协调者的状态进行回滚或者提交）。</p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>​    如果参与者收到了 <code>preCommit</code> 消息后，出现了网络分区，那么参与者等待超时后，都会进行事务的提交，这必然会出现事务不一致的问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;​    我们说为了实现 &lt;code&gt;BASE&lt;/code&gt; 理论，需要在可用性和一致性之间找到一个合适的一致性理论，于是，我们在上篇文章中了解了 &lt;code&gt;2PC&lt;/code&gt; 理论，也就是两阶段提交，二阶段提交原理简单，实现方便，但是缺点则是同步阻塞，单点问题，数据不一致，过于保守。&lt;/p&gt;
&lt;p&gt;​    而为了弥补二阶段提交的缺点，研究者们在他的基础上，提出了三阶段提交。&lt;/p&gt;
    
    </summary>
    
      <category term="分布式算法" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="分布式算法" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>分布式算法5-一致性协议之TCC</title>
    <link href="http://yoursite.com/2019/10/01/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%955-%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%B9%8BTCC/"/>
    <id>http://yoursite.com/2019/10/01/分布式算法5-一致性协议之TCC/</id>
    <published>2019-10-01T09:11:02.000Z</published>
    <updated>2020-11-16T04:52:41.933Z</updated>
    
    <content type="html"><![CDATA[<p>之前网上看到很多写分布式事务的文章，不过大多都是将分布式事务各种技术方案简单介绍一下。很多朋友看了还是不知道分布式事务到底怎么回事，在项目里到底如何使用。</p><p>所以这篇文章，就用大白话+手工绘图，并结合一个电商系统的案例实践，来给大家讲清楚到底什么是 <code>TCC</code> 分布式事务。</p><a id="more"></a><h1 id="TCC原理（Try-Confirm-Cancel）"><a href="#TCC原理（Try-Confirm-Cancel）" class="headerlink" title="TCC原理（Try-Confirm-Cancel）"></a><code>TCC</code>原理（<code>Try-Confirm-Cancel</code>）</h1><p>​    <code>TCC</code>将事务提交分为<code>Try-Confirm-Cancel 3</code>个操作。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Try：预留业务资源/数据效验</span><br><span class="line">Confirm：确认执行业务操作</span><br><span class="line">Cancel：取消执行业务操作</span><br></pre></td></tr></table></figure><p>​    <code>TCC</code>本质上是补偿事务，它的核心思想是<strong>针对每个操作都要注册一个与其对应的确认操作和补偿操作（也就是撤销操作）。</strong> 它是一个业务层面的协议，你也可以将<code>TCC</code>理解为编程模型，<code>TCC</code>的<code>3</code>个操作是需要在业务代码中编码实现的，为了实现一致性，确认操作和补偿操作必须是等幂的，因为这<code>2</code> 个操作可能会失败重试。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20181226111719813.png" style="zoom:67%;"></p><p>​    <code>TCC</code>不依赖于数据库的事务，而是在业务中实现了分布式事务，这样能减轻数据库的压力，但对业务代码的入侵性也更强，实现的复杂度也更高。所以，我推荐在需要分布式事务能力时，优先考虑现成的事务型数据库（比如 <code>MySQL XA</code>），当现有的事务型数据库不能满足业务的需求时，再考虑基于<code>TCC</code>实现分布式事务。</p><h2 id="TCC优缺点"><a href="#TCC优缺点" class="headerlink" title="TCC优缺点"></a><code>TCC</code>优缺点</h2><h3 id="TCC优点"><a href="#TCC优点" class="headerlink" title="TCC优点"></a><code>TCC</code>优点</h3><p>​    让应用自己定义数据库操作的粒度，使得降低锁冲突、提高吞吐量成为可能。</p><h3 id="TCC不足之处"><a href="#TCC不足之处" class="headerlink" title="TCC不足之处"></a><code>TCC</code>不足之处</h3><ul><li>对应用的侵入性强。业务逻辑的每个分支都需要实现<code>try</code>、<code>confirm</code>、<code>cancel</code>三个操作，应用侵入性较强，改造成本高。</li><li>实现难度较大。需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。为了满足一致性的要求，<code>confirm</code>和<code>cancel</code>接口必须实现幂等。</li></ul><h1 id="业务场景介绍"><a href="#业务场景介绍" class="headerlink" title="业务场景介绍"></a>业务场景介绍</h1><p>​    咱们先来看看业务场景，假设你现在有一个电商系统，里面有一个支付订单的场景。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185635.png" alt="img"></p><p>​    那对一个订单支付之后，我们需要做下面的步骤：</p><ul><li>更改订单的状态为“已支付”</li><li>扣减商品库存</li><li>给会员增加积分</li><li>创建销售出库单通知仓库发货</li></ul><p>这是一系列比较真实的步骤，无论大家有没有做过电商系统，应该都能理解。</p><h1 id="进一步思考"><a href="#进一步思考" class="headerlink" title="进一步思考"></a>进一步思考</h1><p>​    好，业务场景有了，现在我们要更进一步，实现一个 <code>TCC</code> 分布式事务的效果。</p><p>​    什么意思呢？也就是说，[1] 订单服务-修改订单状态，[2] 库存服务-扣减库存，[3] 积分服务-增加积分，[4] 仓储服务-创建销售出库单。</p><p>​    上述这几个步骤，要么一起成功，要么一起失败，必须是一个整体性的事务。</p><p>​    举个例子，现在订单的状态都修改为“已支付”了，结果库存服务扣减库存失败。那个商品的库存原来是 <code>100</code> 件，现在卖掉了 <code>2</code> 件，本来应该是 <code>98</code> 件了。</p><p>​    结果呢？由于库存服务操作数据库异常，导致库存数量还是 <code>100</code>。这不是在坑人么，当然不能允许这种情况发生了！</p><p>​    但是如果你不用 <code>TCC</code> 分布式事务方案的话，就用个 <code>Spring Cloud</code> 开发这么一个微服务系统，很有可能会干出这种事儿来。</p><p>​    我们来看看下面的这个图，直观的表达了上述的过程：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185641.png" alt="img"></p><p>​    所以说，我们有必要使用 <code>TCC</code> 分布式事务机制来保证各个服务形成一个整体性的事务。</p><p>​    上面那几个步骤，要么全部成功，如果任何一个服务的操作失败了，就全部一起回滚，撤销已经完成的操作。</p><p>​    比如说库存服务要是扣减库存失败了，那么订单服务就得撤销那个修改订单状态的操作，然后得停止执行增加积分和通知出库两个操作。</p><p>​    说了那么多，老规矩，给大家上一张图，大伙儿顺着图来直观的感受一下：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185646.png" alt="img"></p><h1 id="落地实现-TCC-分布式事务"><a href="#落地实现-TCC-分布式事务" class="headerlink" title="落地实现 TCC 分布式事务"></a>落地实现 <code>TCC</code> 分布式事务</h1><p>​    那么现在到底要如何来实现一个 <code>TCC</code> 分布式事务，使得各个服务，要么一起成功？要么一起失败呢？</p><p>​    大家稍安勿躁，我们这就来一步一步的分析一下。咱们就以一个 <code>Spring Cloud</code> 开发系统作为背景来解释。</p><h2 id="TCC-实现阶段一：Try"><a href="#TCC-实现阶段一：Try" class="headerlink" title="TCC 实现阶段一：Try"></a><code>TCC</code> 实现阶段一：<code>Try</code></h2><p>​    首先，订单服务那儿，它的代码大致来说应该是这样子的：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderService</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 库存服务</span></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> InventoryService inventoryService;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 积分服务</span></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> CreditService creditService;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 仓储服务</span></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> WmsService wmsService;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对这个订单完成支付</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">pay</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="comment">//对本地的的订单数据库修改订单状态为"已支付"</span></span><br><span class="line">        orderDAO.updateStatus(OrderStatus.PAYED);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//调用库存服务扣减库存</span></span><br><span class="line">        inventoryService.reduceStock();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//调用积分服务增加积分</span></span><br><span class="line">        creditService.addCredit();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//调用仓储服务通知发货</span></span><br><span class="line">        wmsService.saleDelivery();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    如果你之前看过 <code>Spring Cloud</code> 架构原理那篇文章，同时对 <code>Spring Cloud</code> 有一定的了解的话，应该是可以理解上面那段代码的。</p><p>​    其实就是订单服务完成本地数据库操作之后，通过 <code>Spring Cloud</code> 的 <code>Feign</code> 来调用其他的各个服务罢了。</p><p>​    但是光是凭借这段代码，是不足以实现 <code>TCC</code> 分布式事务的啊？！兄弟们，别着急，我们对这个订单服务修改点儿代码好不好。</p><p>​    首先，上面那个订单服务先把自己的状态修改为：<code>OrderStatus.UPDATING</code>。</p><p>​    这是啥意思呢？也就是说，在 <code>pay()</code> 那个方法里，你别直接把订单状态修改为已支付啊！你先把订单状态修改为 <code>UPDATING</code>，也就是修改中的意思。</p><p>​    这个状态是个没有任何含义的这么一个状态，代表有人正在修改这个状态罢了。</p><p>​    然后呢，库存服务直接提供的那个 <code>reduceStock()</code> 接口里，也别直接扣减库存啊，你可以是冻结掉库存。</p><p>​    举个例子，本来你的库存数量是 <code>100</code>，你别直接 <code>100 - 2 = 98</code>，扣减这个库存！</p><p>​    你可以把可销售的库存：<code>100 - 2 = 98</code>，设置为 <code>98</code> 没问题，然后在一个单独的冻结库存的字段里，设置一个 <code>2</code>。也就是说，有 <code>2</code> 个库存是给冻结了。</p><p>​    积分服务的 <code>addCredit()</code> 接口也是同理，别直接给用户增加会员积分。你可以先在积分表里的一个预增加积分字段加入积分。</p><p>​    比如：用户积分原本是 <code>1190</code>，现在要增加 <code>10</code> 个积分，别直接 <code>1190 + 10 = 1200</code> 个积分啊！</p><p>​    你可以保持积分为 <code>1190</code> 不变，在一个预增加字段里，比如说 <code>prepare_add_credit</code> 字段，设置一个 <code>10</code>，表示有 <code>10</code> 个积分准备增加。</p><p>​    仓储服务的 <code>saleDelivery()</code> 接口也是同理啊，你可以先创建一个销售出库单，但是这个销售出库单的状态是“<code>UNKNOWN</code>”。</p><p>​    也就是说，刚刚创建这个销售出库单，此时还不确定它的状态是什么呢！</p><p>​    上面这套改造接口的过程，其实就是所谓的 <code>TCC</code> 分布式事务中的第一个 <code>T</code> 字母代表的阶段，也就是 <code>Try</code> 阶段。</p><p>​    总结上述过程，如果你要实现一个 <code>TCC</code> 分布式事务，首先你的业务的主流程以及各个接口提供的业务含义，不是说直接完成那个业务操作，而是完成一个 <code>Try</code> 的操作。</p><p>​    这个操作，一般都是锁定某个资源，设置一个预备类的状态，冻结部分数据，等等，大概都是这类操作。</p><p>​    咱们来一起看看下面这张图，结合上面的文字，再来捋一捋整个过程：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185623.png" alt="img"></p><h2 id="TCC-实现阶段二：Confirm"><a href="#TCC-实现阶段二：Confirm" class="headerlink" title="TCC 实现阶段二：Confirm"></a><code>TCC</code> 实现阶段二：<code>Confirm</code></h2><p>​    然后就分成两种情况了，第一种情况是比较理想的，那就是各个服务执行自己的那个 <code>Try</code> 操作，都执行成功了，<code>Bingo</code>！</p><p>​    这个时候，就需要依靠 <code>TCC</code> 分布式事务框架来推动后续的执行了。这里简单提一句，如果你要玩儿 <code>TCC</code> 分布式事务，必须引入一款 <code>TCC</code> 分布式事务框架，比如国内开源的 <code>ByteTCC</code>、<code>Himly</code>、<code>TCC-transaction</code>。</p><p>​    否则的话，感知各个阶段的执行情况以及推进执行下一个阶段的这些事情，不太可能自己手写实现，太复杂了。</p><p>​    如果你在各个服务里引入了一个 <code>TCC</code> 分布式事务的框架，订单服务里内嵌的那个 <code>TCC</code> 分布式事务框架可以感知到，各个服务的 <code>Try</code> 操作都成功了。</p><p>​    此时，<code>TCC</code> 分布式事务框架会控制进入 <code>TCC</code> 下一个阶段，第一个 <code>C</code> 阶段，也就是 <code>Confirm</code> 阶段。</p><p>​    为了实现这个阶段，你需要在各个服务里再加入一些代码。比如说，订单服务里，你可以加入一个 <code>Confirm</code> 的逻辑，就是正式把订单的状态设置为“已支付”了，大概是类似下面这样子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderServiceConfirm</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">pay</span><span class="params">()</span></span>&#123;</span><br><span class="line">        orderDao.updateStatus(OrderStatus.PAYED);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    库存服务也是类似的，你可以有一个 <code>InventoryServiceConfirm</code> 类，里面提供一个 <code>reduceStock()</code> 接口的 <code>Confirm</code> 逻辑，这里就是将之前冻结库存字段的 <code>2</code> 个库存扣掉变为 <code>0</code>。</p><p>​    这样的话，可销售库存之前就已经变为 <code>98</code> 了，现在冻结的 <code>2</code> 个库存也没了，那就正式完成了库存的扣减。</p><p>​    积分服务也是类似的，可以在积分服务里提供一个 <code>CreditServiceConfirm</code> 类，里面有一个 <code>addCredit()</code> 接口的 <code>Confirm</code> 逻辑，就是将预增加字段的 <code>10</code> 个积分扣掉，然后加入实际的会员积分字段中，从 <code>1190</code> 变为 <code>1120</code>。</p><p>​    仓储服务也是类似，可以在仓储服务中提供一个 <code>WmsServiceConfirm</code> 类，提供一个 <code>saleDelivery()</code> 接口的  <code>Confirm</code> 逻辑，将销售出库单的状态正式修改为“已创建”，可以供仓储管理人员查看和使用，而不是停留在之前的中间状态“<code>UNKNOWN</code>”了。</p><p>​    好了，上面各种服务的 <code>Confirm</code> 的逻辑都实现好了，一旦订单服务里面的 <code>TCC</code> 分布式事务框架感知到各个服务的 <code>Try</code> 阶段都成功了以后，就会执行各个服务的 <code>Confirm</code> 逻辑。</p><p>​    订单服务内的 <code>TCC</code> 事务框架会负责跟其他各个服务内的 <code>TCC</code> 事务框架进行通信，依次调用各个服务的 <code>Confirm</code> 逻辑。然后，正式完成各个服务的所有业务逻辑的执行。</p><p>​    同样，给大家来一张图，顺着图一起来看看整个过程：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185653.png" alt="img"></p><h2 id="TCC-实现阶段三：Cancel"><a href="#TCC-实现阶段三：Cancel" class="headerlink" title="TCC 实现阶段三：Cancel"></a><code>TCC</code> 实现阶段三：<code>Cancel</code></h2><p>​    好，这是比较正常的一种情况，那如果是异常的一种情况呢？</p><p>​    举个例子：在 <code>Try</code> 阶段，比如积分服务吧，它执行出错了，此时会怎么样？</p><p>​    那订单服务内的 <code>TCC</code> 事务框架是可以感知到的，然后它会决定对整个 <code>TCC</code> 分布式事务进行回滚。</p><p>​    也就是说，会执行各个服务的第二个 <code>C</code> 阶段，<code>Cancel</code> 阶段。同样，为了实现这个 <code>Cancel</code> 阶段，各个服务还得加一些代码。</p><p>​    首先订单服务，它得提供一个 <code>OrderServiceCancel</code> 的类，在里面有一个 <code>pay()</code> 接口的 <code>Cancel</code> 逻辑，就是可以将订单的状态设置为“<code>CANCELED</code>”，也就是这个订单的状态是已取消。</p><p>​    库存服务也是同理，可以提供 <code>reduceStock()</code> 的 <code>Cancel</code> 逻辑，就是将冻结库存扣减掉 <code>2</code>，加回到可销售库存里去，<code>98 + 2 = 100</code>。</p><p>​    积分服务也需要提供 <code>addCredit()</code> 接口的 <code>Cancel</code> 逻辑，将预增加积分字段的 <code>10</code> 个积分扣减掉。</p><p>​    仓储服务也需要提供一个 <code>saleDelivery()</code> 接口的 <code>Cancel</code> 逻辑，将销售出库单的状态修改为“<code>CANCELED</code>”设置为已取消。</p><p>​    然后这个时候，订单服务的 <code>TCC</code> 分布式事务框架只要感知到了任何一个服务的 <code>Try</code> 逻辑失败了，就会跟各个服务内的 <code>TCC</code> 分布式事务框架进行通信，然后调用各个服务的 <code>Cancel</code> 逻辑。</p><p>​    大家看看下面的图，直观的感受一下：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185707.png" alt="img"></p><h2 id="总结与思考"><a href="#总结与思考" class="headerlink" title="总结与思考"></a>总结与思考</h2><p>​    好了，兄弟们，聊到这儿，基本上大家应该都知道 <code>TCC</code> 分布式事务具体是怎么回事了！</p><p>​    总结一下，你要玩儿 <code>TCC</code> 分布式事务的话：首先需要选择某种 <code>TCC</code> 分布式事务框架，各个服务里就会有这个 <code>TCC</code> 分布式事务框架在运行。</p><p>​    然后你原本的一个接口，要改造为 <code>3</code> 个逻辑，<code>Try-Confirm-Cancel</code>：</p><ul><li>先是服务调用链路依次执行 <code>Try</code> 逻辑。</li><li>如果都正常的话，<code>TCC</code> 分布式事务框架推进执行 <code>Confirm</code> 逻辑，完成整个事务。</li><li>如果某个服务的 <code>Try</code> 逻辑有问题，<code>TCC</code> 分布式事务框架感知到之后就会推进执行各个服务的 <code>Cancel</code> 逻辑，撤销之前执行的各种操作。</li></ul><p>这就是所谓的 <code>TCC</code> 分布式事务。<code>TCC</code> 分布式事务的核心思想，说白了，就是当遇到下面这些情况时：</p><ul><li>某个服务的数据库宕机了。</li><li>某个服务自己挂了。</li><li>那个服务的 <code>Redis</code>、<code>Elasticsearch</code>、<code>MQ</code> 等基础设施故障了。</li><li>某些资源不足了，比如说库存不够这些。</li></ul><p>先来 <code>Try</code> 一下，不要把业务逻辑完成，先试试看，看各个服务能不能基本正常运转，能不能先冻结我需要的资源。</p><p>​    如果 <code>Try</code> 都 <code>OK</code>，也就是说，底层的数据库、<code>Redis</code>、<code>Elasticsearch</code>、<code>MQ</code> 都是可以写入数据的，并且你保留好了需要使用的一些资源（比如冻结了一部分库存）。</p><p>​    接着，再执行各个服务的 <code>Confirm</code> 逻辑，基本上 <code>Confirm</code> 就可以很大概率保证一个分布式事务的完成了。</p><p>​    那如果 <code>Try</code> 阶段某个服务就失败了，比如说底层的数据库挂了，或者 <code>Redis</code> 挂了，等等。</p><p>​    此时就自动执行各个服务的 <code>Cancel</code> 逻辑，把之前的 <code>Try</code> 逻辑都回滚，所有服务都不要执行任何设计的业务逻辑。保证大家要么一起成功，要么一起失败。</p><p>​    等一等，你有没有想到一个问题？如果有一些意外的情况发生了，比如说订单服务突然挂了，然后再次重启，<code>TCC</code> 分布式事务框架是如何保证之前没执行完的分布式事务继续执行的呢？</p><p>​    所以，<code>TCC</code> 事务框架都是要记录一些分布式事务的活动日志的，可以在磁盘上的日志文件里记录，也可以在数据库里记录。保存下来分布式事务运行的各个阶段和状态。</p><p>​    问题还没完，万一某个服务的 <code>Cancel</code> 或者 <code>Confirm</code> 逻辑执行一直失败怎么办呢？</p><p>​    那也很简单，<code>TCC</code> 事务框架会通过活动日志记录各个服务的状态。举个例子，比如发现某个服务的 <code>Cancel</code> 或者 <code>Confirm</code> 一直没成功，会不停的重试调用它的 <code>Cancel</code> 或者 <code>Confirm</code> 逻辑，务必要它成功！</p><p>​    当然了，如果你的代码没有写什么 <code>Bug</code>，有充足的测试，而且 <code>Try</code> 阶段都基本尝试了一下，那么其实一般 <code>Confirm</code>、<code>Cancel</code> 都是可以成功的！</p><p>​    最后，再给大家来一张图，来看看给我们的业务，加上分布式事务之后的整个执行流程：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185722.png" alt="img"></p><p>​    不少大公司里，其实都是自己研发 <code>TCC</code> 分布式事务框架的，专门在公司内部使用，比如我们就是这样。</p><p>​    不过如果自己公司没有研发 <code>TCC</code> 分布式事务框架的话，那一般就会选用开源的框架。</p><p>​    这里笔者给大家推荐几个比较不错的框架，都是咱们国内自己开源出去的：<code>ByteTCC</code>，<code>TCC-transaction</code>，<code>Himly</code>。</p><p>​    大家有兴趣的可以去它们的 <code>GitHub</code> 地址，学习一下如何使用，以及如何跟 <code>Spring Cloud</code>、<code>Dubbo</code> 等服务框架整合使用。</p><p>​    只要把那些框架整合到你的系统里，很容易就可以实现上面那种奇妙的 <code>TCC</code> 分布式事务的效果了。</p><p>​    下面，我们来讲讲可靠消息最终一致性方案实现的分布式事务，同时聊聊在实际生产中遇到的运用该方案的高可用保障架构。</p><h1 id="最终一致性分布式事务如何保障实际生产中-99-99-高可用？"><a href="#最终一致性分布式事务如何保障实际生产中-99-99-高可用？" class="headerlink" title="最终一致性分布式事务如何保障实际生产中 99.99% 高可用？"></a>最终一致性分布式事务如何保障实际生产中 <code>99.99%</code> 高可用？</h1><p>​    上面咱们聊了聊 <code>TCC</code> 分布式事务，对于常见的微服务系统，大部分接口调用是同步的，也就是一个服务直接调用另外一个服务的接口。</p><p>​    这个时候，用 <code>TCC</code> 分布式事务方案来保证各个接口的调用，要么一起成功，要么一起回滚，是比较合适的。</p><p>​    但是在实际系统的开发过程中，可能服务间的调用是异步的。也就是说，一个服务发送一个消息给 <code>MQ</code>，即消息中间件，比如 <code>RocketMQ</code>、<code>RabbitMQ</code>、<code>Kafka</code>、<code>ActiveMQ</code> 等等。</p><p>​    然后，另外一个服务从 <code>MQ</code> 消费到一条消息后进行处理。这就成了基于 <code>MQ</code> 的异步调用了。</p><p>​    那么针对这种基于 <code>MQ</code> 的异步调用，如何保证各个服务间的分布式事务呢？也就是说，我希望的是基于 <code>MQ</code> 实现异步调用的多个服务的业务逻辑，要么一起成功，要么一起失败。</p><p>​    这个时候，就要用上可靠消息最终一致性方案，来实现分布式事务。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185730.png" alt="img"></p><p>​    大家看上图，如果不考虑各种高并发、高可用等技术挑战的话，单从“可靠消息”以及“最终一致性”两个角度来考虑，这种分布式事务方案还是比较简单的。</p><h2 id="可靠消息最终一致性方案的核心流程"><a href="#可靠消息最终一致性方案的核心流程" class="headerlink" title="可靠消息最终一致性方案的核心流程"></a>可靠消息最终一致性方案的核心流程</h2><h3 id="①上游服务投递消息"><a href="#①上游服务投递消息" class="headerlink" title="①上游服务投递消息"></a>①上游服务投递消息</h3><p>​    如果要实现可靠消息最终一致性方案，一般你可以自己写一个可靠消息服务，实现一些业务逻辑。</p><p>​    <strong>首先</strong>，上游服务需要发送一条消息给可靠消息服务。这条消息说白了，你可以认为是对下游服务一个接口的调用，里面包含了对应的一些请求参数。</p><p>​    <strong>然后</strong>，可靠消息服务就得把这条消息存储到自己的数据库里去，状态为“待确认”。</p><p>​    <strong>接着</strong>，上游服务就可以执行自己本地的数据库操作，根据自己的执行结果，再次调用可靠消息服务的接口。</p><p>​    如果本地数据库操作执行成功了，那么就找可靠消息服务确认那条消息。如果本地数据库操作失败了，那么就找可靠消息服务删除那条消息。</p><p>​    此时如果是确认消息，那么可靠消息服务就把数据库里的消息状态更新为“已发送”，同时将消息发送给 <code>MQ</code>。</p><p>​    这里有一个很关键的点，就是更新数据库里的消息状态和投递消息到 <code>MQ</code>。这俩操作，你得放在一个方法里，而且得开启本地事务。</p><p>​    啥意思呢？如果数据库里更新消息的状态失败了，那么就抛异常退出了，就别投递到 <code>MQ</code>；如果投递 <code>MQ</code> 失败报错了，那么就要抛异常让本地数据库事务回滚。这俩操作必须得一起成功，或者一起失败。</p><p>​    如果上游服务是通知删除消息，那么可靠消息服务就得删除这条消息。</p><h3 id="②下游服务接收消息"><a href="#②下游服务接收消息" class="headerlink" title="②下游服务接收消息"></a>②下游服务接收消息</h3><p>​    下游服务就一直等着从 <code>MQ</code> 消费消息好了，如果消费到了消息，那么就操作自己本地数据库。</p><p>​    如果操作成功了，就反过来通知可靠消息服务，说自己处理成功了，然后可靠消息服务就会把消息的状态设置为“已完成”。</p><h3 id="③如何保证上游服务对消息的-100-可靠投递？"><a href="#③如何保证上游服务对消息的-100-可靠投递？" class="headerlink" title="③如何保证上游服务对消息的 100% 可靠投递？"></a>③如何保证上游服务对消息的 <code>100%</code> 可靠投递？</h3><p>​    上面的核心流程大家都看完：一个很大的问题就是，如果在上述投递消息的过程中各个环节出现了问题该怎么办？</p><p>​    我们如何保证消息 <code>100%</code> 的可靠投递，一定会从上游服务投递到下游服务？别着急，下面我们来逐一分析。</p><p>​    如果上游服务给可靠消息服务发送待确认消息的过程出错了，那没关系，上游服务可以感知到调用异常的，就不用执行下面的流程了，这是没问题的。</p><p>​    如果上游服务操作完本地数据库之后，通知可靠消息服务确认消息或者删除消息的时候，出现了问题。</p><p>​    比如：没通知成功，或者没执行成功，或者是可靠消息服务没成功的投递消息到 <code>MQ</code>。这一系列步骤出了问题怎么办？</p><p>​    其实也没关系，因为在这些情况下，那条消息在可靠消息服务的数据库里的状态会一直是“待确认”。</p><p>​    此时，我们在可靠消息服务里开发一个后台定时运行的线程，不停的检查各个消息的状态。</p><p>​    如果一直是“待确认”状态，就认为这个消息出了点什么问题。此时的话，就可以回调上游服务提供的一个接口，问问说，兄弟，这个消息对应的数据库操作，你执行成功了没啊？</p><p>​    如果上游服务答复说，我执行成功了，那么可靠消息服务将消息状态修改为“已发送”，同时投递消息到 <code>MQ</code>。</p><p>​    如果上游服务答复说，没执行成功，那么可靠消息服务将数据库中的消息删除即可。</p><p>​    通过这套机制，就可以保证，可靠消息服务一定会尝试完成消息到 <code>MQ</code> 的投递。</p><h3 id="④如何保证下游服务对消息的-100-可靠接收？"><a href="#④如何保证下游服务对消息的-100-可靠接收？" class="headerlink" title="④如何保证下游服务对消息的 100% 可靠接收？"></a>④如何保证下游服务对消息的 <code>100%</code> 可靠接收？</h3><p>​    那如果下游服务消费消息出了问题，没消费到？或者是下游服务对消息的处理失败了，怎么办？</p><p>​    其实也没关系，在可靠消息服务里开发一个后台线程，不断的检查消息状态。</p><p>​    如果消息状态一直是“已发送”，始终没有变成“已完成”，那么就说明下游服务始终没有处理成功。</p><p>​    此时可靠消息服务就可以再次尝试重新投递消息到 <code>MQ</code>，让下游服务来再次处理。</p><p>​    只要下游服务的接口逻辑实现幂等性，保证多次处理一个消息，不会插入重复数据即可。</p><h3 id="⑤如何基于-RocketMQ-来实现可靠消息最终一致性方案？"><a href="#⑤如何基于-RocketMQ-来实现可靠消息最终一致性方案？" class="headerlink" title="⑤如何基于 RocketMQ 来实现可靠消息最终一致性方案？"></a>⑤如何基于 <code>RocketMQ</code> 来实现可靠消息最终一致性方案？</h3><p>在上面的通用方案设计里，完全依赖可靠消息服务的各种自检机制来确保：</p><ul><li>如果上游服务的数据库操作没成功，下游服务是不会收到任何通知。</li><li>如果上游服务的数据库操作成功了，可靠消息服务死活都会确保将一个调用消息投递给下游服务，而且一定会确保下游服务务必成功处理这条消息。</li></ul><p>通过这套机制，保证了基于 <code>MQ</code> 的异步调用/通知的服务间的分布式事务保障。其实阿里开源的 <code>RocketMQ</code>，就实现了可靠消息服务的所有功能，核心思想跟上面类似。</p><p>​    只不过 <code>RocketMQ</code> 为了保证高并发、高可用、高性能，做了较为复杂的架构实现，非常的优秀。有兴趣的同学，自己可以去查阅 <code>RocketMQ</code> 对分布式事务的支持。</p><h2 id="可靠消息最终一致性方案的高可用保障生产实践"><a href="#可靠消息最终一致性方案的高可用保障生产实践" class="headerlink" title="可靠消息最终一致性方案的高可用保障生产实践"></a>可靠消息最终一致性方案的高可用保障生产实践</h2><h3 id="背景引入"><a href="#背景引入" class="headerlink" title="背景引入"></a>背景引入</h3><p>​    上面那套方案和思想，很多同学应该都知道是怎么回事儿，我们也主要就是铺垫一下这套理论思想。</p><p>​    在实际落地生产的时候，如果没有高并发场景的，完全可以参照上面的思路自己基于某个 <code>MQ</code> 中间件开发一个可靠消息服务。</p><p>​    如果有高并发场景的，可以用 <code>RocketMQ</code> 的分布式事务支持上面的那套流程都可以实现。</p><p>​    今天给大家分享的一个核心主题，就是这套方案如何保证 <code>99.99%</code> 的高可用。</p><p>​    大家应该发现了这套方案里保障高可用性最大的一个依赖点，就是 <code>MQ</code> 的高可用性。</p><p>​    任何一种 <code>MQ</code> 中间件都有一整套的高可用保障机制，无论是 <code>RabbitMQ</code>、<code>RocketMQ</code> 还是 <code>Kafka</code>。</p><p>​    所以在大公司里使用可靠消息最终一致性方案的时候，我们通常对可用性的保障都是依赖于公司基础架构团队对 MQ 的高可用保障。</p><p>​    也就是说，大家应该相信兄弟团队，<code>99.99%</code> 可以保障 <code>MQ</code> 的高可用，绝对不会因为 <code>MQ</code> 集群整体宕机，而导致公司业务系统的分布式事务全部无法运行。</p><p>​    但是现实是很残酷的，很多中小型的公司，甚至是一些中大型公司，或多或少都遇到过 <code>MQ</code> 集群整体故障的场景。</p><p>​    <code>MQ</code> 一旦完全不可用，就会导致业务系统的各个服务之间无法通过 <code>MQ</code> 来投递消息，导致业务流程中断。</p><p>​    比如最近就有一个朋友的公司，也是做电商业务的，就遇到了 <code>MQ</code> 中间件在自己公司机器上部署的集群整体故障不可用，导致依赖 <code>MQ</code> 的分布式事务全部无法跑通，业务流程大量中断的情况。</p><p>​    这种情况，就需要针对这套分布式事务方案实现一套高可用保障机制。</p><h3 id="基于-KV-存储的队列支持的高可用降级方案"><a href="#基于-KV-存储的队列支持的高可用降级方案" class="headerlink" title="基于 KV 存储的队列支持的高可用降级方案"></a>基于 <code>KV</code> 存储的队列支持的高可用降级方案</h3><p>​    大家来看看下面这张图，这是我曾经指导过朋友的一个公司针对可靠消息最终一致性方案设计的一套高可用保障降级机制。</p><p>​    这套机制不算太复杂，可以非常简单有效的保证那位朋友公司的高可用保障场景，一旦 <code>MQ</code> 中间件出现故障，立马自动降级为备用方案。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185746.png" alt="img" style="zoom:150%;"></p><h3 id="①自行封装-MQ-客户端组件与故障感知"><a href="#①自行封装-MQ-客户端组件与故障感知" class="headerlink" title="①自行封装 MQ 客户端组件与故障感知"></a>①自行封装 <code>MQ</code> 客户端组件与故障感知</h3><p>​    首先第一点，你要做到自动感知 <code>MQ</code> 的故障接着自动完成降级，那么必须动手对 <code>MQ</code> 客户端进行封装，发布到公司 <code>Nexus</code> 私服上去。</p><p>​    然后公司需要支持 <code>MQ</code> 降级的业务服务都使用这个自己封装的组件来发送消息到 <code>MQ</code>，以及从 <code>MQ</code> 消费消息。</p><p>​    在你自己封装的 <code>MQ</code> 客户端组件里，你可以根据写入 <code>MQ</code> 的情况来判断 <code>MQ</code> 是否故障。</p><p>​    比如说，如果连续 <code>10</code> 次重新尝试投递消息到 <code>MQ</code> 都发现异常报错，网络无法联通等问题，说明 <code>MQ</code> 故障，此时就可以自动感知以及自动触发降级开关。</p><h3 id="②基于-KV-存储中队列的降级方案"><a href="#②基于-KV-存储中队列的降级方案" class="headerlink" title="②基于 KV 存储中队列的降级方案"></a>②基于 <code>KV</code> 存储中队列的降级方案</h3><p>​    如果 <code>MQ</code> 挂掉之后，要是希望继续投递消息，那么就必须得找一个 <code>MQ</code> 的替代品。</p><p>​    举个例子，比如我那位朋友的公司是没有高并发场景的，消息的量很少，只不过可用性要求高。此时就可以使用类似 <code>Redis</code> 的 <code>KV</code> 存储中的队列来进行替代。</p><p>​    由于 <code>Redis</code> 本身就支持队列的功能，还有类似队列的各种数据结构，所以你可以将消息写入 <code>KV</code> 存储格式的队列数据结构中去。</p><blockquote><p>PS：关于 <code>Redis</code> 的数据存储格式、支持的数据结构等基础知识，请大家自行查阅了，网上一大堆。</p></blockquote><p>​    但是，这里有几个大坑，一定要注意一下：</p><p>​    <strong>第一个</strong>，任何 <code>KV</code> 存储的集合类数据结构，建议不要往里面写入数据量过大，否则会导致大 <code>Value</code> 的情况发生，引发严重的后果。</p><p>​    因此绝不能在 <code>Redis</code> 里搞一个 <code>Key</code>，就拼命往这个数据结构中一直写入消息，这是肯定不行的。</p><p>​    <strong>第二个</strong>，绝对不能往少数 <code>Key</code> 对应的数据结构中持续写入数据，那样会导致热 <code>Key</code> 的产生，也就是某几个 <code>Key</code> 特别热。</p><p>​    大家要知道，一般 <code>KV</code> 集群，都是根据 <code>Key</code> 来 <code>Hash</code> 分配到各个机器上的，你要是老写少数几个 <code>Key</code>，会导致 <code>KV</code> 集群中的某台机器访问过高，负载过大。</p><p>​    基于以上考虑，下面是笔者当时设计的方案：</p><ul><li>根据它们每天的消息量，在 <code>KV</code> 存储中固定划分上百个队列，有上百个 <code>Key</code> 对应。</li><li>这样保证每个 <code>Key</code> 对应的数据结构中不会写入过多的消息，而且不会频繁的写少数几个 <code>Key</code>。</li><li>一旦发生了 <code>MQ</code> 故障，可靠消息服务可以对每个消息通过 <code>Hash</code> 算法，均匀的写入固定好的上百个 <code>Key</code> 对应的 <code>KV</code> 存储的队列中。</li></ul><p>同时需要通过 <code>ZK</code> 触发一个降级开关，整个系统在 <code>MQ</code> 这块的读和写全部立马降级。</p><h3 id="③下游服务消费-MQ-的降级感知"><a href="#③下游服务消费-MQ-的降级感知" class="headerlink" title="③下游服务消费 MQ 的降级感知"></a>③下游服务消费 <code>MQ</code> 的降级感知</h3><p>​    下游服务消费 <code>MQ</code> 也是通过自行封装的组件来做的，此时那个组件如果从 <code>ZK</code> 感知到降级开关打开了，首先会判断自己是否还能继续从 <code>MQ</code> 消费到数据？</p><p>​    如果不能了，就开启多个线程，并发的从 <code>KV</code> 存储的各个预设好的上百个队列中不断的获取数据。</p><p>​    每次获取到一条数据，就交给下游服务的业务逻辑来执行。通过这套机制，就实现了 <code>MQ</code> 故障时候的自动故障感知，以及自动降级。如果系统的负载和并发不是很高的话，用这套方案大致是没问题的。</p><p>​    因为在生产落地的过程中，包括大量的容灾演练以及生产实际故障发生时的表现来看，都是可以有效的保证 <code>MQ</code> 故障时，业务流程继续自动运行的。</p><h3 id="④故障的自动恢复"><a href="#④故障的自动恢复" class="headerlink" title="④故障的自动恢复"></a>④故障的自动恢复</h3><p>​    如果降级开关打开之后，自行封装的组件需要开启一个线程，每隔一段时间尝试给 <code>MQ</code> 投递一个消息看看是否恢复了。</p><p>​    如果 <code>MQ</code> 已经恢复可以正常投递消息了，此时就可以通过 <code>ZK</code> 关闭降级开关，然后可靠消息服务继续投递消息到 <code>MQ</code>，下游服务在确认 <code>KV</code> 存储的各个队列中已经没有数据之后，就可以重新切换为从 <code>MQ</code> 消费消息。</p><h3 id="⑤更多的业务细节"><a href="#⑤更多的业务细节" class="headerlink" title="⑤更多的业务细节"></a>⑤更多的业务细节</h3><p>​    上面说的那套方案是一套通用的降级方案，但是具体的落地是要结合各个公司不同的业务细节来决定的，很多细节多没法在文章里体现。</p><p>​    比如说你们要不要保证消息的顺序性？是不是涉及到需要根据业务动态，生成大量的 <code>Key</code>？等等。</p><p>​    此外，这套方案实现起来还是有一定的成本的，所以建议大家尽可能还是 <code>Push</code> 公司的基础架构团队，保证 <code>MQ</code> 的 <code>99.99%</code> 可用性，不要宕机。</p><p>​    其次就是根据大家公司实际对高可用的需求来决定，如果感觉 <code>MQ</code> 偶尔宕机也没事，可以容忍的话，那么也不用实现这种降级方案。</p><p>​    但是如果公司领导认为 <code>MQ</code> 中间件宕机后，一定要保证业务系统流程继续运行，那么还是要考虑一些高可用的降级方案，比如本文提到的这种。</p><p>​    最后再说一句，真要是一些公司涉及到每秒几万几十万的高并发请求，那么对 <code>MQ</code> 的降级方案会设计的更加的复杂，那就远远不是这么简单可以做到的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前网上看到很多写分布式事务的文章，不过大多都是将分布式事务各种技术方案简单介绍一下。很多朋友看了还是不知道分布式事务到底怎么回事，在项目里到底如何使用。&lt;/p&gt;
&lt;p&gt;所以这篇文章，就用大白话+手工绘图，并结合一个电商系统的案例实践，来给大家讲清楚到底什么是 &lt;code&gt;TCC&lt;/code&gt; 分布式事务。&lt;/p&gt;
    
    </summary>
    
      <category term="分布式算法" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="分布式算法" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>分布式算法2-CAP理论</title>
    <link href="http://yoursite.com/2019/10/01/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%952-CAP%E7%90%86%E8%AE%BA/"/>
    <id>http://yoursite.com/2019/10/01/分布式算法2-CAP理论/</id>
    <published>2019-10-01T09:11:02.000Z</published>
    <updated>2020-11-16T04:52:31.800Z</updated>
    
    <content type="html"><![CDATA[<h2 id="CAP理论"><a href="#CAP理论" class="headerlink" title="CAP理论"></a><code>CAP</code>理论</h2><p>​    <code>CAP</code>原则又称<code>CAP</code>定理，指的是在一个分布式系统中， <code>Consistency</code>（一致性）、<code>Availability</code>（可用性）、<code>Partition tolerance</code>（分区容错性），三者不可得兼。</p><a id="more"></a><p>​    <code>CAP</code>原则是<code>NOSQL</code>数据库的基石。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/67aa4a0e56dcc8ad34bf1b0232f12748.jpg" style="zoom: 33%;"></p><p>分布式系统的<code>CAP</code>理论：理论首先把分布式系统中的三个特性进行了如下归纳：</p><ul><li><strong>一致性（<code>C</code>）</strong>：在分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本）</li><li><strong>可用性（<code>A</code>）</strong>：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。（对数据更新具备高可用性）</li><li><strong>分区容忍性（<code>P</code>）</strong>：以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在<code>C</code>和<code>A</code>之间做出选择。</li></ul><h2 id="概念详解"><a href="#概念详解" class="headerlink" title="概念详解"></a>概念详解</h2><h3 id="Partition-tolerance"><a href="#Partition-tolerance" class="headerlink" title="Partition tolerance"></a><code>Partition tolerance</code></h3><p>​    先看<code>Partition tolerance</code>，中文叫做<strong>“分区容错”</strong>。</p><p>​    大多数分布式系统都分布在多个子网络。每个子网络就叫做一个区（<code>partition</code>）。分区容错的意思是，区间通信可能失败。比如，一台服务器放在中国，另一台服务器放在美国，这就是两个区，它们之间可能无法通信。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/bg2018071601.png" style="zoom: 67%;"></p><p>​    上图中，<code>G1</code>和<code>G2</code>是两台跨区的服务器。<code>G1</code>向<code>G2</code>发送一条消息，<code>G2</code>可能无法收到。系统设计的时候，必须考虑到这种情况。</p><p>​    一般来说，分区容错无法避免，因此可以认为<code>CAP</code>的<code>P</code>总是成立。<code>CAP</code>定理告诉我们，剩下的<code>C</code>和<code>A</code>无法同时做到。</p><h3 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a><code>Consistency</code></h3><p>​    <code>Consistency</code>中文叫做”一致性”。意思是，写操作之后的读操作，必须返回该值。举例来说，某条记录是 <code>v0</code>，用户向<code>G1</code>发起一个写操作，将其改为<code>v1</code>。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/bg2018071602.png" style="zoom:67%;"></p><p>​    接下来，用户的读操作就会得到<code>v1</code>。这就叫<strong>一致性</strong>。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/bg2018071603.png" style="zoom:67%;"></p><p>​    问题是，用户有可能向<code>G2</code>发起读操作，由于<code>G2</code>的值没有发生变化，因此返回的是<code>v0</code>。<code>G1</code>和<code>G2</code>读操作的结果不一致，这就不满足一致性了。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/bg2018071604.png" style="zoom:67%;"></p><p>​    为了让<code>G2</code>也能变为<code>v1</code>，就要在<code>G1</code>写操作的时候，让<code>G1</code>向<code>G2</code>发送一条消息，要求<code>G2</code>也改成<code>v1</code>。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/bg2018071605.png" style="zoom:67%;"></p><p>​    这样的话，用户向<code>G2</code>发起读操作，也能得到<code>v1</code>。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/bg2018071606.png" style="zoom:67%;"></p><p>对于一致性，也可以分为从客户端和服务端两个不同的视角来理解。</p><ul><li><p><strong>客户端</strong><br>从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。也就是<code>client</code>同时访问<code>G1</code>和<code>G2</code>，如何获取更新的最新的数据。</p></li><li><p><strong>服务端</strong></p><p>从服务端来看，则是更新如何分布到整个系统，以保证数据最终一致。也就是<code>G1</code>节点和<code>G2</code>节点如何通信保持数据的一致。</p></li></ul><p>对于一致性，一致的程度不同大体可以分为强、弱、最终一致性三类。</p><ul><li><p><strong>强一致性</strong></p><p>对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。比如<code>client</code>更新<code>G1</code>为<code>V1</code>，那么<code>client</code>读取<code>G2</code>的时候也应该是<code>V1</code>。</p></li><li><p><strong>弱一致性</strong></p><p>如果能容忍后续的部分或者全部访问不到，则是弱一致性。比如<code>client</code>更新<code>G1</code>为<code>V1</code>，那么可以容忍<code>client</code>读取<code>G2</code>的时候是<code>V0</code>。</p></li><li><p><strong>最终一致性</strong></p><p>如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。比如<code>client</code>更新<code>G1</code>为<code>V1</code>，可以使得<code>client</code>在一段时间之后读取的时候是<code>V1</code>。</p></li></ul><blockquote><p>注意：不要将弱一致性、最终一致性放到<code>CAP</code>理论里混为一谈。你可以认为弱一致性、最终一致性和<code>CAP</code>的<code>C</code>一点关系也没有，因为<code>CAP</code>的<code>C</code>是更新操作完成后，任何节点看到的数据完全一致。弱一致性、最终一致性本身和<code>CAP</code>的<code>C</code>一致性是违背的。</p></blockquote><h3 id="Availability"><a href="#Availability" class="headerlink" title="Availability"></a><code>Availability</code></h3><p>​    <code>Availability</code>中文叫做<strong>“可用性”</strong>，意思是只要收到用户的请求，服务器就必须给出回应。</p><p>​    用户可以选择向<code>G1</code>或<code>G2</code>发起读操作。不管是哪台服务器，只要收到请求，就必须告诉用户，到底是<code>v0</code>还是<code>v1</code>，否则就不满足可用性。</p><h2 id="取舍策略"><a href="#取舍策略" class="headerlink" title="取舍策略"></a>取舍策略</h2><p>​    <code>CAP</code>三个特性只能满足其中两个，那么取舍的策略就共有三种：</p><p>​    <strong><code>CA without P</code></strong>：<strong>如果不要求<code>P</code>（不允许分区），则<code>C</code>（强一致性）和<code>A</code>（可用性）是可以保证的。</strong>但放弃<code>P</code>的同时也就意味着放弃了系统的扩展性，也就是分布式节点受限，没办法部署子节点，这是违背分布式系统设计的初衷的。传统的关系型数据库<code>RDBMS</code>：<code>Oracle</code>、<code>MySQL</code>就是<code>CA</code>。</p><p>​    <strong><code>CP without A</code></strong>：<strong>如果不要求<code>A</code>（可用），相当于每个请求都需要在服务器之间保持强一致。</strong><code>P</code>（分区）会导致同步时间无限延长(也就是等待数据同步完才能正常访问服务)，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。设计成<code>CP</code>的系统其实不少，最典型的就是分布式数据库，如<code>Redis</code>、<code>HBase</code>等。对于这些分布式数据库来说，数据的一致性是最基本的要求，因为如果连这个标准都达不到，那么直接采用关系型数据库就好，没必要再浪费资源来部署分布式数据库。</p><p>​    <strong><code>AP wihtout C</code></strong>：<strong>要高可用并允许分区，则需放弃一致性。</strong>一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。典型的应用就如某米的抢购手机场景，可能前几秒你浏览商品的时候页面提示是有库存的，当你选择完商品准备下单的时候，系统提示你下单失败，商品已售完。这其实就是先在<code>A</code>（可用性）方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，虽然多少会影响一些用户体验，但也不至于造成用户购物流程的严重阻塞。放弃一致性不是说一致性就不保证了，而是逐渐的变得一致。</p><p>​    实时证明，大多数都是牺牲了一致性。像<code>12306</code>还有淘宝网，就好比是你买火车票，本来你看到的是还有一张票，其实在这个时刻已经被买走了，你填好了信息准备买的时候发现系统提示你没票了。这就是牺牲了一致性。</p><p>​    但是不是说牺牲一致性一定是最好的。就好比<code>mysql</code>中的事务机制，张三给李四转了<code>100</code>块钱，这时候必须保证张三的账户上少了<code>100</code>，李四的账户多了<code>100</code>。因此需要数据的一致性，而且什么时候转钱都可以，也需要可用性。但是可以转钱失败是可以允许的。</p><h2 id="正确理解CAP理论"><a href="#正确理解CAP理论" class="headerlink" title="正确理解CAP理论"></a>正确理解<code>CAP</code>理论</h2><p>​    目前，<code>CAP</code>理论普遍被当作是大数据技术的理论基础。同时，根据该理论，业界有一种非常流行、非常“专业”的认识，那就是：关系型数据库设计选择了<code>C</code>（一致性）与<code>A</code>（可用性），<code>NoSQL</code>数据库设计则不同。其中，<code>HBase</code>选择了<code>C</code>（一致性）与<code>P</code>（分区可容忍性），<code>Cassandra</code>选择了<code>A</code>（可用性）与<code>P</code>（分区可容忍性）。</p><p>　实际上，这种认识是不准确的，甚至是不正确的。</p><p><strong>常见的理解及分析</strong></p><p>　目前流行的、对<code>CAP</code>理论解释的情形是从同一数据在网络环境中的多个副本出发的。为了保证数据不会丢失，在企业级的数据管理方案中，一般必须考虑数据的冗余存储问题，而这应该是通过在网络上的其他独立物理存储节点上保留另一份、或多份数据副本来实现的。</p><p>　数据在节点<code>A</code>、<code>B</code>、<code>C</code>上保留了三份，如果对节点<code>A</code>上的数据进行了修改，然后再让客户端通过网络对该数据进行读取。那么，客户端的读取操作什么时候返回呢？</p><p>　有这样两种情况：一种情况是要求节点<code>A</code>、<code>B</code>、<code>C</code>的三份数据完全一致后返回。也就是说，这时从任何一个网络节点读取的数据都是一样的，这就是所谓的强一致性读。同时<code>A</code>、<code>B</code>、<code>C</code>三个节点中任何一个宕机，都会导致数据不可用。也就是说，要保证强一致性，网络中的副本越多，数据的可用性就越差；</p><p>　另一种情况是，允许读操作立即返回，容忍<code>B</code>节点的读取与<code>A</code>节点的读取不一致的情况发生。这样一来，可用性显然得到了提高，网络中的副本也可以多一些，唯一得不到保证的是数据一致性。当然，对写操作同样也有多个节点一致性的情况。</p><p>　可以看出，上述对<code>CAP</code>理论的解释主要是从网络上多个节点之间的读写一致性出发考虑问题的。而这一点，对于关系型数据库意味着什么呢？当然主要是指通常所说的<code>Standby</code>情况。对此，在实践中我们大多已经采取了弱一致性的异步延时同步方案，以提高可用性。这种情况并不存在关系型数据库为保证<code>C</code>、<code>A</code>而放弃<code>P</code>的情况；而对海量数据管理的需求，关系型数据库扩展过程中所遇到的性能瓶颈，似乎也并不是<code>CAP</code>理论中所描述的那种原因造成的。</p><p>　因此，如果根据现有的大多数资料对<code>CAP</code>理论的如上解释，即只将其当作分布式系统中多个数据副本之间的读写一致性问题的通用理论对待，那么就可以得出结论：<strong><code>CAP</code>既适用于<code>NoSQL</code>数据库，也适用于关系型数据库。它是<code>NoSQL</code>数据库、关系型数据库，乃至一切分布式系统在设计数据多个副本之间读写一致性问题时需要遵循的共同原则。</strong></p><h3 id="两种重要的分布式场景"><a href="#两种重要的分布式场景" class="headerlink" title="两种重要的分布式场景"></a>两种重要的分布式场景</h3><p>​    下面我们要说的重点与核心是：关于对<code>CAP</code>理论中一致性<code>C</code>的理解，除了上述数据副本之间的读写一致性以外，分布式环境中还有两种非常重要的场景，那就是<strong>事务与关联</strong>。</p><p>　先来看看分布式环境中的事务场景。我们知道，在关系型数据库的事务操作遵循<code>ACID</code>原则，其中的一致性<code>C</code>，主要是指一个事务中相关联的数据在事务操作结束后是一致的。所谓<code>ACID</code>原则，是指在写入/移动资料的过程中，为保证交易正确可靠所必须具备的四个特性：即原子性（<code>Atomicity</code>，或称不可分割性）、一致性（<code>Consistency</code>）、隔离性（<code>Isolation</code>，又称独立性）和持久性（<code>Durability</code>）。</p><p>　例如银行的一个存款交易事务，将导致交易流水表增加一条记录。同时，必须导致账户表余额发生变化，这两个操作必须是一个事务中全部完成，保证相关数据的一致性。而前文解释的<code>CAP</code>理论中的<code>C</code>是指对一个数据多个备份的读写一致性。表面上看，这两者不是一回事，但实际上，却是本质基本相同的事物：<strong>数据请求会等待多个相关数据操作全部完成才返回。</strong>对分布式系统来讲，这就是我们通常所说的分布式事务问题。</p><p>　众所周知，分布式事务一般采用两阶段提交策略来实现，这是一个非常耗时的复杂过程，会严重影响系统效率，在实践中我们尽量避免使用它。在实践过程中，如果我们为了扩展数据容量将数据分布式存储，而事务的要求又完全不能降低。那么，系统的可用性一定会大大降低，在现实中我们一般都采用对这些数据不分散存储的策略。</p><p>　当然，我们也可以说，最常使用的关系型数据库，因为这个原因，扩展性（分区可容忍性<code>P</code>）受到了限制，这是完全符合CAP理论的。但同时我们应该意识到，这对<code>NoSQL</code>数据库也是一样的。如果<code>NoSQL</code>数据库也要求严格的分布式事务功能，情况并不会比关系型数据库好多少。只是在<code>NoSQL</code>的设计中，我们往往会弱化甚至去除事务的功能，该问题才表现得不那么明显而已。</p><p>　因此，在扩展性问题上，如果要说关系型数据库是为了保证<code>C</code>、<code>A</code>而牺牲<code>P</code>，在尽量避免分布式事务这一点上来看，应该是正确的。也就是说：<strong>关系型数据库应该具有强大的事务功能，如果分区扩展，可用性就会降低；而<code>NoSQL</code>数据库干脆弱化甚至去除了事务功能，因此，分区的可扩展性就大大增加了。</strong></p><p>　再来看看分布式环境中的关联场景。初看起来，关系型数据库中常用的多表关联操作与<code>CAP</code>理论就更加不沾边了。但仔细考虑，也可以用它来解释数据库分区扩展对关联所带来的影响。对一个数据库来讲，采用了分区扩展策略来扩充容量，数据分散存储了，很显然多表关联的性能就会下降，因为我们必须在网络上进行大量的数据迁移操作，这与<code>CAP</code>理论中数据副本之间的同步操作本质上也是相同的。</p><p>　因此，如果要保证系统的高可用性，需要同时实现强大的多表关系操作的关系型数据库在分区可扩展性上就遇到了极大的限制，而<code>NoSQL</code>数据库则干脆在设计上弱化甚至去除了多表关联操作。那么，从这一点上来理解“<code>NoSQL</code>数据库是为了保证<code>A</code>与<code>P</code>，而牺牲<code>C</code>”的说法，也是可以讲得通的。当然，我们应该理解，关联问题在很多情况下不是并行处理的优点所在，这在很大程度上与<code>Amdahl</code>定律相符合。</p><p>　所以，从事务与关联的角度来关系型数据库的分区可扩展性为什么受限的原因是最为清楚的。而<code>NoSQL</code>数据库也正是因为弱化，甚至去除了像事务与关联（全面地讲，其实还有索引等特性）等在分布式环境中会严重影响系统可用性的功能，才获得了更好的水平可扩展性。</p><p>　那么，如果将事务与关联也纳入<code>CAP</code>理论中一致性<code>C</code>的范畴的话，问题就很清楚了：<strong>关于“关系型数据库为了保证一致性<code>C</code>与可用性<code>A</code>，而不得不牺牲分区可容忍性<code>P</code>”的说法便是正确的了。但关于“<code>NoSQL</code>选择了<code>C</code>与<code>P</code>，或者<code>A</code>与<code>P</code>”的说法则是错误的，所有的<code>NoSQL</code>数据库在设计策略的大方向上都是选择了<code>A</code>与<code>P</code>，从来没有完全选择<code>C</code>与<code>P</code>的情况存在。</strong></p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>　现在看来，如果理解<code>CAP</code>理论只是指多个数据副本之间读写一致性的问题，那么它对关系型数据库与<code>NoSQL</code>数据库来讲是完全一样的，它只是运行在分布式环境中的数据管理设施在设计读写一致性问题时需要遵循的一个原则而已，却并不是<code>NoSQL</code>数据库具有优秀的水平可扩展性的真正原因。</p><p>​    而如果将<code>CAP</code>理论中的一致性<code>C</code>理解为<strong>读写一致性、事务与关联操作的综合</strong>，则可以认为关系型数据库选择了<code>C</code>与<code>A</code>，而<code>NoSQL</code>数据库则全都是选择了<code>A</code>与<code>P</code>，但并没有选择<code>C</code>与<code>P</code>的情况存在。这才是用<code>CAP</code>理论来支持<code>NoSQL</code>数据库设计正确认识。</p><p>　其实，这种认识正好与被广泛认同的<code>NoSQL</code>的另一个理论基础相吻合，即与<code>ACID</code>对着干的<code>BASE</code>（基本可用性、软状态与最终一致性）。因为<code>BASE</code>的含义正好是指“<code>NoSQL</code>数据库设计可以通过牺牲一定的数据一致性和容错性来换取高性能的保持甚至提高”，即<code>NoSQL</code>数据库都应该是牺牲<code>C</code>来换取<code>P</code>，而不是牺牲<code>A</code>。可用性<code>A</code>正好是所有<code>NoSQL</code>数据库都普遍追求的特性。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;CAP理论&quot;&gt;&lt;a href=&quot;#CAP理论&quot; class=&quot;headerlink&quot; title=&quot;CAP理论&quot;&gt;&lt;/a&gt;&lt;code&gt;CAP&lt;/code&gt;理论&lt;/h2&gt;&lt;p&gt;​    &lt;code&gt;CAP&lt;/code&gt;原则又称&lt;code&gt;CAP&lt;/code&gt;定理，指的是在一个分布式系统中， &lt;code&gt;Consistency&lt;/code&gt;（一致性）、&lt;code&gt;Availability&lt;/code&gt;（可用性）、&lt;code&gt;Partition tolerance&lt;/code&gt;（分区容错性），三者不可得兼。&lt;/p&gt;
    
    </summary>
    
      <category term="分布式算法" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="分布式算法" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>分布式算法9-一致性协议之ZAB</title>
    <link href="http://yoursite.com/2019/10/01/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%959-%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%B9%8BZAB/"/>
    <id>http://yoursite.com/2019/10/01/分布式算法9-一致性协议之ZAB/</id>
    <published>2019-10-01T09:11:02.000Z</published>
    <updated>2020-11-16T04:53:09.028Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Zookeeper"><a href="#一、Zookeeper" class="headerlink" title="一、Zookeeper"></a>一、<code>Zookeeper</code></h2><blockquote><p>​    <code>Zookeeper</code> 致力于提供一个高性能、高可用，且具有严格的顺序访问控制能力(主要是写操作的严格顺序性)的分布式协调服务。高性能使得  <code>Zookeeper</code>  能够应用于那些对系统吞吐有明确要求的大型分布式系统中，高可用使得分布式的单点问题得到了很好的解决，而严格的顺序访问控制使得客户端能够基于  <code>Zookeeper</code> 实现一些复杂的同步原语(<code>Zxid</code>)。</p></blockquote><a id="more"></a><ol><li><strong>简单的数据模型</strong><br> <code>Zookeeper</code> 使得分布式程序能够通过一个共享的树形结构的名字空间来进行相互协调，即 <code>Zookeeper</code>  服务器内存中的数据模型由一系列被称为 <code>ZNode</code> 的数据节点组成，<code>Zookeeper</code>  将全量的数据存储在内存中，以此来提高服务器吞吐、减少延迟的目的。</li><li><strong>可构建集群</strong><br> 一个 <code>Zookeeper</code> 集群通常由一组机器构成，组成 <code>Zookeeper</code> 集群的而每台机器都会在内存中维护当前服务器状态，并且每台机器之间都相互通信。</li><li><strong>顺序访问</strong><br> 对于来自客户端的每个更新请求，<code>Zookeeper</code> 都会分配一个全局唯一的递增编号，这个编号反映了所有事务操作的先后顺序。</li><li><strong>高性能</strong><br> <code>Zookeeper</code> 将全量数据存储在内存中，并直接服务于客户端的所有非事务请求，因此它尤其适用于以读操作为主的应用场景。</li></ol><h2 id="二、ZAB-原子消息广播协议"><a href="#二、ZAB-原子消息广播协议" class="headerlink" title="二、ZAB (原子消息广播协议)"></a>二、<code>ZAB</code> (原子消息广播协议)</h2><blockquote><p>​    <code>ZAB</code> (<code>ZooKeeper Atomic Broadcast</code>) 是 <code>Zookeeper</code> 数据一致性的核心算法，是 <code>Zookeeper</code> 实现分布式系统数据一致性。<code>ZAB</code> 协议专门为 <code>Zookeeper</code> 设计了一种支持崩溃恢复的原子广播协议。</p></blockquote><p>​    <code>ZAB</code> 协议的核心：<strong>所有事务请求(写请求)必须由一个全局唯一的 <code>Leader</code> 服务器来协调处理</strong>  ，而余下的其他服务器则成为 <code>Follower</code> 服务器。 <code>Leader</code> 服务器负责将一个客户端事务请求转换成一个事务  <code>proposal</code>(提议)，并将该 Proposal 分发给集群中所有的 <code>Follower</code> 服务器。之后 <code>Leader</code> 服务器需要等待所有  <code>Follower</code> 服务器的反馈，一旦超过半数的 <code>Follower</code> 服务器进行了正确的反馈后，那么 <code>Leader</code> 就会再次向所有的  <code>Follower</code> 服务器分 发 <code>Commit</code> 消息，要求其将前一个 <code>proposal</code> 进行提交。</p><p>​    <code>ZAB</code> 协议包括两种基本的模式，分别是 <strong>崩溃恢复</strong>和<strong>消息广播</strong>。</p><p>​    当整个服务框架在启动过程中，或是当 <code>Leader</code> 服务器出现网络中断、崩溃退出与重启等异常情况时， <code>ZAB</code>  协议就会进入恢复模式并选举产生新的 <code>Leader</code> 服务器。当选举产生了新的 <code>Leader</code> 服务器同时集群中已经有过半的机器与该 <code>Leader</code>  服务器完成了状态同步之后，<code>ZAB</code> 协议就会退出恢复模式。</p><p>​    当集群中已经有过半的 <code>Follower</code> 服务器完成了和 <code>Leader</code>  服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。当一台同样遵守 <code>ZAB</code> 协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个 <code>Leader</code> 服务器在负责进行消息广播， 那么新加入的服务器就会自觉地进入数据恢复模式：找到 <code>Leader</code>  所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。</p><p>​    下面重点讲解崩溃回复和消息广播的过程。</p><h3 id="2-1-消息广播"><a href="#2-1-消息广播" class="headerlink" title="2.1 消息广播"></a>2.1 消息广播</h3><p>​    <code>ZAB</code> 协议的消息广播过程使用的是一个原子广播协议，类似于一个二阶段提交过程。针对客户端的事务请求， <code>Leader</code> 服务器会为其生成对应的事务 <code>Proposal</code> ，并将其发送给集群中其余所有的机器，然后再分別收集各自的选票，最后进行事务提交。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185950.png" alt="消息广播"><br> 　在 <code>ZAB</code> 协议的二阶段提交过程中，移除了中断逻辑，所有的 Foll<code>o</code>wer 服务器要么正常反馈 <code>Leader</code> 提出的事务 <code>Proposal</code>  ,要么就抛弃 <code>Leader</code> 服务器。同时， <code>ZAB</code> 协议将二阶段提交中的中断逻辑移除意味着我们可以在过半的 <code>Follower</code> 服务器已经反馈  <code>Ack</code> 之后就开始提交事务 <code>Proposal</code> 了，而不需要等待集群中所有的 <code>Follower</code>  服务器都反馈响应。这种简化了的二阶段提交模型无法处理 <code>Leader</code> 服务器崩溃退出而带来的数据不一致问题，此时采用崩溃恢复模式来解决这个问题。</p><p>​    在整个消息广播过程中， <code>Leader</code> 服务器会为每个事务请求生成对应的 <code>Proposal</code> 来进行广播，并且在广播事务 <code>Proposal</code> 之前， <code>Leader</code> 服务器会首先为这个事务 <code>Proposal</code> 分配一个全局单调递增的唯一事务<code>ID</code> (即 <code>ZXID</code>)。</p><p><code>Leader</code> 服务器会为每一个 <code>Follower</code> 服务器都各自分配一个单独的队列，然后将需要广播的事务 <code>Proposal</code>  依次放入这些队列中去，并且根据 <code>FIFO</code> 策略进行消息发送。每一个 <code>Follower</code> 服务器在接收到这个事务 <code>Proposal</code>  之后，都会首先将其以事务日志的形式写入到本地磁盘中去，并且在成功写入后反馈给 <code>Leader</code> 服务器一个 <code>Ack</code> 响应。当 <code>Leader</code>  服务器接收到超过半数 <code>Follower</code> 的 <code>Ack</code> 响应后，就会广播一个 <code>Commit</code> 消息给所有的 <code>Follower</code>  服务器以通知其进行事务提交，同时 <code>Leader</code> 自身也会完成对事务的提交。</p><h3 id="2-2-崩溃恢复"><a href="#2-2-崩溃恢复" class="headerlink" title="2.2 崩溃恢复"></a>2.2 崩溃恢复</h3><p>​    <code>Leader</code> 服务器出现崩溃，或者说由于网络原因导致 <code>Leader</code> 服务器失去了与过半 <code>Follower</code>  的联系，那么就会进入崩溃恢复模式。<code>Leader</code> 选举算法不仅仅需要让 <code>Leader</code> 自己知道其自身已经被选举为 <code>Leader</code>  ，同时还需要让集群中的所有其他机器也能够快速地感知到选举产生的新的 <code>Leader</code> 服务器。</p><p>​    <code>ZAB</code> 协议规定了如果一个事务 <code>Proposal</code> 在一台机器上被处理成功，那么应该在所有的机器上都被处理成功，哪怕机器出现故障崩溃。</p><p>​    下面介绍两种崩溃恢复中的场景和 <code>ZAB</code> 协议需要保证的特性：</p><ol><li><p><code>ZAB</code> 协议需要确保那些已经在 <code>Leader</code> 服务器上提交的事务最终被所有服务器都提交</p><p>假设一个事务在 <code>Leader</code> 服务器上被提交了，并且已经得到过半 <code>Follower</code> 服务器的 <code>Ack</code> 反馈，但是在它将 <code>Commit</code>  消息发送给所有 <code>Follower</code> 机器之前， <code>Leader</code> 服务器挂了，针对这种情况， <code>ZAB</code>  协议就需要确保该事务最终能够在所有的服务器上都被提交成功，否则将出现不一致。</p></li><li><p><code>ZAB</code> 协议需要确保丢弃那些只在 <code>Leader</code> 服务器上被提出的事务</p><p>假设初始的 <code>Leader</code> 服务器 在提出了一个事务之后就崩溃退出了，导致集群中的其他服务器都没有收到这个事务，当该服务器恢复过来再次加入到集群中的时候 ，<code>ZAB</code>协议需要确保丢弃这个事务。</p></li></ol><p>针对以上两点需求，<code>ZAB</code> 协议需要设计的选举算法应该满足：确保提交已经被 <code>Leader</code> 提交的事务 <code>Proposal</code>，同时丢弃已经被跳过的事务 <code>Proposal</code> 。</p><p>​    如果让 <code>Leader</code> 选举算法能够保证新选举出来的 <code>Leader</code> 服务器拥有集群中所有机器最高编号（即 <code>ZXID</code> 最大）的事务  <code>Proposal</code>,那么就可以保证这个新选举出来的 <code>Leader</code> 一定具有所有已经提交的提案。同时，如果让具有最高编号事务 <code>Proposal</code>  的机器来成为 <code>Leader</code>，就可以省去 <code>Leader</code> 服务器检查 <code>Proposal</code> 的提交和丢弃工作的这一步操作。</p><h3 id="2-3-数据同步"><a href="#2-3-数据同步" class="headerlink" title="2.3 数据同步"></a>2.3 数据同步</h3><p>​    <code>Leader</code> 服务器会为每一个 <code>Follower</code> 服务器都准备一个队列，并将那些没有被各 <code>Follower</code> 服务器同步的事务以  <code>Proposal</code> 消息的形式逐个发送给 <code>Follower</code> 服务器，并在每一个 <code>Proposal</code> 消息后面紧接着再发送一个 <code>Commit</code>  消息，以表示该事务已经被提交。等到 <code>Follower</code> 服务器将所有其尚未同步的事务 <code>Proposal</code> 都从 <code>Leader</code>  服务器上同步过来并成功应用到本地数据库中后， <code>Leader</code> 服务器就会将该 <code>Follower</code> 服务器加入到真正的可用 <code>Follower</code>  列表中，并开始之后的其他流程。</p><p>​    下面来看 <code>ZAB</code> 协议是如何处理那些需要被丢弃的事务 <code>Proposal</code> 的。在 <code>ZAB</code> 协议的事务编号 <code>ZXID</code> 设计中， <code>ZXID</code>  是一个 <code>64</code> 位的数字，低 <code>32</code> 位可以看作是一个简单的单调递增的计数器，针对客户端的每一个事务请求， <code>Leader</code> 服务器在产生一个新的事务  <code>Proposal</code> 的时候，都会对该计数器进行加 <code>1</code> 操作；高 <code>32</code> 位代表了 <code>Leader</code> 周期 <code>epoch</code> 的编号，每当选举产生一个新的  <code>Leader</code> 服务器，就会从这个 <code>Leader</code> 服务器上取出其本地日志中最大事务 <code>Proposal</code> 的 <code>ZXID</code> ，并从该 <code>ZXID</code>  中解析出对应的 <code>epoch</code> 值，然后再对其进行加 <code>1</code> 操作，之后就会以此编号作为新的 <code>epoch</code>， 并将低 <code>32</code> 位置 <code>0</code> 来开始生成新的  <code>ZXID</code> 。</p><p>​    基于这样的策略，当一个包含了上一个 <code>Leader</code> 周期中尚未提交过的事务 <code>Proposal</code>  的服务器启动加入到集群中，发现此时集群中已经存在 <code>Leader</code>，将自身以 <code>Follower</code> 角色连接上 <code>Leader</code> 服务器之后，<code>Leader</code>  服务器会根据自己服务器上最后被提交的 <code>Proposal</code> 来和 <code>Follower</code> 服务器的 <code>Proposal</code> 进行比对，发现 <code>Follower</code>  中有上一个 <code>Leader</code> 周期的事务 <code>Proposal</code> 时，<code>Leader</code> 会要求 <code>Follower</code>  进行一个回退操作-回退到一个确实已经被集群中过半机器提交的最新的事务 <code>Proposal</code> 。</p><h3 id="2-4-ZAB-协议原理"><a href="#2-4-ZAB-协议原理" class="headerlink" title="2.4 ZAB 协议原理"></a>2.4 <code>ZAB</code> 协议原理</h3><p><code>ZAB</code> 主要包括消息广播和崩溃恢复两个过程，进一步可以分为三个阶段，分别是<strong>发现（<code>Discovery</code>）</strong>、<strong>同步（<code>Synchronization</code>）</strong>、<strong>广播（<code>Broadcast</code>）阶段</strong>。<code>ZAB</code> 的每一个分布式进程会循环执行这三个阶段，称为主进程周期。</p><ol><li>发现<br> 即要求 <code>zookeeper</code> 集群必须选择出一个 <code>Leader</code> 进程，同时 <code>Leader</code> 会维护一个 <code>Follower</code> 可用列表。</li><li>同步<br> <code>Leader</code> 要负责将本身的数据与 <code>Follower</code> 完成同步，做到多副本存储。这样也是体现了 <code>CAP</code> 中高可用和分区容错。<code>Follower</code> 将队列中未处理完的请求消费完成后，写入本地事物日志中。</li><li>广播<br> <code>Leader</code> 可以接受客户端新的 <code>proposal</code> 请求，将新的 <code>proposal</code> 请求广播给所有的 <code>Follower</code>。</li></ol><p>在正常运行过程中，<code>ZAB</code> 协议会一直运行于阶段三来反复进行消息广播流程，如果出现崩溃或其他原因导致 <code>Leader</code> 缺失，那么此时 <code>ZAB</code> 协议会再次进入发现阶段，选举新的 <code>Leader</code>。</p><p>每个进程都有可能处于如下三种状态之一：</p><ul><li><code>LOOKING</code>：<code>Leader</code> 选举阶段。</li><li><code>FOLLOWING</code>：<code>Follower</code> 服务器和 <code>Leader</code> 服务器保持同步状态。</li><li><code>LEADING</code>：<code>Leader</code> 服务器作为主进程领导状态。</li></ul><p>所有进程初始状态都是 <code>LOOKING</code> 状态，此时不存在 <code>Leader</code>，此时，进程会试图选举出一个新的  <code>Leader</code>，之后，如果进程发现已经选举出新的 <code>Leader</code> 了，那么它就会切换到 <code>FOLLOWING</code> 状态，并开始和 <code>Leader</code>  保持同步，处于 <code>FOLLOWING</code> 状态的进程称为 <code>Follower</code>，<code>LEADING</code> 状态的进程称为 <code>Leader</code>，当 <code>Leader</code>  崩溃或放弃领导地位时，其余的 <code>Follower</code> 进程就会转换到 <code>LOOKING</code> 状态开始新一轮的 <code>Leader</code> 选举。</p><p>​    一个 <code>Follower</code> 只能和一个 <code>Leader</code> 保持同步，<code>Leader</code> 进程和所有与所有的 <code>Follower</code>  进程之间都通过心跳检测机制来感知彼此的情况。若 <code>Leader</code> 能够在超时时间内正常收到心跳检测，那么 <code>Follower</code> 就会一直与该  <code>Leader</code> 保持连接，而如果在指定时间内 <code>Leader</code> 无法从过半的 <code>Follower</code> 进程那里接收到心跳检测，或者 <code>TCP</code> 连接断开，那么  <code>Leader</code> 会放弃当前周期的领导，比你转换到 <code>LOOKING</code> 状态，其他的 <code>Follower</code> 也会选择放弃这个 <code>Leader</code>，同时转换到  <code>LOOKING</code> 状态，之后会进行新一轮的 <code>Leader</code> 选举，并在选举产生新的 <code>Leader</code> 之后开始新的一轮主进程周期。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、Zookeeper&quot;&gt;&lt;a href=&quot;#一、Zookeeper&quot; class=&quot;headerlink&quot; title=&quot;一、Zookeeper&quot;&gt;&lt;/a&gt;一、&lt;code&gt;Zookeeper&lt;/code&gt;&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;​    &lt;code&gt;Zookeeper&lt;/code&gt; 致力于提供一个高性能、高可用，且具有严格的顺序访问控制能力(主要是写操作的严格顺序性)的分布式协调服务。高性能使得  &lt;code&gt;Zookeeper&lt;/code&gt;  能够应用于那些对系统吞吐有明确要求的大型分布式系统中，高可用使得分布式的单点问题得到了很好的解决，而严格的顺序访问控制使得客户端能够基于  &lt;code&gt;Zookeeper&lt;/code&gt; 实现一些复杂的同步原语(&lt;code&gt;Zxid&lt;/code&gt;)。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="分布式算法" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="分布式算法" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>分布式算法1-ACID理论</title>
    <link href="http://yoursite.com/2019/10/01/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%951-ACID%E7%90%86%E8%AE%BA/"/>
    <id>http://yoursite.com/2019/10/01/分布式算法1-ACID理论/</id>
    <published>2019-10-01T09:11:02.000Z</published>
    <updated>2020-11-16T04:52:27.331Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ACID理论概念"><a href="#ACID理论概念" class="headerlink" title="ACID理论概念"></a><code>ACID</code>理论概念</h2><p>​    <code>ACID</code>理论是对事务特性的抽象和总结，方便我们实现事务。你可以理解成：如果实现了操作的 <code>ACID</code>特性，那么就实现了事务。而大多数人觉得比较难，是因为分布式系统涉及多个节点间的操作。加锁、时间序列等机制，只能保证单个节点上操作的<code>ACID</code>特性，无法保证节点间操作的<code>ACID</code> 特性。</p><p>​    如果我们想掌握<code>ACID</code>理论，就要掌握分布式事务协议，比如二阶段提交协议和<code>TCC</code>（<code>Try-Confirm-Cancel</code>）。</p><a id="more"></a><h2 id="ACID理论释义"><a href="#ACID理论释义" class="headerlink" title="ACID理论释义"></a><code>ACID</code>理论释义</h2><h3 id="原子性（Atomicity）"><a href="#原子性（Atomicity）" class="headerlink" title="原子性（Atomicity）"></a>原子性（<code>Atomicity</code>）</h3><p>​    <strong>原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚。</strong>因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。</p><h3 id="一致性（Consistency）"><a href="#一致性（Consistency）" class="headerlink" title="一致性（Consistency）"></a>一致性（<code>Consistency</code>）</h3><p>​    <strong>一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。</strong><br>​    拿转账来说，假设用户<code>A</code>和用户<code>B</code>两者的钱加起来一共是<code>5000</code>，那么不管<code>A</code>和<code>B</code>之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是<code>5000</code>，这就是事务的一致性。</p><h3 id="隔离性（Isolation）"><a href="#隔离性（Isolation）" class="headerlink" title="隔离性（Isolation）"></a>隔离性（<code>Isolation</code>）</h3><p>​    <strong>隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。</strong><br>​    即要达到这么一种效果：对于任意两个并发的事务<code>T1</code>和<code>T2</code>，在事务<code>T1</code>看来，<code>T2</code>要么在<code>T1</code>开始之前就已经结束，要么在<code>T1</code>结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。</p><h3 id="持久性（Durability）"><a href="#持久性（Durability）" class="headerlink" title="持久性（Durability）"></a>持久性（<code>Durability</code>）</h3><p>​    <strong>持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。</strong><br>​    例如我们在使用<code>JDBC</code>操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们程序执行完成直到看到提示后，就可以认定事务以及正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成，否则就会造成我们看到提示事务处理完毕，但是数据库因为故障而没有执行事务的重大错误。</p><h2 id="二阶段提交协议"><a href="#二阶段提交协议" class="headerlink" title="二阶段提交协议"></a>二阶段提交协议</h2><p>​    <strong><code>2PC</code>，就是二阶段提交协议。</strong>顾名思义，就是将事务的提交过程分成两个阶段来处理。</p><h3 id="请求提交事务"><a href="#请求提交事务" class="headerlink" title="请求提交事务"></a>请求提交事务</h3><h4 id="阶段一：请求阶段-表决"><a href="#阶段一：请求阶段-表决" class="headerlink" title="阶段一：请求阶段(表决)"></a>阶段一：请求阶段(表决)</h4><ul><li><p><strong><code>提交事务请求</code></strong></p><p>协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应。</p></li><li><p><strong><code>执行事务</code></strong></p><p>各个参与者节点执行事务操作。参与者在本地执行事务，写本地的<code>redo</code>和<code>undo</code>日志，但不提交，到达一种”万事俱备，只欠东风”的状态。</p></li><li><p><strong><code>各参与者向协调者反馈事务询问的响应</code></strong></p><p>如果参与者成功执行了事务操作，那么就反馈给协调者<code>Yes</code>响应，表示事务可以执行；如果参与者没有成功执行事务，那么就反馈给协调者<code>No</code>响应，表示事务不可以执行。</p></li></ul><h4 id="阶段二：提交阶段-执行"><a href="#阶段二：提交阶段-执行" class="headerlink" title="阶段二：提交阶段(执行)"></a>阶段二：提交阶段(执行)</h4><ul><li><p><strong><code>执行事务提交</code></strong></p><p>假如协调者从所有参与者获得的响应都为<code>Yes</code>，那么就执行事务提交，协调者向所有参与者节点发出<code>Commit</code>请求。</p></li><li><p><strong><code>参与者提交事务</code></strong>  </p><p>参与者接收到<code>Commit</code>请求后，会正式执行事务提交操作，并在完成提交之后释放在整个事务执行期间占用的事务资源,并向协调者发送<code>Ack</code>消息。</p></li><li><p><strong><code>完成事务</code></strong></p><p>协调者接收到所有参与者反馈的<code>Ack</code>消息后，完成事务。</p></li></ul><p>其中的过程如下图所示：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20180806102931660.png" style="zoom:80%;"></p><h3 id="中断事务"><a href="#中断事务" class="headerlink" title="中断事务"></a>中断事务</h3><p>​    假如任何一个参与者反馈给协调者<code>No</code>响应，或者在等待超时之后，协调者没有收到所有参与者的响应，那么就会中断事务。</p><ul><li><p><strong><code>发送回滚请求</code></strong></p><p>协调者向所有参与者发送<code>Rollback</code>请求。</p></li><li><p><strong><code>事务回滚</code></strong></p><p>参与者接收到<code>Rollback</code>请求后，会利用其在阶段一中记录的<code>Undo</code>信息来执行事务回滚操作，完成回滚之后释放在整个事务执行期间占用的资源，并向协调者发送<code>Ack</code>消息，协调者接收到所有的参与者反馈的<code>Ack</code>消息后，完成事务中断。</p></li></ul><p>中断的过程就是这样，下面一张时序图，描述中断事务的过程。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20180806103331402.png" style="zoom: 80%;"></p><h3 id="二阶段提交的缺点"><a href="#二阶段提交的缺点" class="headerlink" title="二阶段提交的缺点"></a>二阶段提交的缺点</h3><h4 id="同步阻塞问题。"><a href="#同步阻塞问题。" class="headerlink" title="同步阻塞问题。"></a>同步阻塞问题。</h4><p>​    执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。</p><h4 id="单点故障。"><a href="#单点故障。" class="headerlink" title="单点故障。"></a>单点故障。</h4><p>​    由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）</p><h4 id="数据不一致。"><a href="#数据不一致。" class="headerlink" title="数据不一致。"></a>数据不一致。</h4><p>​    在二阶段提交的阶段二中，当协调者向参与者发送<code>commit</code>请求之后，发生了局部网络异常或者在发送<code>commit</code>请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了<code>commit</code>请求。<br>​    而在这部分参与者接到<code>commit</code>请求之后就会执行<code>commit</code>操作。但是其他部分未接到<code>commit</code>请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据不一致性的现象。</p><h3 id="二阶段提交无法解决的问题"><a href="#二阶段提交无法解决的问题" class="headerlink" title="二阶段提交无法解决的问题"></a>二阶段提交无法解决的问题</h3><p>​    当协调者出错，同时参与者也出错时，两阶段无法保证事务执行的完整性。<br>​    考虑协调者在发出<code>commit</code>消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。</p><h2 id="TCC（Try-Confirm-Cancel）"><a href="#TCC（Try-Confirm-Cancel）" class="headerlink" title="TCC（Try-Confirm-Cancel）"></a><code>TCC</code>（<code>Try-Confirm-Cancel</code>）</h2><p>​    <code>TCC</code>将事务提交分为<code>Try-Confirm-Cancel 3</code>个操作。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Try：预留业务资源/数据效验</span><br><span class="line">Confirm：确认执行业务操作</span><br><span class="line">Cancel：取消执行业务操作</span><br></pre></td></tr></table></figure><p>​    <code>TCC</code>本质上是补偿事务，它的核心思想是<strong>针对每个操作都要注册一个与其对应的确认操作和补偿操作（也就是撤销操作）。</strong> 它是一个业务层面的协议，你也可以将<code>TCC</code>理解为编程模型，<code>TCC</code>的<code>3</code>个操作是需要在业务代码中编码实现的，为了实现一致性，确认操作和补偿操作必须是等幂的，因为这<code>2</code> 个操作可能会失败重试。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20181226111719813.png" style="zoom:67%;"></p><p>​    <code>TCC</code>不依赖于数据库的事务，而是在业务中实现了分布式事务，这样能减轻数据库的压力，但对业务代码的入侵性也更强，实现的复杂度也更高。所以，我推荐在需要分布式事务能力时，优先考虑现成的事务型数据库（比如 <code>MySQL XA</code>），当现有的事务型数据库不能满足业务的需求时，再考虑基于<code>TCC</code>实现分布式事务。</p><h3 id="TCC优缺点"><a href="#TCC优缺点" class="headerlink" title="TCC优缺点"></a><code>TCC</code>优缺点</h3><h4 id="TCC优点"><a href="#TCC优点" class="headerlink" title="TCC优点"></a><code>TCC</code>优点</h4><p>​    让应用自己定义数据库操作的粒度，使得降低锁冲突、提高吞吐量成为可能。</p><h4 id="TCC不足之处"><a href="#TCC不足之处" class="headerlink" title="TCC不足之处"></a><code>TCC</code>不足之处</h4><ul><li>对应用的侵入性强。业务逻辑的每个分支都需要实现<code>try</code>、<code>confirm</code>、<code>cancel</code>三个操作，应用侵入性较强，改造成本高。</li><li>实现难度较大。需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。为了满足一致性的要求，<code>confirm</code>和<code>cancel</code>接口必须实现幂等。</li></ul><h3 id="TCC事务应用场景"><a href="#TCC事务应用场景" class="headerlink" title="TCC事务应用场景"></a><code>TCC</code>事务应用场景</h3><p>​    我们通过用户下单使用<code>余额+红包支付</code>来看一下<code>TCC</code>事务的具体应用。</p><p>​    假设用户下单操作来自<code>3</code>个系统下单系统、资金账户系统、红包账户系统，下单成功需要同时调用资金账户服务和红包服务完成支付</p><p>假设购买商品<code>1000</code>元，使用账户红包<code>200</code>元，余额<code>800</code>元，确认支付。</p><ul><li><strong><code>Try</code>操作</strong><br> <code>tryX</code>下单系统创建待支付订单<br><code>tryY</code>冻结账户红包<code>200</code>元<br> <code>tryZ</code>冻结资金账户<code>800</code>元</li><li><strong><code>Confirm</code>操作</strong><br> <code>confirmX</code>订单更新为支付成功<br> <code>confirmY</code>扣减账户红包<code>200</code>元<br> <code>confirmZ</code>扣减资金账户<code>800</code>元</li><li><strong><code>Cancel</code>操作</strong><br><code>cancelX</code>订单处理异常，资金红包退回，订单支付失败<br> <code>cancelY</code>冻结红包失败，账户余额退回，订单支付失败<br> <code>cancelZ</code>冻结余额失败，账户红包退回，订单支付失败</li></ul><h2 id="幂等性"><a href="#幂等性" class="headerlink" title="幂等性"></a>幂等性</h2><p>​    在分布式系统中确切地保证消息传递是不可能的。实现一次准确的消息传递完全基于消息生成者从消息使用者那里接收到确认消息。但是，<code>ACK</code>本身是不可靠的，因为它也将通过网络传播。在处理消息后，由于网络问题或消费者崩溃，很可能会丢失<code>ACK</code>。</p><p>任何通过网络进行通信，都可能会出现三种故障情形：</p><ul><li>该请求尚未到达服务端</li><li>请求已到达服务端，但在处理期间出现异常</li><li>服务端处理了请求，但响应丢失了</li></ul><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/2019090615551468.png" alt></p><p>​    那么如何才能实现准确的消息传递？它的答案是幂等性！你必须使你的消费者操作具有幂等性。<strong>幂等性就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。</strong>举个最简单的例子，用户购买商品后支付，支付扣款成功，但是返回结果的时候网络异常，此时钱已经扣了，用户再次点击按钮，此时不应该进行第二次扣款。</p><p>添加幂等性处理方法：</p><ul><li>唯一标识。你可以生成唯一标识符并将其添加到业务中。这样，如果在服务提供端存储该<code>ID</code>，则可以轻松发现重复调用。</li><li>利用工作流引擎，比如<code>Apache ActiveMQ</code>可以在生产者向代理发送消息时过滤掉重复项，代理索引可以跟踪和识别重复项并将其丢弃。</li><li>请求哈希。如果你使用消息传递，则可以通过存储消息的哈希值来执行相同的操作。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​    我们可以将<code>ACID</code>特性理解为<code>CAP</code>中一致性的边界，最强的一致性。<code>2PC</code>用在集群间一致性数据同步，所有参与者完成的是同一件事，可以理解为它们在一个<code>start transaction--commit</code>里面，具有强一致性；而<code>TCC</code>是对业务过程的拆分，数据是最终一致。</p><p>​    根据<code>CAP</code>理论，如果在分布式系统中实现了一致性，可用性必然受到影响。比如，如果出现一个节点故障，则整个分布式事务的执行都是失败的。实际上，绝大部分场景对一致性要求没那么高，短暂的不一致是能接受的，另外，也基于可用性和并发性能的考虑，<strong>建议在开发实现分布式系统，如果不是必须，尽量不要实现事务，可以考虑采用最终一致性。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ACID理论概念&quot;&gt;&lt;a href=&quot;#ACID理论概念&quot; class=&quot;headerlink&quot; title=&quot;ACID理论概念&quot;&gt;&lt;/a&gt;&lt;code&gt;ACID&lt;/code&gt;理论概念&lt;/h2&gt;&lt;p&gt;​    &lt;code&gt;ACID&lt;/code&gt;理论是对事务特性的抽象和总结，方便我们实现事务。你可以理解成：如果实现了操作的 &lt;code&gt;ACID&lt;/code&gt;特性，那么就实现了事务。而大多数人觉得比较难，是因为分布式系统涉及多个节点间的操作。加锁、时间序列等机制，只能保证单个节点上操作的&lt;code&gt;ACID&lt;/code&gt;特性，无法保证节点间操作的&lt;code&gt;ACID&lt;/code&gt; 特性。&lt;/p&gt;
&lt;p&gt;​    如果我们想掌握&lt;code&gt;ACID&lt;/code&gt;理论，就要掌握分布式事务协议，比如二阶段提交协议和&lt;code&gt;TCC&lt;/code&gt;（&lt;code&gt;Try-Confirm-Cancel&lt;/code&gt;）。&lt;/p&gt;
    
    </summary>
    
      <category term="分布式算法" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="分布式算法" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>分布式算法10-一致性协议之一致性hash算法</title>
    <link href="http://yoursite.com/2019/10/01/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%9510-%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2019/10/01/分布式算法10-一致性哈希算法/</id>
    <published>2019-10-01T09:11:02.000Z</published>
    <updated>2020-11-16T04:53:14.516Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分布式系统中一致性哈希算法"><a href="#分布式系统中一致性哈希算法" class="headerlink" title="分布式系统中一致性哈希算法"></a>分布式系统中一致性哈希算法</h1><h1 id="业务场景"><a href="#业务场景" class="headerlink" title="业务场景"></a>业务场景</h1><p>​    近年来<code>B2C</code>、<code>O2O</code>等商业概念的提出和移动端的发展，使得分布式系统流行了起来。分布式系统相对于单系统，解决了流量大、系统高可用和高容错等问题。功能强大也意味着实现起来需要更多技术的支持。例如系统访问层的负载均衡，缓存层的多实例主从复制备份，数据层的分库分表等。</p><a id="more"></a><p>​    我们以负载均衡为例，常见的负载均衡方法有很多，但是它们的优缺点也都很明显：</p><ul><li><strong>随机访问策略。</strong>系统随机访问，缺点：可能造成服务器负载压力不均衡，俗话讲就是撑的撑死，饿的饿死。</li><li><strong>轮询策略。</strong>请求均匀分配，如果服务器有性能差异，则无法实现性能好的服务器能够多承担一部分。</li><li><strong>权重轮询策略。</strong>权值需要静态配置，无法自动调节，不适合对长连接和命中率有要求的场景。</li><li><strong>Hash取模策略。</strong>不稳定，如果列表中某台服务器宕机，则会导致路由算法产生变化，由此导致命中率的急剧下降。</li><li><strong>一致性哈希策略。</strong></li></ul><p>以上几个策略，排除本篇介绍的一致性哈希，可能使用最多的就是 Hash取模策略了。Hash取模策略的缺点也是很明显的，这种缺点也许在负载均衡的时候不是很明显，但是在涉及数据访问的主从备份和分库分表中就体现明显了。</p><h1 id="使用Hash取模的问题"><a href="#使用Hash取模的问题" class="headerlink" title="使用Hash取模的问题"></a>使用<code>Hash</code>取模的问题</h1><h2 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h2><p>​    负载均衡时，假设现有<code>3</code>台服务器(编号分别为<code>0</code>、<code>1</code>、<code>2</code>)，使用哈希取模的计算方式则是：对访问者的<code>IP</code>，通过固定算式<code>hash(IP) % N</code>（<code>N</code>为服务器的个数），使得每个<code>IP</code>都可以定位到特定的服务器。</p><p>​    例如现有<code>IP</code>地址 <code>10.58.34.31</code>，对<code>IP</code>哈希取模策时，计算结果为<code>2</code>，即访问编号为<code>2</code>的服务器：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">String ip = <span class="string">"10.58.34.31"</span>;</span><br><span class="line"><span class="keyword">int</span> v1 = hash(ip) % <span class="number">3</span>;</span><br><span class="line">System.out.println(<span class="string">"访问服务器："</span> + v1);<span class="comment">// 访问服务器：2</span></span><br></pre></td></tr></table></figure><p>​    如果此时服务器<code>2</code>宕机了，则会导致所有计算结果为<code>2</code>的 <code>IP</code> 对应的用户都访问异常（包括上例中的<code>IP</code>）。或者你新增了一台服务器<code>3</code>，这时不修改<code>N</code>值的话那么服务器<code>3</code>永远不会被访问到。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111190143.png" alt="img"></p><p>​    当然如果你能动态获取到当前可用服务器的个数，亦即<code>N</code>值是根据当前可用服务器个数动态来变化的，则可解决此问题。但是对于类似要在特定地区或特定<code>IP</code>来访问特定服务器的这种需求就会造成访问偏差。</p><h2 id="分库分表"><a href="#分库分表" class="headerlink" title="分库分表"></a>分库分表</h2><p>​    负载均衡中有这种问题，那么分库分表中同样也有这样的问题。例如随着业务的飞速增长，我们的注册用户也越来越多，单个用户表数量已经达到千万级甚至更大。由于<code>Mysql</code>的单表建议百万级数据存储，所以这时为了保证系统查询和运行效率，肯定会考虑到分库分表。</p><p>​    对于分库分表，数据的分配是个重要的问题，你需要保证数据分配在这个服务器，那么在查询时也需要到该服务器上来查询，否则会造成数据查询丢失的问题。</p><p>​    通常是根据用户的 <code>ID</code> 哈希取模得到的值然后路由到对应的存储位置，计算公式为：<code>hash(userId) % N</code>，其中<code>N</code>为分库或分表的个数。</p><p>​    例如分库数为<code>2</code>时，计算结果为<code>1</code>，则<code>ID</code>为<code>1010</code>的用户存储在编号为<code>1</code>对应的库中：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">String userId = <span class="string">"1010"</span>;</span><br><span class="line"><span class="keyword">int</span> v1 = hash(userId) % <span class="number">2</span>;</span><br><span class="line">System.out.println(<span class="string">"存储："</span> + v1);<span class="comment">// 存储：1</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111190144.png" alt="img"></p><p>​    之后业务数量持续增长，又新增一台用户服务库，当我们根据<code>ID=1010</code>去查询数据时，路由计算方式为：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> v2 = hash(userId) % <span class="number">3</span>;</span><br><span class="line">System.out.println(<span class="string">"存储："</span> + v2);<span class="comment">// 存储：0</span></span><br></pre></td></tr></table></figure><p>​    我们得到的路由值是<code>0</code>，最后的结果就不用说了，存在编号<code>1</code>上的数据我们去编号为<code>0</code>的库上去查询肯定是得不到查询结果的。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111190145.png" alt="img"></p><p>​    为了数据可用，你需要做数据迁移，按照新的路由规则对所有用户重新分配存储地址。每次的库或表的数量改变你都需要做一次全部用户信息数据的迁移。不用想这其中的工作量是有多费时费力了。</p><p>​    是否有某种方法，有效解决这种分布式存储结构下动态增加或删除节点所带来的问题，能保证这种不受实例数量变化影响而准确路由到正确的实例上的算法或实现机制呢？解决这些问题，一致性哈希算法诞生了。</p><h1 id="基本思想原理"><a href="#基本思想原理" class="headerlink" title="基本思想原理"></a>基本思想原理</h1><blockquote><p>​    一致性哈希算法在<code>1997</code>年由麻省理工学院的<code>Karger</code>等人在解决分布式<code>Cache</code>中提出的，设计目标是为了解决因特网中的热点<code>(Hot  spot)</code>问题，初衷和<code>CARP</code>十分类似。一致性哈希修正了<code>CARP</code>使用的简单哈希算法带来的问题，使得<code>DHT</code>可以在<code>P2P</code>环境中真正得到应用。</p></blockquote><p>​    上面说的哈希取模方法，它是针对一个点的，业务布局严重依赖于这个计算的点值结果。你结算的结果是<code>2</code>，那么就对应到编号为<code>2</code>的服务器上。这样的映射就造成了业务容错性和可扩展性极低。</p><p>​    我们思考下，是否可以将这个计算结果的点值赋予范围的意义？我们知道<code>Hash</code>取模之后得到的是一个 <code>int</code> 型的整值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Objects 类中默认的 hash 方法</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">hash</span><span class="params">(Object... values)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Arrays.hashCode(values);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    既然 <code>hash</code>的计算结果是 <code>int</code> 类型，而 <code>java</code> 中 <code>int</code> 的最小值是<code>-2^31</code>，最大值是<code>2^31-1</code>。意味着任何通过哈希取模之后的无符号值都会在 <code>0 ~ 2^31-1</code>范围之间，共<code>2^32</code>个数。那我们是否可以不对服务器的数量进行取模而是直接对<code>2^32</code>取模。这就形成了一致性哈希的基本算法思想，什么意思呢？</p><blockquote><p><strong>这里需要注意一点：</strong>默认的 <code>hash</code> 方法结果是有负值的情况，因此需要我们重写<code>hash</code>方法，保证哈希值的非负性。</p></blockquote><p>​    简单来说，一致性<code>Hash</code>算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数 <code>H</code> 的值空间为 <code>0 ~ 2^32-1</code>（即哈希值是一个<code>32</code>位无符号整形），整个哈希环如下：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111190146.png" alt="img"></p><p>​    整个空间圆按<strong>顺时针</strong>方向布局，圆环的正上方的点代表<code>0</code>，<code>0</code>点右侧的第一个点代表<code>1</code>。以此类推<code>2</code>、<code>3</code>、<code>4</code>、<code>5</code>、<code>6</code>……直到<code>2^32-1</code>，也就是说<code>0</code>点左侧的第一个点代表<code>2^32-1</code>， <code>0</code>和<code>2^32-1</code>在零点中方向重合，我们把这个由<code>2^32</code>个点组成的圆环称为 <strong><code>Hash</code>环</strong>。</p><p>​    那么，一致性哈希算法与上图中的圆环有什么关系呢？仍然以之前描述的场景为例，假设我们有<code>4</code>台服务器，服务器<code>0</code>、服务器<code>1</code>、服务器<code>2</code>，服务器<code>3</code>，那么，在生产环境中，这<code>4</code>台服务器肯定有自己的 <code>IP</code> 地址或主机名，我们使用它们各自的 <code>IP</code> 地址或主机名作为关键字进行哈希计算，使用哈希后的结果对<code>2^32</code>取模，可以使用如下公式示意：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hash（服务器的IP地址） %  <span class="number">2</span>^<span class="number">32</span></span><br></pre></td></tr></table></figure><p>​    最后会得到一个 <code>[0, 2^32-1]</code>之间的一个无符号整形数，这个整数就代表服务器的编号。同时这个整数肯定处于<code>[0, 2^32-1]</code>之间，那么，上图中的 <code>hash</code> 环上必定有一个点与这个整数对应。那么这个服务器就可以映射到这个环上。</p><p>​    多个服务器都通过这种方式进行计算，最后都会各自映射到圆环上的某个点，这样每台机器就能确定其在哈希环上的位置，如下图所示。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111190147.png" alt="img"></p><h1 id="如何提高容错性和扩展性的"><a href="#如何提高容错性和扩展性的" class="headerlink" title="如何提高容错性和扩展性的"></a>如何提高容错性和扩展性的</h1><p>​    那么用户访问，如何分配访问的服务器呢？我们根据用户的 <code>IP</code> 使用上面相同的函数 <code>Hash</code> 计算出哈希值，并确定此数据在环上的位置，从此位置沿环 <strong>顺时针行走</strong>，遇到的第一台服务器就是其应该定位到的服务器。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111190148.png" alt="img"></p><p>​    从上图可以看出 <strong>用户<code>1</code></strong> 顺时针遇到的第一台服务器是 <strong>服务器<code>3</code></strong> ，所以该用户被分配给服务器<code>3</code>来提供服务。同理可以看出用户<code>2</code>被分配给了服务器<code>2</code>。</p><p><strong>1. 新增服务器节点</strong></p><p>​    如果这时需要新增一台服务器节点，一致性哈希策略是如何应对的呢？如下图所示，我们新增了一台服务器<code>4</code>，通过上述一致性哈希算法计算后得出它在哈希环的位置。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111190149.png" alt="img"></p><p>​    可以发现，原来访问服务器<code>3</code>的用户<code>1</code>现在访问的对象是服务器<code>4</code>，用户能正常访问且服务不需要停机就可以自动切换。</p><p><strong>2. 删除服务器节点</strong></p><p>​    如果这时某台服务器异常宕机或者运维撤销了一台服务器，那么这时会发生什么情况呢？如下图所示，假设我们撤销了服务器<code>2</code>。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111190150.png" alt="img"></p><p>​    可以看出，我们服务仍然能正常提供服务，只不过这时用户<code>2</code>会被分配到服务<code>1</code>上了而已。</p><p>​    通过一致性哈希的方式，我们提高了我们系统的容错性和可扩展性，分布式节点的变动不会影响整个系统的运行且不需要我们做一些人为的调整策略。</p><h1 id="Hash环的数据倾斜问题"><a href="#Hash环的数据倾斜问题" class="headerlink" title="Hash环的数据倾斜问题"></a><code>Hash</code>环的数据倾斜问题</h1><p>​    一致性哈希虽然为我们提供了稳定的切换策略，但是它也有一些小缺陷。因为 <code>hash</code>取模算法得到的结果是随机的，我们并不能保证各个服务节点能均匀的分配到哈希环上。</p><p>​    例如当有<code>4</code>个服务节点时，我们把哈希环认为是一个圆盘时钟，我们并不能保证<code>4</code>个服务节点刚好均匀的落在时钟的 <code>12</code>、<code>3</code>、<code>6</code>、<code>9</code>点上。</p><p>​    分布不均匀就会产生一个问题，用户的请求访问就会不均匀，同时<code>4</code>个服务承受的压力就会不均匀。这种问题现象我们称之为，<strong><code>Hash</code>环的数据倾斜问题</strong>。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111190151.png" alt="img"></p><p>​    如上图所示，服务器<code>0</code> 到 服务器<code>1</code> 之间的哈希点值占据比例最大，大量请求会集中到 服务器<code>1</code> 上，而只有极少量会定位到 服务器<code>0</code> 或其他几个节点上，从而出现 <code>hash</code>环偏斜的情况。</p><p>​    如果想要均衡的将缓存分布到每台服务器上，最好能让这每台服务器尽量多的、均匀的出现在<code>hash</code>环上，但是如上图中所示，真实的服务器资源只有<code>4</code>台，我们怎样凭空的让它们多起来呢？</p><p>​    既然没有多余的真正的物理服务器节点，我们就只能将现有的物理节点通过虚拟的方法复制出来。</p><p>​    这些由实际节点虚拟复制而来的节点被称为 <strong>“虚拟节点”</strong>，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器<code>IP</code>或主机名的后面增加编号来实现。</p><p>​    如上图所示，假如 服务器<code>1</code> 的 <code>IP</code> 是 <code>192.168.32.132</code>，那么原 服务器<code>1</code> 节点在环形空间的位置就是<code>hash(&quot;192.168.32.132&quot;) % 2^32</code>。</p><p>​    我们基于 服务器<code>1</code> 构建两个虚拟节点，<code>Server1-A</code> 和 <code>Server1-B</code>，虚拟节点在环形空间的位置可以利用（<code>IP</code>+后缀）计算，例如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hash(<span class="string">"192.168.32.132#A"</span>) % <span class="number">2</span>^<span class="number">32</span></span><br><span class="line">hash(<span class="string">"192.168.32.132#B"</span>) % <span class="number">2</span>^<span class="number">32</span></span><br></pre></td></tr></table></figure><p>​    此时，环形空间中不再有物理节点 服务器<code>1</code>，服务器<code>2</code>，……，替代的是只有虚拟节点 <code>Server1-A</code>，<code>Server1-B</code>，<code>Server2-A</code>，<code>Server2-B</code>，……。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111190152.png" alt="img"></p><p>​    同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到 <code>“Server1-A”</code>、<code>“Server1-B”</code> 两个虚拟节点的数据均定位到 服务器<code>1</code>上。这样就解决了服务节点少时数据倾斜的问题。</p><p>​    在实际应用中，通常将虚拟节点数设置为<code>32</code>甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。由于虚拟节点数量较多，与虚拟节点的映射关系也变得相对均衡了。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>​    一致性哈希一般在分布式缓存中使用的也比较多，本篇只介绍了服务的负载均衡和分布式存储，对于分布式缓存其实原理是类似的，读者可以自己举一反三来思考下。</p><p>​    其实，在分布式存储和分布式缓存中，当服务节点发生变化时（新增或减少），一致性哈希算法并不能杜绝数据迁移的问题，但是可以有效避免数据的全量迁移，需要迁移的只是更改的节点和它的上游节点它们两个节点之间的那部分数据。</p><p>​    另外，我们都知道 <code>hash</code>算法  有一个避免不了的问题，就是哈希冲突。对于用户请求<code>IP</code>的哈希冲突，其实只是不同用户被分配到了同一台服务器上，这个没什么影响。但是如果是服务节点有哈希冲突呢？这会导致两个服务节点在哈希环上对应同一个点，其实我感觉这个问题也不大，因为一方面哈希冲突的概率比较低，另一方面我们可以通过虚拟节点也可减少这种情况。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;分布式系统中一致性哈希算法&quot;&gt;&lt;a href=&quot;#分布式系统中一致性哈希算法&quot; class=&quot;headerlink&quot; title=&quot;分布式系统中一致性哈希算法&quot;&gt;&lt;/a&gt;分布式系统中一致性哈希算法&lt;/h1&gt;&lt;h1 id=&quot;业务场景&quot;&gt;&lt;a href=&quot;#业务场景&quot; class=&quot;headerlink&quot; title=&quot;业务场景&quot;&gt;&lt;/a&gt;业务场景&lt;/h1&gt;&lt;p&gt;​    近年来&lt;code&gt;B2C&lt;/code&gt;、&lt;code&gt;O2O&lt;/code&gt;等商业概念的提出和移动端的发展，使得分布式系统流行了起来。分布式系统相对于单系统，解决了流量大、系统高可用和高容错等问题。功能强大也意味着实现起来需要更多技术的支持。例如系统访问层的负载均衡，缓存层的多实例主从复制备份，数据层的分库分表等。&lt;/p&gt;
    
    </summary>
    
      <category term="分布式算法" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="分布式算法" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>分布式算法7-一致性协议之Paxos</title>
    <link href="http://yoursite.com/2019/10/01/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%957-%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%B9%8BPaxos/"/>
    <id>http://yoursite.com/2019/10/01/分布式算法7-一致性协议之Paxos/</id>
    <published>2019-10-01T09:11:02.000Z</published>
    <updated>2020-11-16T04:52:49.298Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Paxos是什么"><a href="#Paxos是什么" class="headerlink" title="Paxos是什么"></a><code>Paxos</code>是什么</h2><blockquote><p>​    <code>Paxos</code>算法是基于<strong>消息传递</strong>且具有<strong>高度容错特性</strong>的<strong>一致性算法</strong>，是目前公认的解决<strong>分布式一致性</strong>问题最有效的算法之一。</p></blockquote><p>​    <code>Google Chubby</code>的作者<code>Mike Burrows</code>说过这个世界上只有一种一致性算法，那就是<strong><code>Paxos</code></strong>，其它的算法都是<strong>残次品</strong>。虽然<code>Mike  Burrows</code>说得有点夸张，但是至少说明了<code>Paxos</code>算法的地位。然而，<code>Paxos</code>算法也因为晦涩难懂而臭名昭著。</p><a id="more"></a><h2 id="问题产生的背景"><a href="#问题产生的背景" class="headerlink" title="问题产生的背景"></a>问题产生的背景</h2><p>​    在常见的分布式系统中，总会发生诸如<strong>机器宕机</strong>或<strong>网络异常</strong>（包括消息的延迟、丢失、重复、乱序，还有网络分区）等情况。<code>Paxos</code>算法需要解决的问题就是如何在一个可能发生上述异常的分布式系统中，快速且正确地在集群内部对<strong>某个数据的值</strong>达成<strong>一致</strong>，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性。</p><blockquote><p>注：这里<strong>某个数据的值</strong>并不只是狭义上的某个数，它可以是一条日志，也可以是一条命令（<code>command</code>）。。。根据应用场景不同，<strong>某个数据的值</strong>有不同的含义。</p></blockquote><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185850.png" alt="问题产生的背景"></p><h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><p>在<code>Paxos</code>算法中，有三种角色：</p><ul><li><strong><code>Client</code></strong>：客户端，发起请求并等待返回。</li></ul><ul><li><strong><code>Proposer</code>（提案者）</strong>：处理客户端请求，将客户端的请求发送到集群中，以便决定这个值是否可以被批准。</li><li><strong><code>Acceptor</code>（接受者）</strong>：负责处理接收到的提议，他们的回复就是一次投票。会存储一些状态来决定是否接收一个值。</li><li><strong><code>Learners</code>（学习者）</strong>：当有同一个<code>value</code>的协议被超过一半的<code>Acceptor</code>采纳并发送消息给<code>Learners</code>时，<code>Learners</code>采纳该协议值。</li><li><strong><code>Leader</code></strong>：一个特殊的<code>Proposer</code>。</li></ul><p>在具体的实现中，一个进程可能<strong>同时充当多种角色</strong>。比如一个进程可能<strong>既是<code>Proposer</code>又是<code>Acceptor</code>又是<code>Learner</code></strong>。还有一个很重要的概念叫<strong>提案（<code>Proposal</code>）</strong>。最终要达成一致的<code>value</code>就在提案里。</p><blockquote><p><strong>注意：</strong></p><ul><li><strong>暂且</strong>认为『<strong>提案=value</strong>』，即提案只包含<code>value</code>。在我们接下来的推导过程中会发现如果提案只包含<code>value</code>，会有问题，于是我们再对提案<strong>重新设计</strong>。</li><li><strong>暂且</strong>认为『<strong><code>Proposer</code>可以直接提出提案</strong>』。在我们接下来的推导过程中会发现如果<code>Proposer</code>直接提出提案会有问题，需要增加一个学习提案的过程。</li></ul></blockquote><p>​    <code>Proposer</code>可以提出（<code>propose</code>）提案；<code>Acceptor</code>可以接受（<code>accept</code>）提案；如果某个提案被选定（<code>chosen</code>），那么该提案里的<code>value</code>就被选定了。</p><p>​    回到刚刚说的『对某个数据的值达成一致』，指的是<code>Proposer</code>、<code>Acceptor</code>、<code>Learner</code>都认为同一个<code>value</code>被选定（<code>chosen</code>）。那么，<code>Proposer</code>、<code>Acceptor</code>、<code>Learner</code>分别在什么情况下才能认为某个<code>value</code>被选定呢？</p><ul><li><code>Proposer</code>：只要<code>Proposer</code>发的提案被<code>Acceptor</code>接受（刚开始先认为只需要一个<code>Acceptor</code>接受即可，在推导过程中会发现需要半数以上的<code>Acceptor</code>同意才行），<code>Proposer</code>就认为该提案里的<code>value</code>被选定了。</li><li><code>Acceptor</code>：只要<code>Acceptor</code>接受了某个提案，<code>Acceptor</code>就任务该提案里的<code>value</code>被选定了。</li><li><code>Learner</code>：<code>Acceptor</code>告诉<code>Learner</code>哪个<code>value</code>被选定，<code>Learner</code>就认为那个<code>value</code>被选定。</li></ul><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185851.png" alt="相关概念"></p><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>​    假设有一组可以<strong>提出（<code>propose</code>）<code>value</code></strong>（<code>value</code>在提案<code>Proposal</code>里）的<strong>进程集合</strong>。一个一致性算法需要保证提出的这么多<code>value</code>中，<strong>只有一个</strong><code>value</code>被选定（<code>chosen</code>）。如果没有<code>value</code>被提出，就不应该有<code>value</code>被选定。如果一个<code>value</code>被选定，那么所有进程都应该能<strong>学习（<code>learn</code>）</strong>到这个被选定的<code>value</code>。对于一致性算法，<strong>安全性（<code>safaty</code>）</strong>要求如下：</p><ul><li>只有被提出的<code>value</code>才能被选定。</li><li>只有一个<code>value</code>被选定。</li><li>如果某个进程认为某个<code>value</code>被选定了，那么这个<code>value</code>必须是真的被选定的那个。</li></ul><p>我们不去精确地定义其<strong>活性（<code>liveness</code>）</strong>要求。我们的目标是保证<strong>最终有一个提出的<code>value</code>被选定</strong>。当一个<code>value</code>被选定后，进程最终也能学习到这个<code>value</code>。</p><blockquote><p><code>Paxos</code>的目标：保证最终有一个<code>value</code>会被选定，当<code>value</code>被选定后，进程最终也能获取到被选定的<code>value</code>。</p></blockquote><p>假设不同角色之间可以通过发送消息来进行通信，那么：</p><ul><li>每个角色以任意的速度执行，可能因出错而停止，也可能会重启。一个<code>value</code>被选定后，所有的角色可能失败然后重启，除非那些失败后重启的角色能记录某些信息，否则等他们重启后无法确定被选定的值。</li><li>消息在传递过程中可能出现任意时长的延迟，可能会重复，也可能丢失。但是消息不会被损坏，即消息内容不会被篡改（拜占庭将军问题）。</li></ul><h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><h3 id="最简单的方案—只有一个Acceptor"><a href="#最简单的方案—只有一个Acceptor" class="headerlink" title="最简单的方案—只有一个Acceptor"></a>最简单的方案—只有一个<code>Acceptor</code></h3><p>​    假设只有一个<code>Acceptor</code>（可以有多个<code>Proposer</code>），只要<code>Acceptor</code>接受它收到的第一个提案，则该提案被选定，该提案里的<code>value</code>就是被选定的<code>value</code>。这样就保证只有一个<code>value</code>会被选定。</p><p>​    但是，如果这个唯一的<code>Acceptor</code>宕机了，那么整个系统就<strong>无法工作</strong>了！</p><p>​    因此，必须要有<strong>多个<code>Acceptor</code></strong>！</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185852.png" alt="只有一个Acceptor"></p><h3 id="多个Acceptor"><a href="#多个Acceptor" class="headerlink" title="多个Acceptor"></a>多个<code>Acceptor</code></h3><p>多个<code>Acceptor</code>的情况如下图。那么，如何保证在多个<code>Proposer</code>和多个<code>Acceptor</code>的情况下选定一个<code>value</code>呢？</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185853.png" alt="多个Acceptor"></p><p>下面开始寻找解决方案。</p><p>​    如果我们希望即使只有一个<code>Proposer</code>提出了一个<code>value</code>，该<code>value</code>也最终被选定。</p><p>​    那么，就得到下面的约束：</p><blockquote><p><code>P1</code>：一个<code>Acceptor</code>必须接受它收到的第一个提案。</p></blockquote><p>​    但是，这又会引出另一个问题：如果每个<code>Proposer</code>分别提出不同的<code>value</code>，发给不同的<code>Acceptor</code>。根据<code>P1</code>，<code>Acceptor</code>分别接受自己收到的<code>value</code>，就导致不同的<code>value</code>被选定。出现了不一致。如下图：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185854.png" alt="幻灯片08.png"></p><p>​    刚刚是因为『一个提案只要被一个<code>Acceptor</code>接受，则该提案的<code>value</code>就被选定了』才导致了出现上面不一致的问题。因此，我们需要加一个规定：</p><blockquote><p>规定：一个提案被选定需要被<strong>半数以上</strong>的<code>Acceptor</code>接受</p></blockquote><p>​    这个规定又暗示了：『一个<code>Acceptor</code>必须能够接受不止一个提案！』不然可能导致最终没有<code>value</code>被选定。比如上图的情况。<code>v1</code>、<code>v2</code>、<code>v3</code>都没有被选定，因为它们都只被一个<code>Acceptor</code>的接受。</p><p>​    最开始讲的『<strong>提案=<code>value</code></strong>』已经不能满足需求了，于是重新设计提案，给每个提案加上一个提案编号，表示提案被提出的顺序。令『<strong>提案=提案编号+<code>value</code></strong>』。</p><p>​    虽然允许多个提案被选定，但必须保证所有被选定的提案都具有相同的<code>value</code>值。否则又会出现不一致。</p><p>于是有了下面的约束：</p><blockquote><p><code>P2</code>：如果某个<code>value</code>为<code>v</code>的提案被选定了，那么每个编号更高的被选定提案的<code>value</code>必须也是<code>v</code>。</p></blockquote><p>​    一个提案只有被<code>Acceptor</code>接受才可能被选定，因此我们可以把<code>P2</code>约束改写成对<code>Acceptor</code>接受的提案的约束<code>P2a</code>。</p><blockquote><p><code>P2a</code>：如果某个<code>value</code>为<code>v</code>的提案被选定了，那么每个编号更高的被<code>Acceptor</code>接受的提案的<code>value</code>必须也是<code>v</code>。</p></blockquote><p>​    只要满足了<code>P2a</code>，就能满足<code>P2</code>。</p><p>​    但是，考虑如下的情况：假设总的有<code>5</code>个<code>Acceptor</code>。<code>Proposer2</code>提出<code>[M1,V1]</code>的提案，<code>Acceptor2~5</code>（半数以上）均接受了该提案，于是对于<code>Acceptor2~5</code>和<code>Proposer2</code>来讲，它们都认为<code>V1</code>被选定。<code>Acceptor1</code>刚刚从宕机状态恢复过来（之前<code>Acceptor1</code>没有收到过任何提案），此时<code>Proposer1</code>向<code>Acceptor1</code>发送了<code>[M2,V2]</code>的提案（<code>V2≠V1</code>且<code>M2&gt;M1</code>），对于<code>Acceptor1</code>来讲，这是它收到的第一个提案。根据<code>P1</code>（一个<code>Acceptor</code>必须接受它收到的第一个提案。），<code>Acceptor1</code>必须接受该提案！同时<code>Acceptor1</code>认为<code>V2</code>被选定。这就出现了两个问题：</p><ol><li><code>Acceptor1</code>认为<code>V2</code>被选定，<code>Acceptor2~5</code>和<code>Proposer2</code>认为<code>V1</code>被选定。出现了不一致。</li><li><code>V1</code>被选定了，但是编号更高的被<code>Acceptor1</code>接受的提案<code>[M2,V2]</code>的<code>value</code>为<code>V2</code>，且<code>V2≠V1</code>。这就跟<code>P2a</code>（如果某个<code>value</code>为<code>v</code>的提案被选定了，那么每个编号更高的被<code>Acceptor</code>接受的提案的<code>value</code>必须也是<code>v</code>）矛盾了。</li></ol><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185855.png" alt="幻灯片10.png"></p><p>​    所以我们要对<code>P2a</code>约束进行强化！</p><p>​    <code>P2a</code>是对<code>Acceptor</code>接受的提案约束，但其实提案是<code>Proposer</code>提出来的，所有我们可以对<code>Proposer</code>提出的提案进行约束。得到<code>P2b</code>：</p><blockquote><p><code>P2b</code>：如果某个<code>value</code>为<code>v</code>的提案被选定了，那么之后任何<code>Proposer</code>提出的编号更高的提案的<code>value</code>必须也是<code>v</code>。</p></blockquote><p>​    由<code>P2b</code>可以推出<code>P2a</code>进而推出<code>P2</code>。</p><p>​    那么，如何确保在某个<code>value</code>为<code>v</code>的提案被选定后，<code>Proposer</code>提出的编号更高的提案的<code>value</code>都是<code>v</code>呢？</p><p>​    只要满足<code>P2c</code>即可：</p><blockquote><p><code>P2c</code>：对于任意的<code>N</code>和<code>V</code>，如果提案<code>[N, V]</code>被提出，那么存在一个半数以上的<code>Acceptor</code>组成的集合<code>S</code>，满足以下两个条件中的任意一个：</p><ul><li><code>S</code>中每个<code>Acceptor</code>都没有接受过编号小于<code>N</code>的提案。</li><li><code>S</code>中<code>Acceptor</code>接受过的最大编号的提案的<code>value</code>为<code>V</code>。</li></ul></blockquote><h3 id="Proposer生成提案"><a href="#Proposer生成提案" class="headerlink" title="Proposer生成提案"></a><code>Proposer</code>生成提案</h3><p>​    为了满足<code>P2b</code>，这里有个比较重要的思想：<code>Proposer</code>生成提案之前，应该先去<strong>『学习』</strong>已经被选定或者可能被选定的<code>value</code>，然后以该<code>value</code>作为自己提出的提案的<code>value</code>。如果没有<code>value</code>被选定，<code>Proposer</code>才可以自己决定<code>value</code>的值。这样才能达成一致。这个学习的阶段是通过一个<strong>『<code>Prepare</code>请求』</strong>实现的。</p><p>于是我们得到了如下的<strong>提案生成算法</strong>：</p><ol><li><p><code>Proposer</code>选择一个<strong>新的提案编号<code>N</code></strong>，然后向<strong>某个<code>Acceptor</code>集合</strong>（半数以上）发送请求，要求该集合中的每个<code>Acceptor</code>做出如下响应（<code>response</code>）。<br> (a) 向<code>Proposer</code>承诺保证<strong>不再接受</strong>任何编号<strong>小于<code>N</code>的提案</strong>。<br> (b) 如果<code>Acceptor</code>已经接受过提案，那么就向<code>Proposer</code>响应<strong>已经接受过</strong>的编号小于<code>N</code>的<strong>最大编号的提案</strong>。</p><p>我们将该请求称为<strong>编号为<code>N</code></strong>的<strong><code>Prepare</code>请求</strong>。</p></li><li><p>如果<code>Proposer</code>收到了<strong>半数以上</strong>的<code>Acceptor</code>的<strong>响应</strong>，那么它就可以生成编号为<code>N</code>，<code>Value</code>为<code>V</code>的<strong>提案<code>[N,V]</code></strong>。这里的<code>V</code>是所有的响应中<strong>编号最大的提案的<code>Value</code></strong>。如果所有的响应中<strong>都没有提案</strong>，那 么此时<code>V</code>就可以由<code>Proposer</code><strong>自己选择</strong>。<br> 生成提案后，<code>Proposer</code>将该<strong>提案</strong>发送给<strong>半数以上</strong>的<code>Acceptor</code>集合，并期望这些<code>Acceptor</code>能接受该提案。我们称该请求为<strong><code>Accept</code>请求</strong>。（注意：此时接受<code>Accept</code>请求的<code>Acceptor</code>集合<strong>不一定</strong>是之前响应<code>Prepare</code>请求的<code>Acceptor</code>集合）</p></li></ol><h3 id="Acceptor接受提案"><a href="#Acceptor接受提案" class="headerlink" title="Acceptor接受提案"></a><code>Acceptor</code>接受提案</h3><p>​    <code>Acceptor</code><strong>可以忽略任何请求</strong>（包括<code>Prepare</code>请求和<code>Accept</code>请求）而不用担心破坏算法的<strong>安全性</strong>。因此，我们这里要讨论的是什么时候<code>Acceptor</code>可以响应一个请求。</p><p>​    我们对<code>Acceptor</code>接受提案给出如下约束：</p><blockquote><p><code>P1a</code>：一个<code>Acceptor</code>只要尚<strong>未响应过</strong>任何<strong>编号大于<code>N</code></strong>的<strong><code>Prepare</code>请求</strong>，那么他就可以<strong>接受</strong>这个<strong>编号为<code>N</code>的提案</strong>。</p></blockquote><p>​    如果<code>Acceptor</code>收到一个编号为<code>N</code>的<code>Prepare</code>请求，在此之前它已经响应过编号大于<code>N</code>的<code>Prepare</code>请求。根据<code>P1a</code>，该<code>Acceptor</code>不可能接受编号为<code>N</code>的提案。因此，该<code>Acceptor</code>可以忽略编号为<code>N</code>的<code>Prepare</code>请求。当然，也可以回复一个<code>error</code>，让<code>Proposer</code>尽早知道自己的提案不会被接受。</p><p>​    因此，一个<code>Acceptor</code><strong>只需记住</strong>：1. 已接受的编号最大的提案；2. 已响应的请求的最大编号。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185856.jpg" alt="小优化"></p><h3 id="Paxos算法描述"><a href="#Paxos算法描述" class="headerlink" title="Paxos算法描述"></a><code>Paxos</code>算法描述</h3><p>​    经过上面的推导，我们总结下<code>Paxos</code>算法的流程。<code>Paxos</code>算法分为<strong>两个阶段</strong>，是一个个经典<strong>两阶段提交（<code>2PC</code>）</strong>。会经历<code>Prepare-&gt;Promise-&gt;Propose-&gt;Accept</code>四个阶段信息的交互。具体如下：</p><h4 id="阶段一"><a href="#阶段一" class="headerlink" title="阶段一"></a>阶段一</h4><p>​    (a) <code>Proposer</code>选择一个<strong>提案编号<code>N</code></strong>，然后向<strong>半数以上</strong>的<code>Acceptor</code>发送编号为<code>N</code>的<strong><code>Prepare</code>请求</strong>。</p><p>​    (b) 如果一个<code>Acceptor</code>收到一个编号为<code>N</code>的<code>Prepare</code>请求，且<code>N</code><strong>大于</strong>该<code>Acceptor</code>已经<strong>响应过的</strong>所有<strong><code>Prepare</code>请求</strong>的编号，那么它就会将它已经<strong>接受过的编号最大的提案（如果有的话）</strong>作为响应反馈给<code>Proposer</code>，同时该<code>Acceptor</code>承诺<strong>不再接受</strong>任何<strong>编号小于<code>N</code>的提案</strong>。</p><h4 id="阶段二"><a href="#阶段二" class="headerlink" title="阶段二"></a>阶段二</h4><p>​    (a) 如果<code>Proposer</code>收到<strong>半数以上</strong><code>Acceptor</code>对其发出的编号为<code>N</code>的<code>Prepare</code>请求的<strong>响应</strong>，那么它就会发送一个针对<strong><code>[N,V]</code>提案</strong>的<strong><code>Accept</code>请求</strong>给<strong>半数以上</strong>的<code>Acceptor</code>。注意：<code>V</code>就是收到的<strong>响应</strong>中<strong>编号最大的提案的<code>value</code></strong>，如果响应中<strong>不包含任何提案</strong>，那么<code>V</code>就由<code>Proposer</code><strong>自己决定</strong>。</p><p>​        (b) 如果<code>Acceptor</code>收到一个针对编号为<code>N</code>的提案的<code>Accept</code>请求，只要该<code>Acceptor</code><strong>没有</strong>对编号<strong>大于<code>N</code></strong>的<strong><code>Prepare</code>请求</strong>做出过<strong>响应</strong>，它就<strong>接受该提案</strong>。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185857.png" alt="Paxos算法流程"></p><p>例如，以下五个节点的提案过程变化如下</p><ul><li><code>S1</code>首先发起<code>accept(1,red)</code>，并在<code>S1</code>,<code>S2</code>和<code>S3</code>达成多数派，<code>red</code>在<code>S1</code>，<code>S2</code>和<code>S3</code>上持久化</li><li>随后<code>S5</code>发起提案<code>(5,blue)</code>，在<code>S3</code>，<code>S4</code>和<code>S5</code>达成多数派，但是<code>S5</code>在第二阶段时会受到<code>S3</code>发送的<code>response</code>信息，其中包含提案信息<code>(1,red)</code>，则<code>S5</code>最终会<code>accept(5,red)</code>的提案信息给<code>accepter</code>。最终<code>red</code>在<code>S3</code>，<code>S4</code>和<code>S5</code>持久化。</li></ul><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185858.png" alt="img"></p><h2 id="Learner学习被选定的value"><a href="#Learner学习被选定的value" class="headerlink" title="Learner学习被选定的value"></a><code>Learner</code>学习被选定的<code>value</code></h2><p><code>Learner</code>学习（获取）被选定的<code>value</code>有如下三种方案：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185859.png" alt="幻灯片17.png"></p><h2 id="如何保证Paxos算法的活性"><a href="#如何保证Paxos算法的活性" class="headerlink" title="如何保证Paxos算法的活性"></a>如何保证<code>Paxos</code>算法的活性</h2><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185900.png" alt="幻灯片18.png"></p><p>​    通过选取<strong>主<code>Proposer</code></strong>，就可以保证<code>Paxos</code>算法的活性。<code>Paxos</code>中<code>leader Proposer</code>用于避免活锁(例如<code>4</code>个<code>Proposer</code>，<code>2</code>个提议<code>A</code>，<code>2</code>个提议<code>B</code>不能达成一致，导致活锁)，如果<code>leader Proposer</code>稳定，不需要阶段一的<code>prepare</code>步骤，如下图（图中<code>Proposer</code>就是一个<code>Leader</code>）：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185901.png" alt="img"></p><p>​    但<code>leader</code>的存在会带来其他问题，一是如何选举和保持唯一<code>leader</code>(虽然无<code>leader</code>或多<code>leader</code>不影响一致性，但影响决议进程<code>progress</code>)；二是充当<code>leader</code>的节点会承担更多压力，如何均衡节点的负载。租约(<code>lease</code>)可以帮助实现唯一<code>leader</code>，但<code>leader</code>故障情况下可导致服务短期不可用。</p><p>​    至此，我们得到一个<strong>既能保证安全性，又能保证活性</strong>的<strong>分布式一致性算法</strong>——<strong><code>Paxos</code>算法</strong>。</p><h2 id="Muti-Paxos在google-chubby中的应用"><a href="#Muti-Paxos在google-chubby中的应用" class="headerlink" title="Muti-Paxos在google chubby中的应用"></a><code>Muti-Paxos</code>在<code>google chubby</code>中的应用</h2><p>​    <code>Muti-Paxos</code>即是带有<code>leader preposer</code>的集群，其内只有<code>leader</code>才能提出提案。</p><p>​    <code>Google Chubby</code>是一个高可用分布式锁服务，被设计成一个需要访问中心化节点的分布式锁服务。本文只分析<code>chubby</code>服务端的实现。</p><p> <code>Chubby</code>服务端的基本架构大致分为三层</p><p>　　① 最底层是容错日志系统（<code>Fault-Tolerant Log</code>），通过<code>Paxos</code>算法能够保证集群所有机器上的日志完全一致，同时具备较好的容错性。</p><p>　　② 日志层之上是<code>Key-Value</code>类型的容错数据库（<code>Fault-Tolerant DB</code>），其通过下层的日志来保证一致性和容错性。</p><p>　　③ 存储层之上的就是<code>Chubby</code>对外提供的分布式锁服务和小文件存储服务。</p><p> <img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185902.png" alt="img"></p><p>​    <code>Paxos</code>算法用于保证集群内各个副本节点的日志能够保持一致，<code>Chubby</code>事务日志（<code>Transaction  Log</code>）中的每一个<code>Value</code>对应<code>Paxos</code>算法中的一个<code>Instance</code>（对应<code>Proposer</code>），由于<code>Chubby</code>需要对外提供不断的服务，因此事务日志会无限增长，于是在整个<code>Chubby</code>运行过程中，会存在多个<code>Paxos Instance</code>，同时，<code>Chubby</code>会为每个<code>Paxos  Instance</code>都按序分配一个全局唯一的<code>Instance</code>编号，并将其顺序写入到事务日志中去。</p><p>​    在<code>Paxos</code>中，每一个<code>Paxos  Instance</code>都需要进行一轮或多轮的<code>Prepare-&gt;Promise-&gt;Propose-&gt;Accept</code>这样完整的二阶段请求过程来完成对一个提议值的选定，为了保证正确性的前提下尽可能地提高算法运行性能，可以让多个<code>Instance</code>共用一套序号分配机制，并将<code>Prepare-&gt;Promise</code>合并为一个阶段。具体做法如下：</p><ul><li>当某个副本节点通过选举成为<code>Master</code>后，就会使用新分配的编号<code>N</code>来广播一个<code>Prepare</code>消息，该<code>Prepare</code>消息会被所有未达成一致的<code>Instance</code>和目前还未开始的<code>Instance</code>共用。</li><li><p>当<code>Acceptor</code>接收到<code>Prepare</code>消息后，必须对多个<code>Instance</code>同时做出回应，这通常可以通过将反馈信息封装在一个数据包中来实现，假设最多允许<code>K</code>个<code>Instance</code>同时进行提议值的选定，那么：</p><ul><li>当前之多存在<code>K</code>个未达成一致的<code>Instance</code>，将这些未决的<code>Instance</code>各自最后接受的提议值封装进一个数据包，并作为<code>Promise</code>消息返回。</li><li>同时，判断<code>N</code>是否大于当前<code>Acceptor</code>的<code>highestPromisedNum</code>值（当前已经接受的最大的提议编号值），如果大于，那么就标记这些未决<code>Instance</code>和所有未开始的<code>Instance</code>的<code>highestPromisedNum</code>的值为<code>N</code>，这样，这些未决<code>Instance</code>和所有未开始<code>Instance</code>都不能再接受任何编号小于<code>N</code>的提议。</li></ul></li><li><p><code>Master</code>对所有未决<code>Instance</code>和所有未开始<code>Instance</code>分别执行<code>Propose-&gt;Accept</code>阶段的处理，如果<code>Master</code>能够一直稳定运行的话，那么在接下来的算法运行过程中，就不再需要进行<code>Prepare-&gt;Promise</code>处理了。但是，一旦<code>Master</code>发现<code>Acceptor</code>返回了一个<code>Reject</code>消息，说明集群中存在另一个<code>Master</code>并且试图使用更大的提议编号发送了<code>Prepare</code>消息，此时，当前<code>Master</code>就需要重新分配新的提议编号并再次进行<code>Prepare-&gt;Promise</code>阶段的处理。</p></li></ul><p>可见<code>chubby</code>就是一个典型的<code>Muti-Paxos</code>算法应用，在<code>Master</code>稳定运行的情况下，只需要使用同一个编号来依次执行每一个<code>Instance</code>的<code>Promise-&gt;Accept</code>阶段处理。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Paxos是什么&quot;&gt;&lt;a href=&quot;#Paxos是什么&quot; class=&quot;headerlink&quot; title=&quot;Paxos是什么&quot;&gt;&lt;/a&gt;&lt;code&gt;Paxos&lt;/code&gt;是什么&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;​    &lt;code&gt;Paxos&lt;/code&gt;算法是基于&lt;strong&gt;消息传递&lt;/strong&gt;且具有&lt;strong&gt;高度容错特性&lt;/strong&gt;的&lt;strong&gt;一致性算法&lt;/strong&gt;，是目前公认的解决&lt;strong&gt;分布式一致性&lt;/strong&gt;问题最有效的算法之一。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;​    &lt;code&gt;Google Chubby&lt;/code&gt;的作者&lt;code&gt;Mike Burrows&lt;/code&gt;说过这个世界上只有一种一致性算法，那就是&lt;strong&gt;&lt;code&gt;Paxos&lt;/code&gt;&lt;/strong&gt;，其它的算法都是&lt;strong&gt;残次品&lt;/strong&gt;。虽然&lt;code&gt;Mike  Burrows&lt;/code&gt;说得有点夸张，但是至少说明了&lt;code&gt;Paxos&lt;/code&gt;算法的地位。然而，&lt;code&gt;Paxos&lt;/code&gt;算法也因为晦涩难懂而臭名昭著。&lt;/p&gt;
    
    </summary>
    
      <category term="分布式算法" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="分布式算法" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>分布式算法4-一致性协议之2PC</title>
    <link href="http://yoursite.com/2019/10/01/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%954-%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%B9%8B2PC/"/>
    <id>http://yoursite.com/2019/10/01/分布式算法4-一致性协议之2PC/</id>
    <published>2019-10-01T09:11:02.000Z</published>
    <updated>2020-11-16T04:52:39.215Z</updated>
    
    <content type="html"><![CDATA[<p>为了使系统尽量能够达到 <code>CAP</code>，于是有了 <code>BASE</code> 协议，而 <code>BASE</code> 协议是在可用性和一致性之间做的取舍和妥协。人们往往需要在系统的可用性和数据一致性之间反复的权衡。于是呢，就产生我们标题中的一致性协议，而且还不止一个呢。</p><p>​    为了解决分布式问题，涌现了很多经典的算法和协议，最著名的就是二阶段提交协议，三阶段提交协议，<code>Paxos</code> 算法。本文重点介绍二阶段提交协议，简称 <code>2PC</code>。</p><a id="more"></a><h2 id="什么是-2PC"><a href="#什么是-2PC" class="headerlink" title="什么是 2PC"></a>什么是 <code>2PC</code></h2><p>​    在分布式系统中，会有多个机器节点，因此需要一个 “协调者” ，而各个节点就是 “参与者”，协调者统一调度所有分布式节点的执行逻辑，这些被调度的分布式节点就是 “参与者”。</p><p>​    协调者最终决定这些参与者是否要把事务真正进行提交。正式基于这个思想，有了二阶段提交和 三阶段提交。</p><p>​    <code>2PC</code> ，不是 <code>2</code> 个 <code>pc</code> 机的意思，而是 <code>Two-Phase Commit</code> 。可以认为是一种算法，也可以认为是一种协议，主要目的就是为了保证分布式系统数据的一致性。</p><p>​    <strong>顾名思义，二阶段提交就是讲事务的提交过程分成了两个阶段来进行处理。</strong></p><h3 id="二阶段提交流程"><a href="#二阶段提交流程" class="headerlink" title="二阶段提交流程"></a>二阶段提交流程</h3><h4 id="阶段一：请求阶段-表决"><a href="#阶段一：请求阶段-表决" class="headerlink" title="阶段一：请求阶段(表决)"></a>阶段一：请求阶段(表决)</h4><ul><li><p><strong><code>提交事务请求</code></strong></p><p>协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应。</p></li><li><p><strong><code>执行事务</code></strong></p><p>各个参与者节点执行事务操作。参与者在本地执行事务，写本地的<code>redo</code>和<code>undo</code>日志，但不提交，到达一种”万事俱备，只欠东风”的状态。</p></li><li><p><strong><code>各参与者向协调者反馈事务询问的响应</code></strong></p><p>如果参与者成功执行了事务操作，那么就反馈给协调者<code>Yes</code>响应，表示事务可以执行；如果参与者没有成功执行事务，那么就反馈给协调者<code>No</code>响应，表示事务不可以执行。</p></li></ul><p>从上面可以感觉到，这个一个 所谓的 “投票阶段”，什么意思呢？所有的节点都投票决定是否执行事务操作。</p><h4 id="阶段二：提交阶段-执行"><a href="#阶段二：提交阶段-执行" class="headerlink" title="阶段二：提交阶段(执行)"></a>阶段二：提交阶段(执行)</h4><ul><li><p><strong><code>执行事务提交</code></strong></p><p>假如协调者从所有参与者获得的响应都为<code>Yes</code>，那么就执行事务提交，协调者向所有参与者节点发出<code>Commit</code>请求。</p></li><li><p><strong><code>参与者提交事务</code></strong>  </p><p>参与者接收到<code>Commit</code>请求后，会正式执行事务提交操作，并在完成提交之后释放在整个事务执行期间占用的事务资源,并向协调者发送<code>Ack</code>消息。</p></li><li><p><strong><code>完成事务</code></strong></p><p>协调者接收到所有参与者反馈的<code>Ack</code>消息后，完成事务。</p></li></ul><p>其中的过程如下图所示：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20180806102931660.png" style="zoom:80%;"></p><h3 id="中断事务"><a href="#中断事务" class="headerlink" title="中断事务"></a>中断事务</h3><p>​    假如任何一个参与者反馈给协调者<code>No</code>响应，或者在等待超时之后，协调者没有收到所有参与者的响应，那么就会中断事务。</p><ul><li><p><strong><code>发送回滚请求</code></strong></p><p>协调者向所有参与者发送<code>Rollback</code>请求。</p></li><li><p><strong><code>事务回滚</code></strong></p><p>参与者接收到<code>Rollback</code>请求后，会利用其在阶段一中记录的<code>Undo</code>信息来执行事务回滚操作，完成回滚之后释放在整个事务执行期间占用的资源，并向协调者发送<code>Ack</code>消息，协调者接收到所有的参与者反馈的<code>Ack</code>消息后，完成事务中断。</p></li></ul><p>中断的过程就是这样，下面一张时序图，描述中断事务的过程。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20180806103331402.png" style="zoom: 80%;"></p><p>从上面的逻辑可以看出，二阶段提交就做了<code>2</code>个事情：<strong>投票</strong>，<strong>执行</strong>。</p><blockquote><p><strong>核心是对每个事务都采用先尝试后提交的处理方式，因此也可以将二阶段提交看成一个强一致性的算法。</strong></p></blockquote><h3 id="二阶段提交的缺点"><a href="#二阶段提交的缺点" class="headerlink" title="二阶段提交的缺点"></a>二阶段提交的缺点</h3><h4 id="同步阻塞问题"><a href="#同步阻塞问题" class="headerlink" title="同步阻塞问题"></a>同步阻塞问题</h4><p>​    执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。</p><h4 id="单点故障"><a href="#单点故障" class="headerlink" title="单点故障"></a>单点故障</h4><p>​    由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）</p><h4 id="数据不一致"><a href="#数据不一致" class="headerlink" title="数据不一致"></a>数据不一致</h4><p>​    在二阶段提交的阶段二中，当协调者向参与者发送<code>commit</code>请求之后，发生了局部网络异常或者在发送<code>commit</code>请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了<code>commit</code>请求。<br>​    而在这部分参与者接到<code>commit</code>请求之后就会执行<code>commit</code>操作。但是其他部分未接到<code>commit</code>请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据不一致性的现象。</p><h4 id="过于保守"><a href="#过于保守" class="headerlink" title="过于保守"></a>过于保守</h4><p>​    如果在二阶段提交的提交询问阶段中，参与者出现故障而导致协调者始终无法获取到所有参与者的响应信息，这时协调者只能依靠其自身的超时机制来判断是否需要中断事务，显然，这种策略过于保守。换句话说，<strong>二阶段提交协议没有设计较为完善的容错机制，任意一个节点是失败都会导致整个事务的失败</strong>。</p><h3 id="二阶段提交无法解决的问题"><a href="#二阶段提交无法解决的问题" class="headerlink" title="二阶段提交无法解决的问题"></a>二阶段提交无法解决的问题</h3><p>​    当协调者出错，同时参与者也出错时，两阶段无法保证事务执行的完整性。<br>​    考虑协调者在发出<code>commit</code>消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​    我们可以将<code>ACID</code>特性理解为<code>CAP</code>中一致性的边界，最强的一致性。<code>2PC</code>用在集群间一致性数据同步，所有参与者完成的是同一件事，可以理解为它们在一个<code>start transaction--commit</code>里面，具有强一致性；而<code>TCC</code>是对业务过程的拆分，数据是最终一致。</p><p>​    根据<code>CAP</code>理论，如果在分布式系统中实现了一致性，可用性必然受到影响。比如，如果出现一个节点故障，则整个分布式事务的执行都是失败的。实际上，绝大部分场景对一致性要求没那么高，短暂的不一致是能接受的，另外，也基于可用性和并发性能的考虑，<strong>建议在开发实现分布式系统，如果不是必须，尽量不要实现事务，可以考虑采用最终一致性。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;为了使系统尽量能够达到 &lt;code&gt;CAP&lt;/code&gt;，于是有了 &lt;code&gt;BASE&lt;/code&gt; 协议，而 &lt;code&gt;BASE&lt;/code&gt; 协议是在可用性和一致性之间做的取舍和妥协。人们往往需要在系统的可用性和数据一致性之间反复的权衡。于是呢，就产生我们标题中的一致性协议，而且还不止一个呢。&lt;/p&gt;
&lt;p&gt;​    为了解决分布式问题，涌现了很多经典的算法和协议，最著名的就是二阶段提交协议，三阶段提交协议，&lt;code&gt;Paxos&lt;/code&gt; 算法。本文重点介绍二阶段提交协议，简称 &lt;code&gt;2PC&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="分布式算法" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="分布式算法" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>分布式算法3-Base理论</title>
    <link href="http://yoursite.com/2019/10/01/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%953-Base%E7%90%86%E8%AE%BA/"/>
    <id>http://yoursite.com/2019/10/01/分布式算法3-Base理论/</id>
    <published>2019-10-01T09:11:02.000Z</published>
    <updated>2020-11-16T04:52:35.838Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>​    在<code>CAP</code> 定理中，我们说，<code>CAP</code> 不可能同时满足，而分区容错是对于分布式系统而言，是必须的。最后，我们说，如果系统能够同时实现 <code>CAP</code> 是再好不过的了，所以出现了 <code>BASE</code> 理论，今天就来讲讲 <code>Base</code> 理论。</p><a id="more"></a><h2 id="什么是-Base-理论"><a href="#什么是-Base-理论" class="headerlink" title="什么是 Base 理论"></a>什么是 <code>Base</code> 理论</h2><blockquote><p><code>BASE</code>：全称：<code>Basically Available</code>(基本可用)，<code>Soft state</code>（软状态）,和 <code>Eventually consistent</code>（最终一致性）三个短语的缩写，来自 <code>ebay</code> 的架构师提出。</p></blockquote><p>​    <code>Base</code> 理论是对 <code>CAP</code> 中一致性和可用性权衡的结果，其来源于对大型互联网分布式实践的总结，是基于 <code>CAP</code> 定理逐步演化而来的。其核心思想是：</p><blockquote><p>​    既是无法做到强一致性（<code>Strong consistency</code>），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（<code>Eventual consistency</code>）。</p></blockquote><h2 id="Basically-Available-基本可用"><a href="#Basically-Available-基本可用" class="headerlink" title="Basically Available(基本可用)"></a><code>Basically Available</code>(基本可用)</h2><p>什么是基<strong>本可用</strong>呢？假设系统，出现了不可预知的故障，但还是能用，相比较正常的系统而言：</p><ol><li>响应时间上的损失：正常情况下的搜索引擎 0.5 秒即返回给用户结果，而<strong>基本可用</strong>的搜索引擎可以在 <code>1</code> 秒作用返回结果。</li><li>功能上的损失：在一个电商网站上，正常情况下，用户可以顺利完成每一笔订单，但是到了大促期间，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。</li></ol><h2 id="Soft-state-软状态"><a href="#Soft-state-软状态" class="headerlink" title="Soft state(软状态)"></a><code>Soft state</code>(软状态)</h2><p>​    什么是软状态呢？相对于原子性而言，要求多个节点的数据副本都是一致的，这是一种 “硬状态”。</p><p>​    <strong>软状态</strong>指的是：允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。</p><h2 id="Eventually-consistent-最终一致性）"><a href="#Eventually-consistent-最终一致性）" class="headerlink" title="Eventually consistent(最终一致性）"></a><code>Eventually consistent</code>(最终一致性）</h2><p>​    这个比较好理解了哈。上面说软状态，不可能一直是软状态，必须有个时间期限。在期限过后，应当保证所有副本保持数据一致性。从而达到数据的<strong>最终一致性</strong>。这个时间期限取决于网络延时，系统负载，数据复制方案设计等等因素。</p><p>​    稍微官方一点的说法就是：</p><blockquote><p>​    系统能够保证在没有其他新的更新操作的情况下，数据最终一定能够达到一致的状态，因此所有客户端对系统的数据访问最终都能够获取到最新的值。</p></blockquote><p>而在实际工程实践中，<strong>最终一致性分为 5 种：</strong></p><h3 id="因果一致性（Causal-consistency）"><a href="#因果一致性（Causal-consistency）" class="headerlink" title="因果一致性（Causal consistency）"></a>因果一致性（Causal consistency）</h3><p>​    <strong>因果一致性</strong>指的是：如果节点 A 在更新完某个数据后通知了节点 B，那么节点 B 之后对该数据的访问和修改都是基于 A 更新后的值。于此同时，和节点 A 无因果关系的节点 C 的数据访问则没有这样的限制。</p><h3 id="读己之所写（Read-your-writes）"><a href="#读己之所写（Read-your-writes）" class="headerlink" title="读己之所写（Read your writes）"></a>读己之所写（Read your writes）</h3><p>​    <strong>这种就很简单了</strong>是指当节点 <code>A</code> 更新一个数据后，它自身总是能访问到自身更新过的最新值，而不会看到旧值。其实也算一种因果一致性。</p><h3 id="会话一致性（Session-consistency）"><a href="#会话一致性（Session-consistency）" class="headerlink" title="会话一致性（Session consistency）"></a>会话一致性（Session consistency）</h3><p>​    <strong>会话一致性</strong>将对系统数据的访问过程框定在了一个会话当中：系统能保证在同一个有效的会话中实现 “读己之所写” 的一致性，也就是说，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。</p><h3 id="单调读一致性（Monotonic-read-consistency）"><a href="#单调读一致性（Monotonic-read-consistency）" class="headerlink" title="单调读一致性（Monotonic read consistency）"></a>单调读一致性（Monotonic read consistency）</h3><p>​    <strong>单调读一致性</strong>是指如果一个节点从系统中读取出一个数据项的某个值后，那么系统对于该节点后续的任何数据访问都不应该返回更旧的值。</p><h3 id="单调写一致性（Monotonic-write-consistency）"><a href="#单调写一致性（Monotonic-write-consistency）" class="headerlink" title="单调写一致性（Monotonic write consistency）"></a>单调写一致性（Monotonic write consistency）</h3><p>​    <strong>单调写一致性</strong>指一个系统要能够保证来自同一个节点的写操作被顺序的执行。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201110145257.png" alt="img"></p><p>​    然而，在实际的实践中，这 <code>5</code> 种系统往往会结合使用，以构建一个具有最终一致性的分布式系统。实际上，不只是分布式系统使用最终一致性，关系型数据库在某个功能上，也是使用最终一致性的，比如备份，数据库的复制过程是需要时间的，这个复制过程中，业务读取到的值就是旧的。当然，最终还是达成了数据一致性。这也算是一个最终一致性的经典案例。</p><h2 id="BASE和ACID的不同"><a href="#BASE和ACID的不同" class="headerlink" title="BASE和ACID的不同"></a><code>BASE</code>和<code>ACID</code>的不同</h2><ul><li><code>ACID</code>是传统数据库常用的设计理念, 追求强一致性模型。</li><li><code>BASE</code>支持的是大型分布式系统，提出通过牺牲强一致性获得高可用性。</li></ul><p><code>ACID</code>和<code>BASE</code>代表了两种截然相反的设计哲学。</p><p>​    总的来说，<code>BASE</code>  理论面向大型高可用可扩展的分布式系统，与<code>ACID</code>这种强一致性模型不同，常常是牺牲强一致性来获得可用性，并允许数据在一段时间是不一致的。虽然两者处于【一致性-可用性】分布图的两级，但两者并不是孤立的，对于分布式系统来说，往往依据业务的不同和使用的系统组件不同，而需要灵活的调整一致性要求，也因此，常常会组合使用<code>ACID</code>和<code>BASE</code>。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;​    在&lt;code&gt;CAP&lt;/code&gt; 定理中，我们说，&lt;code&gt;CAP&lt;/code&gt; 不可能同时满足，而分区容错是对于分布式系统而言，是必须的。最后，我们说，如果系统能够同时实现 &lt;code&gt;CAP&lt;/code&gt; 是再好不过的了，所以出现了 &lt;code&gt;BASE&lt;/code&gt; 理论，今天就来讲讲 &lt;code&gt;Base&lt;/code&gt; 理论。&lt;/p&gt;
    
    </summary>
    
      <category term="分布式算法" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="分布式算法" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>分布式算法11-ACID和CAP中的C的区别</title>
    <link href="http://yoursite.com/2019/10/01/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%9511-ACID%E5%92%8CCAP%E4%B8%AD%E7%9A%84C%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://yoursite.com/2019/10/01/分布式算法11-ACID和CAP中的C的区别/</id>
    <published>2019-10-01T09:11:02.000Z</published>
    <updated>2020-11-16T04:53:25.316Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ACID与CAP定理中C的区别"><a href="#ACID与CAP定理中C的区别" class="headerlink" title="ACID与CAP定理中C的区别"></a><code>ACID</code>与<code>CAP</code>定理中<code>C</code>的区别</h1><p>​    <strong><code>ACID</code></strong>和<strong><code>CAP</code></strong>定理中都有<code>C</code>，代表<code>Consistent</code>一致性，很多人容易将这两个<code>C</code>混为一谈，其实这两个一致性是有区别的。</p><a id="more"></a><p>​    事务的定义是一系列操作，要么全部成功，要么全部不成功，数据库的事务机制是通过<code>ACID</code>实现的，<code>ACID</code>中的一致性的定义是：<strong>一个事务可以封装状态改变（除非它是一个只读的）。事务必须始终保持系统处于一致的状态，不管在任何给定的时间并发事务有多少。</strong></p><p>​    也就是说：如果事务是并发多个，系统也必须如同串行事务一样操作。其主要特征是<strong>保护性</strong>和<strong>不变性</strong>(<strong><code>Preserving an  Invariant</code></strong>)，以转账案例为例，假设有五个账户，每个账户余额是<code>100</code>元，那么五个账户总额是<code>500</code>元，如果在这个<code>5</code>个账户之间同时发生多个转账，无论并发多少个，比如在<code>A</code>与<code>B</code>账户之间转账<code>5</code>元，在<code>C</code>与<code>D</code>账户之间转账<code>10</code>元，在<code>B</code>与<code>E</code>之间转账<code>15</code>元，五个账户总额也应该还是<code>500</code>元，这就是保护性和不变性。</p><p>​    如果说<code>ACID</code>的<code>C</code>是节点服务器的数据完整性，而<code>CAP</code>的一致性是<strong>分布式多服务器之间复制数据以取得这些服务器拥有同样的数据，这是一种分布式领域的一致性概念。</strong>因此两者是完全不同的概念。</p><p>​    分布式领域中的一致性有的强弱之分，<strong>强一致性</strong>也就是指一旦有写操作写入任何一个服务器，立即在其他服务器之间同步复制新的数据，这样，  任何服务器上任何读操作总是能看到最近写入的新数据。如果不能立即看到最近写入的新数据，而可能过了一段时间才能看到，则属于<strong>弱一致性</strong>或<strong>最终一致性</strong>了。</p><p>​    强一致性分为<strong>由写实现一致性</strong>(<code>Consistency by writes</code>)、<strong>由读实现一致性</strong>(<code>Consistency by reads</code>)和<strong>由冲裁实现一致性</strong>(<code>Consistency by Quorum</code>)。</p><p>​    <strong>由写实现一致性</strong>：在写入数据同时，将数据复制到其他服务器上，读取任何一台都可以获得新的写入数据，复制数据是在写操作完成，读操作轻量。</p><p>​    <strong>由读实现一致性</strong>：写入一旦服务器后，不再复制，而是在读取时使用版本来协调复制(如<strong><code>vector clock</code></strong>算法)，这样我们简化了写操作，而将负担加在读操作。</p><p>​    <strong>由冲裁实现一致性</strong>：如果写入时复制到其他<code>2/3</code>大多数服务器，读取时也是从<code>2/3</code>大多数服务器读取，读取这边负责解决哪个更新是最新结果，这在读操作和写操作之间分担了负载。</p><p>​    回到事务话题，如果要在分布式系统中实现像<code>ACID</code>那样的事务机制，只有强一致性还是不够的，如果我们操作步骤顺序很重要，不可以中断或打乱，我们要么一起一次执行它们，如果并发执行这些操作步骤，无论怎么并发，也要如同它们是在独立执行，我们最终得到的结果总是相同的，这是一种更强的一致性：<strong>线性一致性</strong>(<code>linearizable consistency</code>)，类似<code>ACID</code>中的<strong>隔离层</strong>(<code>serial isolation level</code>)。</p><p>​    The CAP FAQ将<code>CAP</code>定理中的一致性定义为这种线性一致性或称为<code>atomic</code>原子一致性。一种比普通一致性更强的一致性，这也是大家又将<code>ACID</code>的<code>C</code>和<code>CAP</code>的<code>C</code>等同在一起的原因。<code>ACID</code>的<code>C</code>与<code>CAP</code>的<code>C</code>的关系类似精确与一致性的关系，如下图：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185501" alt="img"></p><p>​    这种分布式的线性强一致性有两种实现方式：<strong><code>2PC</code>两段提交</strong>和<strong><code>Paxos</code>算法</strong>是常见两种。</p><p>​    通过<code>2PC</code>写入新数据需要经过两次来回，第一次请求<code>commit</code>，第二次才正式确认<code>commit</code>，在这两者之间过程中，所有服务器都会堵塞等待发起者发出整个事务成功还是失败的结果(只有发起者知道所有服务器的情况)，如果失败，所有服务器返回之前状态，相当于写入数据失败，写入数据没有发生过一样。</p><p>​    而<code>Paxos</code>算法能够回避<code>2PC</code>的堵塞死锁等问题，更好地实现服务器之间数据强一致复制，具体内容见<code>Paxos</code>算法。也可参考比<code>Paxos</code>算法改进的<code>Raft</code>算法。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;ACID与CAP定理中C的区别&quot;&gt;&lt;a href=&quot;#ACID与CAP定理中C的区别&quot; class=&quot;headerlink&quot; title=&quot;ACID与CAP定理中C的区别&quot;&gt;&lt;/a&gt;&lt;code&gt;ACID&lt;/code&gt;与&lt;code&gt;CAP&lt;/code&gt;定理中&lt;code&gt;C&lt;/code&gt;的区别&lt;/h1&gt;&lt;p&gt;​    &lt;strong&gt;&lt;code&gt;ACID&lt;/code&gt;&lt;/strong&gt;和&lt;strong&gt;&lt;code&gt;CAP&lt;/code&gt;&lt;/strong&gt;定理中都有&lt;code&gt;C&lt;/code&gt;，代表&lt;code&gt;Consistent&lt;/code&gt;一致性，很多人容易将这两个&lt;code&gt;C&lt;/code&gt;混为一谈，其实这两个一致性是有区别的。&lt;/p&gt;
    
    </summary>
    
      <category term="分布式算法" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="分布式算法" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>分布式算法8-一致性协议之Raft</title>
    <link href="http://yoursite.com/2019/10/01/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%958-%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%B9%8BRaft%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2019/10/01/分布式算法8-一致性协议之Raft算法/</id>
    <published>2019-10-01T09:11:02.000Z</published>
    <updated>2020-11-16T04:52:55.556Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、引子"><a href="#一、引子" class="headerlink" title="一、引子"></a>一、引子</h2><h3 id="1-1-介绍"><a href="#1-1-介绍" class="headerlink" title="1.1 介绍"></a>1.1 介绍</h3><p>​    <code>Raft</code> 是一种为了管理复制日志的一致性算法。它提供了和 <code>Paxos</code> 算法相同的功能和性能，但<code>Raft</code>更加容易理解和实践，在工程领域的重要性毋庸置疑。</p><a id="more"></a><p>​    区别于一般一致性算法，<code>Raft</code>算法的特性如下：</p><ul><li>强<code>Leader</code>：<code>Raft</code> 使用一种更强的领导形式。日志只从<code>Leader</code>流向<code>servers</code>。这种方式简化了对复制日志的管理并且使得 <code>Raft</code> 算法更加易于理解。</li><li><code>Leader</code>选举：<code>Raft</code> 算法使用一个随机计时器来选举<code>Leader</code>。这种方式只是在任何一致性算法都必须实现的心跳机制上增加了一点机制，这在解决冲突的时候会更加简单快捷。</li><li>成员关系调整：<code>Raft</code> 使用一种共同一致的方法来处理集群成员变换的问题，处于调整过程中的两种不同的配置集群中大多数机器会有重叠，这就使得集群在成员变换的时候依然可以继续工作。</li></ul><h3 id="1-2-复制状态机"><a href="#1-2-复制状态机" class="headerlink" title="1.2 复制状态机"></a>1.2 复制状态机</h3><p>​    共识算法通常出现在复制状态机的上下文中。在这种方法中，一个<code>server</code>集群上的多个<code>State Machine</code>计算相同状态的相同副本，并且在一些机器宕掉的情况下也可以继续运行。复制状态机在分布式系统中被用于解决很多容错的问题。复制状态机通常都是基于复制日志实现的，结构如下图：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185939.png" alt="img"></p><p>​    如上图，每一个<code>server</code>存储一个包含一系列指令的<code>log</code>，并且<code>State  Machine</code>顺序执行<code>log</code>中的指令。每一个<code>Log</code>都按照相同的顺序包含相同的指令，所以每一个<code>server</code>都执行相同的指令序列。因为每个状态机(<code>state machines</code>)都是确定的，每一次执行操作都产生相同的状态和同样的结果序列。</p><p><strong><code>Replicated state machines</code> 流程</strong>：</p><ol><li><code>Client</code>请求一致性模块（<code>Consensus Module</code>）。<ol start="2"><li>一致性模块（<code>Consensus Module</code>）接收<code>Client</code>发送来的指令然后追加进自己的<code>Log</code>中。它和其他<code>server</code>上的<code>Consensus Module</code>通信来保证每一个<code>server</code>上的<code>Log</code>最终都以相同的顺序包含相同的请求。</li><li>一旦指令被正确的复制，每一个<code>server</code>的<code>State Machine</code>按照日志顺序处理他们。</li><li><code>State Machine</code>把结果返回给<code>Client</code>。</li></ol></li></ol><h3 id="1-3-可理解性设计"><a href="#1-3-可理解性设计" class="headerlink" title="1.3 可理解性设计"></a>1.3 可理解性设计</h3><p>为了让算法更加可理解，<code>Raft</code>的作者做了<code>2</code>方面的努力：</p><ol><li><p><strong>问题拆分</strong>：<code>Raft</code> 算法被分成领导人选举，日志复制，安全性和角色改变几个部分。</p></li><li><p><strong>状态简化</strong>：减少状态数量。</p></li></ol><h3 id="1-4-自学地址"><a href="#1-4-自学地址" class="headerlink" title="1.4 自学地址"></a>1.4 自学地址</h3><ul><li><a href="https://raft.github.io/" target="_blank" rel="noopener">官网</a></li><li><a href="https://raft.github.io/raft.pdf" target="_blank" rel="noopener">官网论文：In Search of an Understandable Consensus Algorithm (Extended Version)</a></li><li><a href="http://thesecretlivesofdata.com/raft/" target="_blank" rel="noopener">动画演示</a></li></ul><h2 id="二、Raft基础"><a href="#二、Raft基础" class="headerlink" title="二、Raft基础"></a>二、<code>Raft</code>基础</h2><h3 id="2-1-Raft三种身份"><a href="#2-1-Raft三种身份" class="headerlink" title="2.1 Raft三种身份"></a>2.1 <code>Raft</code>三种身份</h3><p>​    <code>raft</code>算法中每个<code>server</code>可能存在<code>3</code>种身份：<strong><code>Leader</code>领导者</strong>、<strong><code>Candidate</code>候选者</strong>、<strong><code>Follower</code>追随者</strong>。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185940.png" alt="img"></p><p>​    如上图，一个<code>Follower</code>没有收到来自<code>Leader</code>的心跳超时后，进入候选者<code>Candidate</code>身份，开始尝试参与选举。当收到超过<code>1/2</code>的选票时就变成<code>Leader</code>，并发送心跳给<code>follower</code>.</p><h3 id="2-2-任期（term）"><a href="#2-2-任期（term）" class="headerlink" title="2.2 任期（term）"></a>2.2 任期（<code>term</code>）</h3><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185941.png" alt="img"></p><p>​    如上图，<code>Raft</code>  把时间分割成任意长度的任期(<code>term</code>)。任期用连续的整数标记。每一段任期(<code>term</code>)从一次选举（<code>election</code>）开始，一个或者多个候选人尝试成为领导者。如果一个候选人赢得选举，然后他就在接下来的任期内充当领导人的职责。在某些情况下，会造成选票的割裂<code>spilt</code>（达不到大部分的票）。在这种情况下，这一任期会以没有领导人结束；一个新的任期（和一次新的选举）会很快重新开始。<code>Raft</code> 保证了在一个给定的任期内，最多只有一个领导者。 </p><h3 id="2-3-RPC"><a href="#2-3-RPC" class="headerlink" title="2.3 RPC"></a>2.3 <code>RPC</code></h3><p>​    <code>Raft</code> 算法中服务器节点之间通信使用远程过程调用（<code>RPCs</code>），并且基本的一致性算法只需要两种类型的  <code>RPCs</code>。请求投票（<code>RequestVote</code>） <code>RPCs</code> 由候选人在选举期间发起，然后附加条目（<code>AppendEntries</code>）<code>RPCs</code>  由领导人发起，用来复制日志和提供一种心跳机制。为了在服务器之间传输快照增加了第三种 <code>RPC</code>。当服务器没有及时的收到 <code>RPC</code>  的响应时，会进行重试， 并且他们能够并行的发起 <code>RPCs</code> 来获得最佳的性能。<br> <code>RPC</code>有三种：</p><ul><li><code>RequestVote RPC</code>：候选人在选举期间发起</li><li><code>AppendEntries RPC</code>：领导人发起的一种心跳机制，复制日志也在该命令中完成</li><li><code>InstallSnapshot RPC</code>: 领导者使用该<code>RPC</code>来发送快照给太落后的追随者</li></ul><p>超时设置：</p><ul><li><code>BroadcastTime</code> : 领导者的心跳超时时间</li><li><code>Election Timeout</code>: 追随者设置的候选超时时间</li><li><code>MTBT</code>:指的是单个服务器发生故障的间隔时间的平均数</li></ul><p><code>BroadcastTime &lt;&lt; ElectionTimeout &lt;&lt; MTBF</code>，两个原则：</p><ul><li><code>BroadcastTime</code>应该比<code>ElectionTimeout</code>小一个数量级，为的是使领导人能够持续发送心跳信息（<code>heartbeat</code>）来阻止追随者们开始选举；</li><li><code>ElectionTimeout</code>也要比<code>MTBF</code>小几个数量级，为的是使得系统稳定运行。</li></ul><p>一般<code>BroadcastTime</code>大约为<code>0.5</code>毫秒到<code>20</code>毫秒，<code>ElectionTimeout</code>一般在<code>10ms</code>到<code>500ms</code>之间。大多数服务器的<code>MTBF</code>都在几个月甚至更长。</p><h3 id="2-4-Raft-5个原则"><a href="#2-4-Raft-5个原则" class="headerlink" title="2.4 Raft 5个原则"></a>2.4 <code>Raft</code> 5个原则</h3><p><code>Raft</code>设定了5个原则，这些原则在任何时候有效！！</p><ul><li>选举安全：对于一个给定的任期号，最多只会有一个领导人被选举出来。</li><li>领导人只追加：领导人绝对不会删除或者覆盖自己的日志，只会追加新<code>log entry</code>。</li><li>日志匹配：如果两个日志包含相同的<code>index</code>和<code>term</code>的<code>log entry</code>，那么这两个日志在<code>index</code>之前的<code>entries</code>完全相同。</li><li>领导人完整性：如果某个<code>log entry</code>在某个任期号中已经被提交，那么这个<code>entry</code>必然出现在更大任期号的所有领导人中。</li><li>状态机安全：如果一个<code>server</code>已经把一个<code>log entry</code>应用到状态机的指定<code>index</code>中，那么不存在其他服务器在这个<code>index</code>提交一个不同的日志。（即相同<code>index</code>只存在相同的<code>log</code>）</li></ul><h2 id="三、Raft核心算法"><a href="#三、Raft核心算法" class="headerlink" title="三、Raft核心算法"></a>三、<code>Raft</code>核心算法</h2><p>​    <code>Raft</code> 将一致性问题分解成了三个相对独立的子问题：<strong>领导者选举（<code>Leader election</code>）</strong>、<strong>日志复制（<code>Log replication</code>）</strong>、<strong>安全性（<code>Safety</code>）</strong>。</p><h3 id="3-1-领导者选举（Leader-election）"><a href="#3-1-领导者选举（Leader-election）" class="headerlink" title="3.1 领导者选举（Leader election）"></a>3.1 领导者选举（<code>Leader election</code>）</h3><p>​    <code>Raft</code> 使用一种”心跳机制”来触发领导人选举。当服务器程序启动时，他们都是跟随者身份。只要从领导人或者候选者处接收到有效的  <code>RPCs</code>,这个服务器节点保持着跟随者状态。<code>Leader</code>周期性的向所有<code>Follower</code>发送心跳包来维持自己的权威。如果一个跟随者在一段时间里没有接收到任何消息，也就是选举超时，那么他就会认为系统中没有可用的领导者,并且发起选举以选出新的领导者。</p><p>​    要开始一次选举过程，<code>Follower</code>先要增加自己的当前任期（<code>term</code>）号并且转换到<code>Candidate</code>状态。然后他会并行的向集群中的其他服务器节点发送请求投票的 <code>RPCs</code> 来给自己投票。候选人会继续保持着当前状态直到以下三件事情之一发生：</p><ul><li><p>(1) 赢得选举: 获得了大部分的针对同一个任期号的选票，那么他就赢得了这次选举并成为领导人。每一个服务器最多会对一个任期号投出一张选票，按照先来先服务的原则。成为领导人，会向其他的服务器发送心跳消息来建立自己的权威并且阻止新的领导人的产生。</p></li><li><p>(2) 其他的服务器成为领导者: 从其他的服务器接收到它是领导人的 追加日志项<code>(AppendEntries) RPC</code>。如果这个领导人的任期号（包含在此次的 <code>RPC</code>中）&gt;= 候选人当前的任期号，那么候选人会承认领导人合法并回到跟随者状态。 如果此次 <code>RPC</code> 中的任期号比自己小，拒绝。</p></li><li><p>(3) 没有任何一个获胜: 如果有多个跟随者同时成为候选人，那么选票可能会被<code>spilt</code>以至于没有候选人可以赢得大多数人的支持。当这种情况发生的时候，每一个候选人都会超时，然后通过增加当前任期号来开始一轮新的选举。然而，没有其他机制的话，选票可能会被无限的重复瓜分。</p></li></ul><p>　　<code>Raft</code> 算法使用随机选举超时（例如 <code>150-300</code> 毫秒）的方法来确保很少会发生选票瓜分的情况，就算发生也能很快的解决。每一个候选人在开始选举时,会重置随机的选举超时时间，然后在超时时间内等待投票的结果；这样减少了在新的选举中另外的选票<code>spilt</code>的可能性。</p><h3 id="3-2-日志复制（Log-replication）"><a href="#3-2-日志复制（Log-replication）" class="headerlink" title="3.2 日志复制（Log replication）"></a>3.2 日志复制（<code>Log replication</code>）</h3><p>​    一旦一个<code>Leader</code>被选举出来，他就开始为客户端提供服务。客户端的每一个请求都包含一条被复制状态机执行的指令。<code>leader</code>把这条指令添加进<code>log</code>作为一个新<code>entry</code>，然后并行给其他的服务器发<code>AppendEntries RPCs</code> ，让他们复制这条<code>entry</code>。当这条<code>log  entry</code>被安全的复制，<code>leader</code>会把<code>entry</code>添加进状态机中然后把执行的结果返回给<code>client</code>。如果<code>follower</code>崩溃或者运行缓慢，再或者网络丢包，<code>leader</code>会不断的重复尝试<code>AppendEntries RPCs</code> （尽管已经回复了客户端）直到所有的<code>follower</code>都最终存储了所有的<code>log entry</code>。 </p><h4 id="3-2-1-正常情况"><a href="#3-2-1-正常情况" class="headerlink" title="3.2.1 正常情况"></a>3.2.1 正常情况</h4><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185942.png" alt="img"></p><p>如上图，每一个<code>log entry</code>存储一条状态机指令和从<code>Leader</code>收到这条指令时的任期号(<code>term number</code>)。每一条<code>log entry</code>都有一个<code>index</code>来表明它在日志中的位置。</p><ol><li><p><code>Leader</code>一旦把创建的<code>Log entry</code>复制到大多数的<code>server</code>上的时候，<code>log entry</code>就会被提交<code>commited</code>（例如上图中的条目 <code>7</code>）。<code>Raft</code> 算法保证所有已提交的日志条目都是持久化的并且最终会被所有可用的状态机执行。</p></li><li><p>同时，领导人的日志中之前的所有日志条目也都会被提交，包括由其他领导人创建的条目。</p></li><li><p>一旦<code>follower</code>得知一个<code>log entry</code>已提交，<code>follower</code>就会把这个<code>log entry</code>添加进本地状态机</p></li></ol><h4 id="3-2-2-Leader崩溃"><a href="#3-2-2-Leader崩溃" class="headerlink" title="3.2.2 Leader崩溃"></a>3.2.2 <code>Leader</code>崩溃</h4><p>​    <code>Leader</code>崩溃后可能导致日志不一致，如下图：</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185943.png" alt="img"></p><p>​    如上图，当一个领导人成功当选时，跟随者可能是任何情况<code>（a-f）</code>。每一个盒子表示是一个<code>log  entry</code>；里面的数字表示任期号。跟随者可能会缺少一些<code>log entry（a-b）</code>，可能会有一些未被提交的<code>log  entry（c-d）</code>，或者两种情况都存在<code>（e-f）</code>。例如，场景 <code>f</code> 可能会这样发生，某服务器在任期 <code>2</code> 的时候是领导人，已附加了一些<code>log  entry</code>到自己的日志中，但在提交之前就崩溃了；很快这个机器就被重启了，在任期 <code>3</code> 重新被选为领导人，并且又增加了一些<code>log  entry</code>到自己的日志中；在任期 <code>2</code> 和任期 <code>3</code> 的日志被提交之前，这个服务器又宕机了，并且在接下来的几个任期里一直处于宕机状态。 </p><p>解决方案：</p><p>​    要使得<code>follower</code>的日志和自己一致，<code>leader</code>必须找到最后两者达成一致的地方，然后删除从那个点之后的所有<code>log</code> <code>entry</code>，发送自己的日志给<code>follower</code>。所有的这些操作都在进行<code>AppendEntries RPCs</code> 的一致性检查时完成。<code>leader</code>针对每一个跟随者维护了一个 <code>nextIndex</code>，这表示下一个需要发送给跟随者的<code>log  entry</code>的<code>index</code>。当一个领导人刚获得权力的时候，他初始化所有的 <code>nextIndex</code>  值为自己的最后一条日志的<code>index</code>加<code>1</code>。如果一个跟随者的日志和领导人不一致，那么在下一次的附加日志 <code>RPC</code>  时的一致性检查就会失败。在被跟随者拒绝之后，领导人就会减小 <code>nextIndex</code> 值并进行重试。最终 <code>nextIndex</code>  会在某个位置使得领导人和跟随者的日志达成一致。当这种情况发生，<code>AppendEntries RPC</code> 就会成功，这时就会把跟随者冲突的日志条目全部删除并且加上领导人的日志。一旦<code>AppendEntries RPC</code> 成功，那么跟随者的日志就会和领导人保持一致，并且在接下来的任期里一直继续保持。 </p><h3 id="3-3-安全性（Safety）"><a href="#3-3-安全性（Safety）" class="headerlink" title="3.3 安全性（Safety）"></a>3.3 安全性（<code>Safety</code>）</h3><p>​    到目前为止描述的机制并不能充分的保证每一个状态机会按照相同的顺序执行相同的指令，例如：一个跟随者可能会进入不可用状态同时领导人已经提交了若干的日志条目，然后这个跟随者可能会被选举为领导人并且覆盖这些日志条目；因此，不同的状态机可能会执行不同的指令序列。</p><h4 id="3-3-1-领导者追加日志（Append-Only"><a href="#3-3-1-领导者追加日志（Append-Only" class="headerlink" title="3.3.1. 领导者追加日志（Append-Only)"></a>3.3.1. 领导者追加日志（<code>Append-Only</code>)</h4><p>​    领导者永远不会覆盖已经存在的日志条目；<br>​    日志永远只有一个流向：从领导者到追随者；</p><h4 id="3-3-2-选举限制（Election-restriction）"><a href="#3-3-2-选举限制（Election-restriction）" class="headerlink" title="3.3.2. 选举限制（Election restriction）"></a>3.3.2. 选举限制（<code>Election restriction</code>）</h4><p>​    <code>Raft</code>使用<code>voting process</code>来防止那些<code>log</code>不包含全部<code>committed  entry</code>的<code>candidate</code>赢得选举。<code>candidate</code>为了赢得选举必须和<code>cluster</code>的<code>majority</code>进行交互，这意味着每个<code>committed  entry</code>必须都在其中的一个<code>majority</code>存在。如果一个<code>candidate</code>的<code>log</code>至少和任何<code>majority</code>中的<code>log</code>保持<code>up-to-date</code>（”<code>up-to-date</code>“将在下文精确定义），那么它就包含了所有<code>committed entry</code>。<code>RequestVote RPC</code>实现了这一约束：<code>RPC</code>中包含了<code>candidate</code>的<code>log</code>信息，如果它自己的<code>log</code>比该<code>candidate</code>的<code>log</code>更新，<code>voter</code>会拒绝投票。</p><p>​    如何比较哪个新（<code>up-to-date</code>）？</p><p>​    <code>Raft</code>通过比较<code>log</code>中<code>last entry</code>的<code>index</code>和<code>term</code>来确定两个<code>log</code>哪个更<code>up-to-date</code>。如果两个<code>log</code>的<code>last  entry</code>有不同的<code>term</code>，那么拥有较大<code>term</code>的那个<code>log</code>更<code>up-to-date</code>。如果两个<code>log</code>以相同的<code>term</code>结束，那么哪个<code>log</code>更长就更<code>up-to-date</code>。</p><h4 id="3-3-3-提交前任期的日志条目"><a href="#3-3-3-提交前任期的日志条目" class="headerlink" title="3.3.3. 提交前任期的日志条目"></a>3.3.3. 提交前任期的日志条目</h4><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185944.png" alt="img"></p><p>​    如上图，展示了为什么领导人无法决定对老任期号的日志条目进行提交。在 <code>(a)</code> 中，<code>S1</code> 是领导者，部分的复制了索引位置 <code>2</code> 的日志条目。在  <code>(b)</code> 中，<code>S1</code> 崩溃了，然后 <code>S5</code> 在任期 <code>3</code> 里通过 <code>S3</code>、<code>S4</code> 和自己的选票赢得选举，然后从客户端接收了一条不一样的日志条目放在了索引 <code>2</code> 处。然后到 <code>(c)</code>，<code>S5</code> 又崩溃了；<code>S1</code> 重新启动，选举成功，开始复制日志。在这时，来自任期 <code>2</code>  的那条日志已经被复制到了集群中的大多数机器上，但是还没有被提交。如果 <code>S1</code> 在 <code>(d)</code> 中又崩溃了，<code>S5</code> 可以重新被选举成功（通过来自  <code>S2</code>，<code>S3</code> 和 <code>S4</code> 的选票），然后覆盖了他们在索引 <code>2</code> 处的日志。反之，如果在崩溃之前，<code>S1</code>  把自己主导的新任期里产生的日志条目复制到了大多数机器上，就如 <code>(e)</code> 中那样，那么在后面任期里面这些新的日志条目就会被提交（因为<code>S5</code>  就不可能选举成功）。 这样在同一时刻就同时保证了，之前的所有老的日志条目就会被提交。 </p><p>解决方案：</p><p>​    为了防止这样问题的发生，<code>Raft</code>不会通过计算备份的数目，来提交<strong>之前<code>term</code></strong>的<code>log entry</code>。只有<code>leader</code>的当前<code>term</code>的<code>log entry</code>才会计算备份数并<code>committed</code>；一旦当前<code>term</code>的<code>entry</code>以这种方式被<code>committed</code>了，那么之前的所有<code>entry</code>都将因为<code>Log Matching Property</code>而被间接<code>committed</code>。</p><h3 id="3-4-集群成员变更"><a href="#3-4-集群成员变更" class="headerlink" title="3.4 集群成员变更"></a>3.4 集群成员变更</h3><p>​    集群成员的变更和成员的宕机与重启不同，因为前者会修改成员个数进而影响到领导者的选取和决议过程，因为在分布式系统这对于<code>majority</code>这个集群中成员大多数的概念是极为重要的。</p><p>​    简单的做法是，运维人员将系统临时下线，修改配置，重新上线。但是这种做法存在两个缺点：</p><ol><li>更改时集群不可用</li><li>人为操作失误风险人为操作失误风险</li></ol><h4 id="3-4-1-直接从一种配置转到新的配置是十分不安全的"><a href="#3-4-1-直接从一种配置转到新的配置是十分不安全的" class="headerlink" title="3.4.1 直接从一种配置转到新的配置是十分不安全的"></a>3.4.1 直接从一种配置转到新的配置是十分不安全的</h4><p>​    如下图所示：<br> <img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185945.png" alt="在这里插入图片描述"></p><p>​    因为各个机器可能在任何的时候进行转换。在这个例子中，集群配额从 <code>3</code> 台机器变成了 <code>5</code> 台。不幸的是，存在这样的一个时间点，两个不同的领导人在同一个任期里都可以被选举成功。一个是通过旧的配置，一个通过新的配置。</p><h4 id="3-4-2-两阶段方法保证安全性"><a href="#3-4-2-两阶段方法保证安全性" class="headerlink" title="3.4.2 两阶段方法保证安全性"></a>3.4.2 两阶段方法保证安全性</h4><p>​    为了保证安全性，配置更改必须使用两阶段方法。在 <code>Raft</code> 中，集群先切换到一个过渡的配置，我们称之为共同一致；一旦共同一致已经被提交了，那么系统就切换到新的配置上。共同一致是老配置和新配置的结合。</p><p>​    共同一致允许独立的服务器在不影响安全性的前提下，在不同的时间进行配置转换过程。此外，共同一致可以让集群在配置转换的过程人依然响应服务器请求。</p><p>​    一个领导人接收到一个改变配置从 <code>C-old</code> 到 <code>C-new</code> 的请求，他会为了共同一致存储配置（图中的  <code>C-old,new</code>），以前面描述的日志条目和副本的形式。一旦一个服务器将新的配置日志条目增加到它的日志中，他就会用这个配置来做出未来所有的决定。领导人完全特性保证了只有拥有 <code>C-old,new</code>  日志条目的服务器才有可能被选举为领导人。当<code>C-old,new</code>日志条目被提交以后，领导人在使用相同的策略提交<code>C-new</code>，如下图所示，<code>C-old</code> 和 <code>C-new</code> 没有任何机会同时做出单方面的决定，这就保证了安全性。</p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185946.png" alt="在这里插入图片描述"></p><p>​    一个配置切换的时间线。虚线表示已经被创建但是还没有被提交的条目，实线表示最后被提交的日志条目。领导人首先创建了 <code>C-old,new</code>  的配置条目在自己的日志中，并提交到 <code>C-old,new</code> 中（<code>C-old,new</code> 的大多数和 <code>C-new</code> 的大多数）。然后他创建 <code>C-new</code>  条目并提交到 <code>C-new</code> 中的大多数。这样就不存在 <code>C-new</code> 和 <code>C-old</code> 可以同时做出决定的时间点。</p><h4 id="3-4-3-日志压缩"><a href="#3-4-3-日志压缩" class="headerlink" title="3.4.3 日志压缩"></a>3.4.3 日志压缩</h4><p>​    日志会随着系统的不断运行会无限制的增长，这会给存储带来压力，几乎所有的分布式系统(<code>Chubby</code>、<code>ZooKeeper</code>)都采用快照的方式进行日志压缩，做完快照之后快照会在稳定持久存储中保存，而快照之前的日志和快照就可以丢弃掉。</p><p>​    <code>Raft</code>的具体做法如下图所示：<br> <img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/20201111185947.png" alt="在这里插入图片描述"></p><p>​    与<code>Raft</code>其它操作<code>Leader-Based</code>不同，<code>snapshot</code>是由各个节点独立生成的。除了日志压缩这一个作用之外，<code>snapshot</code>还可以用于同步状态：<code>slow-follower</code>以及<code>new-server</code>，<code>Raft</code>使用<code>InstallSnapshot RPC</code>完成该过程，不再赘述。</p><h3 id="3-5-Client交互"><a href="#3-5-Client交互" class="headerlink" title="3.5 Client交互"></a>3.5 <code>Client</code>交互</h3><ul><li><code>Client</code>只向领导者发送请求；</li><li><code>Client</code>开始会向追随者发送请求，追随者拒绝<code>Client</code>的请求，并重定向到领导者；</li><li><code>Client</code>请求失败，会超时重新发送请求；</li></ul><p><code>Raft</code>算法要求<code>Client</code>的请求线性化，防止请求被多次执行。有两个解决方案：</p><ul><li><code>Raft</code>算法提出要求每个请求有个唯一标识；</li><li><code>Raft</code>的请求保持幂等性</li></ul><h3 id="3-6-跟随者和候选人崩溃"><a href="#3-6-跟随者和候选人崩溃" class="headerlink" title="3.6 跟随者和候选人崩溃"></a>3.6 跟随者和候选人崩溃</h3><p>​    如果跟随者或者候选人崩溃了，那么后续发送给他们的 <code>RPCs</code> 都会失败。<code>Raft</code> 就会无限的重试；如果崩溃的机器重启了，就成功了。<code>Raft</code> 的 <code>RPCs</code>  都是幂等的，所以这样重试不会造成任何问题。例如一个跟随者如果收到附加日志请求但是他已经包含了这一日志，那么他就会直接忽略这个新的请求。 </p><h3 id="3-7-时间和可用性"><a href="#3-7-时间和可用性" class="headerlink" title="3.7 时间和可用性"></a>3.7 时间和可用性</h3><p>　　我们对于<code>Raft</code>的一个要求是，它的安全性不能依赖于时间：系统不会因为有些事件发生地比预期慢了或快了而产生错误的结果。然而，可用性（系统及时响应<code>client</code>的能力）将不可避免地依赖于时间。比如，因为<code>server</code>崩溃造成的信息交换的时间比通常情况下来得长，<code>candidate</code>就不能停留足够长的时间来赢得<code>election</code>；而没有一个稳定的<code>leader</code>，<code>Raft</code>将不能进一步执行。</p><p>​    <code>leader election</code>是<code>Raft</code>中时间起最重要作用的地方。当系统满足以下的<code>timing requirement</code>的时候，<code>Raft</code>就能够选举并且维护一个稳定的<code>leader</code>：</p><p>​    <strong>广播时间（<code>broadcastTime</code>） &lt;&lt; 选举超时时间（<code>electionTimeout</code>） &lt;&lt; 平均故障间隔时间（<code>MTBF</code>）</strong></p><p>​    在这个不等式中，<code>broadcastTime</code>是<code>server</code>并行地向集群中的每个<code>server</code>发送<code>RPC</code>并且收到回复的平均时间；<code>electionTimeout</code>就是选举超时；<code>MTBF</code>是单个<code>server</code>发生故障的时间间隔。<code>broadcastTime</code>必须比<code>electionTimeout</code>小几个数量级，这样<code>leader</code>就能可靠地发送<code>heartbeat message</code>从而防止<code>follower</code>开始选举；通过随机化的方法确定<code>electionTimeout</code>，该不等式又让<code>split  vote</code>不太可能出现。<code>electionTimeout</code>必须比<code>MTBF</code>小几个数量级，从而让系统能稳定运行。当<code>leader</code>崩溃时，系统会在大概一个<code>electionTimeout</code>里不可用；我们希望这只占整个时间的很小一部分。</p><p>​    <code>broadcastTime</code>和<code>MTBF</code>都是底层系统的特性，而<code>electionTimeout</code>是我们必须选择的。<code>Raft</code>的<code>RPC</code>通常要求接收者持久化信息到<code>stable  storage</code>，因此<code>broadcastTime</code>的范围在<code>0.5ms</code>到<code>20ms</code>之间，这取决于存储技术。因此，<code>electionTimeout</code>可以取<code>10ms</code>到<code>500ms</code>。通常，<code>server</code>的<code>MTBF</code>是几个月或者更多，因此很容易满足<code>timing requirement</code>。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、引子&quot;&gt;&lt;a href=&quot;#一、引子&quot; class=&quot;headerlink&quot; title=&quot;一、引子&quot;&gt;&lt;/a&gt;一、引子&lt;/h2&gt;&lt;h3 id=&quot;1-1-介绍&quot;&gt;&lt;a href=&quot;#1-1-介绍&quot; class=&quot;headerlink&quot; title=&quot;1.1 介绍&quot;&gt;&lt;/a&gt;1.1 介绍&lt;/h3&gt;&lt;p&gt;​    &lt;code&gt;Raft&lt;/code&gt; 是一种为了管理复制日志的一致性算法。它提供了和 &lt;code&gt;Paxos&lt;/code&gt; 算法相同的功能和性能，但&lt;code&gt;Raft&lt;/code&gt;更加容易理解和实践，在工程领域的重要性毋庸置疑。&lt;/p&gt;
    
    </summary>
    
      <category term="分布式算法" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="分布式算法" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>linux-内核模块编译安装</title>
    <link href="http://yoursite.com/2019/06/08/linux-%E5%86%85%E6%A0%B8%E6%A8%A1%E5%9D%97%E7%BC%96%E8%AF%91/"/>
    <id>http://yoursite.com/2019/06/08/linux-内核模块编译/</id>
    <published>2019-06-07T16:00:00.000Z</published>
    <updated>2020-07-01T10:57:57.873Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模块的编译"><a href="#模块的编译" class="headerlink" title="模块的编译"></a>模块的编译</h2><p>​    我们在前面内核编译中驱动移植那块，讲到驱动编译分为<strong>静态编译</strong>和<strong>动态编译</strong>；<strong>静态编译即为将驱动直接编译进内核，动态编译即为将驱动编译成模块。</strong></p><p>​    而动态编译又分为以下两种</p><h3 id="内部编译"><a href="#内部编译" class="headerlink" title="内部编译"></a>内部编译</h3><p>​    在内核源码目录内编译</p><h3 id="外部编译"><a href="#外部编译" class="headerlink" title="外部编译"></a>外部编译</h3><p>​    在内核源码的目录外编译</p><a id="more"></a><h2 id="具体编译过程分析"><a href="#具体编译过程分析" class="headerlink" title="具体编译过程分析"></a>具体编译过程分析</h2><p>​    注：本次编译是外部编译，使用的内核源码是<code>Ubuntu</code>的源代码，而非开发板所用<code>linux 3.14</code>内核源码，运行平台为<code>X86</code>。</p><p>​    对于一个普通的<code>linux</code>设备驱动模块，以下是一个经典的<code>makefile</code>代码，使用下面这个<code>makefile</code>可以完成大部分驱动的编译，使用时只需要修改一下要编译生成的驱动名称即可。只需修改<code>obj-m</code>的值。</p> <figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ifneq</span>  (<span class="variable">$(KERNELRELEASE)</span>,)</span><br><span class="line"><span class="section">obj-m:=hello.o</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">KDIR := /lib/modules/<span class="variable">$(<span class="built_in">shell</span> uname -r)</span>/build</span><br><span class="line">PWD:=<span class="variable">$(<span class="built_in">shell</span> pwd)</span></span><br><span class="line"><span class="section">all:</span></span><br><span class="line">    make -C <span class="variable">$(KDIR)</span> M=<span class="variable">$(PWD)</span> modules</span><br><span class="line"><span class="section">clean:</span></span><br><span class="line">    rm -f *.ko *.o *.symvers *.cmd *.cmd.o</span><br><span class="line"><span class="keyword">endif</span></span><br></pre></td></tr></table></figure><h3 id="Makefile中的变量"><a href="#Makefile中的变量" class="headerlink" title="Makefile中的变量"></a><code>Makefile</code>中的变量</h3><p>先说明以下<code>Makefile</code>中一些变量意义：</p><p>​    （1）<strong><code>KERNELRELEASE</code>：在<code>linux</code>内核源代码中的顶层<code>makefile</code>中有定义。</strong></p><p>​    （2）<strong><code>shell pwd</code>：取得当前工作路径。</strong></p><p>​    （3）<strong><code>shell uname -r</code>：取得当前内核的版本号。</strong></p><p>​    （4）<strong><code>KDIR</code>：当前内核的源代码目录。</strong></p><p>关于<code>linux</code>源码的目录有两个，分别为<strong><code>&quot;/lib/modules/$(shell uname -r)/build&quot;</code>和<code>&quot;/usr/src/linux-header-$(shell uname -r)/&quot;</code></strong></p><p>​    但如果编译过内核就会知道，<code>usr</code>目录下那个源代码一般是我们自己下载后解压的，而<code>lib</code>目录下的则是在编译时自动<code>copy</code>过去的，两者的文件结构完全一样，因此有时也将内核源码目录设置成<code>/usr/src/linux-header-$(shell uname -r)/</code>。关于内核源码目录可以根据自己的存放位置进行修改。</p><p><strong>（5）<code>make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) modules</code></strong></p><p>这就是编译模块了：</p><ul><li><p>首先<strong>改变目录到<code>-C</code>选项指定的位置（即内核源代码目录</strong>），其中保存有内核的顶层<code>makefile</code>；</p></li><li><p><strong><code>M=</code>选项让该<code>makefile</code>在构造<code>modules</code>目标之前返回到模块源代码目录</strong>；然后，<strong><code>modueles</code>目标指向<code>obj-m</code>变量中设定的模块</strong>；在上面的例子中，我们将该变量设置成了<code>hello.o</code>。</p></li></ul><h3 id="Make的的执行步骤"><a href="#Make的的执行步骤" class="headerlink" title="Make的的执行步骤"></a><code>Make</code>的的执行步骤</h3><ul><li><p>第一次进来的时候，宏<code>“KERNELRELEASE”</code>未定义，因此进入<code>else</code>；</p></li><li><p>记录内核路径，记录当前路径；</p><p>​    由于<code>make</code>后面没有目标，所以<code>make</code>会在<code>Makefile</code>中的第一个不是以<code>.</code>开头的目标作为默认的目标执行。默认执行<code>all</code>这个规则</p></li><li><p><strong><code>make -C $(KDIR) M=$(PWD) modules</code></strong></p><p>   <strong><code>-C</code>进入到内核的目录执行<code>Makefile</code> ，在执行的时候<code>KERNELRELEASE</code>就会被赋值，<code>M=$(PWD)</code>表示返回当前目录，再次执行<code>makefile</code>，<code>modules</code>编译成模块的意思。</strong></p><p>所以这里实际运行的是<strong><code>make -C /lib/modules/2.6.13-study/build M=/home/fs/code/1/module/hello/ modules</code>。</strong></p></li><li><p>再次执行该<code>makefile</code>，<code>KERNELRELEASE</code>就有值了，就会执行<code>obj-m:=hello.o</code>。</p><p><code>obj-m：</code>表示把<code>hello.o</code>和其他的目标文件链接成<code>hello.ko</code>模块文件，编译的时候还要先把<code>hello.c</code>编译成<code>hello.o</code>文件</p></li><li><p>可以看出<code>make</code>在这里一共调用了<code>3</code>次</p><p>(1）<code>make</code><br>(2）<code>linux</code>内核源码树的顶层<code>makedile</code>调用，产生<code>.o</code>文件<br>(3）<code>linux</code>内核源码树<code>makefile</code>调用，把<code>.o</code>文件链接成<code>ko</code>文件</p></li></ul><h3 id="编译多文件"><a href="#编译多文件" class="headerlink" title="编译多文件"></a>编译多文件</h3><p>若有多个源文件，则采用如下方法：</p><ul><li><p><strong><code>obj-m := hello.o</code></strong></p></li><li><p><strong><code>hello-objs := file1.o file2.o file3.o</code></strong></p></li></ul><h2 id="内部编译简单说明"><a href="#内部编译简单说明" class="headerlink" title="内部编译简单说明"></a>内部编译简单说明</h2><p>​    如果把<code>hello</code>模块移动到内核源代码中。例如放到<code>/usr/src/linux/driver/</code>中， <code>KERNELRELEASE</code>就有定义了。在<code>/usr/src/linux/Makefile</code>中有<code>KERNELRELEASE=$(VERSION).$(PATCHLEVEL).$(SUBLEVEL)$(EXTRAVERSION)$(LOCALVERSION)</code>。</p><p>​    这时候，<code>hello</code>模块也不再是单独用<code>make</code>编译，而是在内核中用<code>make modules</code>进行编译，此时驱动模块便和内核编译在一起。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;模块的编译&quot;&gt;&lt;a href=&quot;#模块的编译&quot; class=&quot;headerlink&quot; title=&quot;模块的编译&quot;&gt;&lt;/a&gt;模块的编译&lt;/h2&gt;&lt;p&gt;​    我们在前面内核编译中驱动移植那块，讲到驱动编译分为&lt;strong&gt;静态编译&lt;/strong&gt;和&lt;strong&gt;动态编译&lt;/strong&gt;；&lt;strong&gt;静态编译即为将驱动直接编译进内核，动态编译即为将驱动编译成模块。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;​    而动态编译又分为以下两种&lt;/p&gt;
&lt;h3 id=&quot;内部编译&quot;&gt;&lt;a href=&quot;#内部编译&quot; class=&quot;headerlink&quot; title=&quot;内部编译&quot;&gt;&lt;/a&gt;内部编译&lt;/h3&gt;&lt;p&gt;​    在内核源码目录内编译&lt;/p&gt;
&lt;h3 id=&quot;外部编译&quot;&gt;&lt;a href=&quot;#外部编译&quot; class=&quot;headerlink&quot; title=&quot;外部编译&quot;&gt;&lt;/a&gt;外部编译&lt;/h3&gt;&lt;p&gt;​    在内核源码的目录外编译&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-kernel" scheme="http://yoursite.com/categories/Linux-kernel/"/>
    
    
      <category term="Linux-kernel" scheme="http://yoursite.com/tags/Linux-kernel/"/>
    
  </entry>
  
  <entry>
    <title>linux-centos下编译内核rpm包</title>
    <link href="http://yoursite.com/2019/06/07/linux-%E5%86%85%E6%A0%B8%E7%BC%96%E8%AF%91rpm/"/>
    <id>http://yoursite.com/2019/06/07/linux-内核编译rpm/</id>
    <published>2019-06-06T16:00:00.000Z</published>
    <updated>2020-07-01T10:57:52.070Z</updated>
    
    <content type="html"><![CDATA[<h3 id="构建编译所需环境"><a href="#构建编译所需环境" class="headerlink" title="构建编译所需环境"></a>构建编译所需环境</h3><h4 id="rpm编译目录创建"><a href="#rpm编译目录创建" class="headerlink" title="rpm编译目录创建"></a><code>rpm</code>编译目录创建</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[user@host]$ mkdir -p ~/rpmbuild/&#123;BUILD,BUILDROOT,RPMS,SOURCES,SPECS,SRPMS&#125;</span><br><span class="line">[user@host]$ echo &apos;%_topdir %(echo $HOME)/rpmbuild&apos; &gt; ~/.rpmmacros</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="编译所需的一些依赖包安装"><a href="#编译所需的一些依赖包安装" class="headerlink" title="编译所需的一些依赖包安装"></a>编译所需的一些依赖包安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@host]# yum install rpm-build redhat-rpm-config asciidoc hmaccalc perl-ExtUtils-Embed pesign xmlto -y</span><br><span class="line">[root@host]# yum install audit-libs-devel binutils-devel elfutils-devel elfutils-libelf-devel java-devel -y</span><br><span class="line">[root@host]# yum install ncurses-devel newt-devel numactl-devel pciutils-devel python-devel zlib-devel -y[root@host]# yum install make gcc bc openssl-devel -y</span><br></pre></td></tr></table></figure><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><h4 id="下载内核"><a href="#下载内核" class="headerlink" title="下载内核"></a>下载内核</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># https://mirrors.edge.kernel.org/pub/linux/kernel/</span><br></pre></td></tr></table></figure><h4 id="解压包"><a href="#解压包" class="headerlink" title="解压包"></a>解压包</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> tar -zxvf linux-4.14.113.tar.gz</span><br></pre></td></tr></table></figure><h4 id="进入源码目录"><a href="#进入源码目录" class="headerlink" title="进入源码目录"></a>进入源码目录</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> cd linux-4.14.113</span><br></pre></td></tr></table></figure><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><h4 id="拷贝设置"><a href="#拷贝设置" class="headerlink" title="拷贝设置"></a>拷贝设置</h4><p>拷贝正在运行的内核的配置文件(<code>.config</code>文件)到编译根目录 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> cp /boot/config-$(uname -r) .config</span><br></pre></td></tr></table></figure><h4 id="手动设置"><a href="#手动设置" class="headerlink" title="手动设置"></a>手动设置</h4><p>输入命令 <code>make menuconfig</code>。该命令将打开一个配置工具（图 1），它可以让你遍历每个可用模块，然后启用或者禁用你需要或者不需要的模块。 </p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/125627.jpg" style="zoom: 80%;"></p><p><strong>注：</strong> 为了编译迅速，且rpm包大小适中。我们选择<code>kernel hacking</code>选项，进入以后继续选择<code>compile-time checks and compile options</code>， 然后选择<code>compile the kernel with debug info</code>，关闭这个选项，这样编译的rpm包就不带调试信息</p><h4 id="开始编译"><a href="#开始编译" class="headerlink" title="开始编译"></a>开始编译</h4><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># make rpm</span></span><br></pre></td></tr></table></figure><p>编译完成可以在<code>~/.rpmmacros</code>文件中找到<code>rpm</code>包的编译目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> cat ~/.rpmmacros | grep topdir</span><br><span class="line"><span class="meta">%</span>_topdir /home/rpmbuild  //编译rpm包的目录</span><br><span class="line"><span class="meta">#</span> cd /home/rpmbuild/RPMS/x86_64/   //编译好的rpm包就在这个目录</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;构建编译所需环境&quot;&gt;&lt;a href=&quot;#构建编译所需环境&quot; class=&quot;headerlink&quot; title=&quot;构建编译所需环境&quot;&gt;&lt;/a&gt;构建编译所需环境&lt;/h3&gt;&lt;h4 id=&quot;rpm编译目录创建&quot;&gt;&lt;a href=&quot;#rpm编译目录创建&quot; class=&quot;headerlink&quot; title=&quot;rpm编译目录创建&quot;&gt;&lt;/a&gt;&lt;code&gt;rpm&lt;/code&gt;编译目录创建&lt;/h4&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[user@host]$ mkdir -p ~/rpmbuild/&amp;#123;BUILD,BUILDROOT,RPMS,SOURCES,SPECS,SRPMS&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[user@host]$ echo &amp;apos;%_topdir %(echo $HOME)/rpmbuild&amp;apos; &amp;gt; ~/.rpmmacros&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Linux-kernel" scheme="http://yoursite.com/categories/Linux-kernel/"/>
    
    
      <category term="Linux-kernel" scheme="http://yoursite.com/tags/Linux-kernel/"/>
    
  </entry>
  
  <entry>
    <title>linux-ubuntu下编译内核deb包</title>
    <link href="http://yoursite.com/2019/06/03/linux-%E5%86%85%E6%A0%B8%E7%BC%96%E8%AF%91deb/"/>
    <id>http://yoursite.com/2019/06/03/linux-内核编译deb/</id>
    <published>2019-06-02T16:00:00.000Z</published>
    <updated>2020-07-01T10:57:43.128Z</updated>
    
    <content type="html"><![CDATA[<h2 id="传统编译方式"><a href="#传统编译方式" class="headerlink" title="传统编译方式"></a>传统编译方式</h2><p>​    通常，如果我们需要在线编译安装<code>Linux</code>内核，大概要经历以下几个步骤：</p><h3 id="配置内核"><a href="#配置内核" class="headerlink" title="配置内核"></a>配置内核</h3><p>​    最常用的配置内核的方法是<code>“make menuconfig”</code>。</p><a id="more"></a><h3 id="编译内核和模块"><a href="#编译内核和模块" class="headerlink" title="编译内核和模块"></a>编译内核和模块</h3><p>​    依次执行<code>“make”</code>、<code>“make modules”</code>、<code>“make modules_install”</code>、<code>“make install”</code>，如果前面的配置没有问题的话，耐心等待一段时间就可以得到编译好的内核和模块了。</p><h3 id="生成initramfs并配置Grub"><a href="#生成initramfs并配置Grub" class="headerlink" title="生成initramfs并配置Grub"></a>生成<code>initramfs</code>并配置<code>Grub</code></h3><p>​    经过第二个步骤的<code>“make  install”</code>，<code>kbuild</code>系统会把生成的内核镜像拷贝到<code>INSTALL_PATH</code>路径下（默认这个路径是<code>/boot</code>），但这时并不能使用，我们必须配置手动<code>Grub</code>才可以。另外，很多发行版会使用<code>initramfs</code>来做引导之用（还有部分发行版采用<code>initrd</code>），我们还需要为我们的新内核手动生成<code>initramfs</code>镜像。</p><h2 id="编译内核deb安装包"><a href="#编译内核deb安装包" class="headerlink" title="编译内核deb安装包"></a>编译内核<code>deb</code>安装包</h2><p>​    在线编译安装有时不是很方便，可以在性能很好的编译机上编译<code>linux</code>代码为<code>deb</code>安装包，以便在新机器上安装。</p><p>​    如果您是<code>Ubuntu/Debian</code>的用户，可以使用<code>make-kpkg</code>简化这个流程，而且还能带来其他额外的好处。</p><h3 id="安装make-kpkg"><a href="#安装make-kpkg" class="headerlink" title="安装make-kpkg"></a>安装<code>make-kpkg</code></h3><p>​    在<code>Ubuntu</code>下，安装<code>kernel-package</code>这个包之后，就可以使用<code>make-kpkg</code>了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ubuntu ~]# sudo apt-get install kernel-package</span><br></pre></td></tr></table></figure><h3 id="配置内核-1"><a href="#配置内核-1" class="headerlink" title="配置内核"></a>配置内核</h3><p>​    使用<code>make-kpkg</code>编译内核，第一个步骤“配置内核”还是必不可少的。建议在发行版默认的<code>config</code>的基础上再进行配置，这样配置出的内核和发行版本身才会有更好的相容性。比如<code>Ubuntu 10.10</code>，可以在运行<code>“make menuconfig”</code>之前执行命令<code>“cp /boot/config-2.6.35-24-generic  .config”</code>。</p><h3 id="编译内核deb包"><a href="#编译内核deb包" class="headerlink" title="编译内核deb包"></a>编译内核<code>deb</code>包</h3><p>​    配置完内核之后，接下来要执行真正的编译过程。通常我们可以这样下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ubuntu ~]# make-kpkg --initrd --revision 001 --append-to-version -20110107 kernel_image kernek_headers</span><br></pre></td></tr></table></figure><ul><li><code>--initrd</code>选项会让<code>make-kpkg</code>自动帮我们生成<code>initramfs</code>；</li><li><code>--revision</code>会给生成的<code>deb</code>文件加上一个版本信息。这个参数只是影响到文件名，如果不指定，默认会是<code>“10.00.Custom”</code>；</li><li><code>--append-to-version</code>也是一种版本信息，它不仅出现在<code>deb</code>安装包的文件名里，也会影响到<code>kernel</code>的名称，比如本例中，内核更新完成之后，用<code>“uname -r”</code>察看会得到<code>“2.6.36-20110107”</code>；</li><li><code>kernel_image</code>表示生成内核和默认模块的安装包。</li><li><code>kernel_headers</code>表示生成一个内核头文件的安装包。</li></ul><h3 id="安装内核deb包"><a href="#安装内核deb包" class="headerlink" title="安装内核deb包"></a>安装内核<code>deb</code>包</h3><p>​    编译完成后，在源码目录的上层目录会生成<code>kernel</code>相关<code>deb</code>安装包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ubuntu ~]# dpkg -i kernel*.deb</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;传统编译方式&quot;&gt;&lt;a href=&quot;#传统编译方式&quot; class=&quot;headerlink&quot; title=&quot;传统编译方式&quot;&gt;&lt;/a&gt;传统编译方式&lt;/h2&gt;&lt;p&gt;​    通常，如果我们需要在线编译安装&lt;code&gt;Linux&lt;/code&gt;内核，大概要经历以下几个步骤：&lt;/p&gt;
&lt;h3 id=&quot;配置内核&quot;&gt;&lt;a href=&quot;#配置内核&quot; class=&quot;headerlink&quot; title=&quot;配置内核&quot;&gt;&lt;/a&gt;配置内核&lt;/h3&gt;&lt;p&gt;​    最常用的配置内核的方法是&lt;code&gt;“make menuconfig”&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-kernel" scheme="http://yoursite.com/categories/Linux-kernel/"/>
    
    
      <category term="Linux-kernel" scheme="http://yoursite.com/tags/Linux-kernel/"/>
    
  </entry>
  
  <entry>
    <title>hexo腾讯云COS部署+Markdown图床</title>
    <link href="http://yoursite.com/2019/05/08/hexo%E8%85%BE%E8%AE%AF%E4%BA%91cos%E9%83%A8%E7%BD%B2/"/>
    <id>http://yoursite.com/2019/05/08/hexo腾讯云cos部署/</id>
    <published>2019-05-08T14:16:02.000Z</published>
    <updated>2019-05-09T01:27:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>静态博客网站的需求就是一片足够大的空间，而腾讯云COS提供免费50G的存储空间，无疑是一个不错的选择。</p><p>同时也可以用腾讯云cos制作Markdown的图床，这样网站访问速度会比较快</p><a id="more"></a><h2 id="hexo部署腾讯云cos"><a href="#hexo部署腾讯云cos" class="headerlink" title="hexo部署腾讯云cos"></a>hexo部署腾讯云cos</h2><h3 id="域名"><a href="#域名" class="headerlink" title="域名"></a>域名</h3><p>​        域名注册与备案</p><blockquote><p>注意：腾讯云cos绑定cdn域名加速时需要备案域名，而域名的备案需要购买腾讯云的服务器。这点比较坑！</p><p>根据个人情况选择，也可以选择上篇博文，采用<code>hexo双站点部署</code></p></blockquote><h3 id="创建存储桶-Bucket"><a href="#创建存储桶-Bucket" class="headerlink" title="创建存储桶(Bucket)"></a>创建存储桶(Bucket)</h3><ul><li><strong>创建存储桶</strong> <a href="http://console.cloud.tencent.com/cos5/bucket" target="_blank" rel="noopener">COS地址</a></li></ul><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/WX20180902-141726@2x.png" alt></p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/WX20180902-141825@2x.png" alt></p><p>填写名称后，选择权限为<strong>公有读私有写</strong>。</p><h3 id="配置存储桶"><a href="#配置存储桶" class="headerlink" title="配置存储桶"></a>配置存储桶</h3><ul><li><p><strong>选择基础配置</strong><br><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/WX20180902-141924@2x.png" alt></p></li><li><p><strong>编辑静态网站</strong><br><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/WX20180902-141947@2x.png" alt></p></li><li><p><strong>打开设置</strong><br><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/WX20180902-142142@2x.png" alt></p></li></ul><h3 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h3><ul><li>域名管理，添加域名，选择<strong>静态网站源站</strong><br><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/WX20180902-142336@2x.png" alt></li></ul><ul><li>域名解析，添加记录<br><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/WX20180902-142950@2x.png" alt></li></ul><ul><li>第一个框为<strong>二级域名</strong>，第二个框为<strong>记录值</strong>。<br><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/WX20180902-143210@2x.png" alt></li></ul><h3 id="上传文件测试"><a href="#上传文件测试" class="headerlink" title="上传文件测试"></a>上传文件测试</h3><ul><li>在test存储桶上传<code>CNAME</code>文件和<code>index.html</code>进行测试。</li><li><p>CANME文件的内容为（域名换成自己的域名）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test.fiftykg.com</span><br></pre></td></tr></table></figure></li><li><p><code>index.html</code>的内容为：</p></li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"zh"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>测试主页<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">测试主页内容</span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>在浏览器中查看结果：<br><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/WX20180902-183101@2x.png" alt></li></ul><h3 id="刷新CDN缓存"><a href="#刷新CDN缓存" class="headerlink" title="刷新CDN缓存"></a>刷新CDN缓存</h3><ul><li><p>上图的效果是不能立刻看到的，大部分时候需要等待。</p><p>有可能你会看到以下的效果：<br><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/WX20180902-182126@2x.png" alt></p></li></ul><ul><li>首先请确认你的网站类型为<code>静态网站源站</code>。如果设置正确，那么可能需要手动刷新以下cdn的缓存：</li></ul><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/WX20180902-182852@2x.png" alt></p><h3 id="hexo部署"><a href="#hexo部署" class="headerlink" title="hexo部署"></a>hexo部署</h3><p>安装cos部署插件：</p><p><code>npm install hexo-deployer-cos --save</code></p><ul><li>在根目录的<code>_config.yml</code>配置：<br><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/WX20180902-184622@2x.png" alt></li></ul><ul><li><p>region等参数可以在下图位置查看：<br><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/WX20180902-184118@2x.png" alt></p><p>秘钥可以在<code>访问管理</code>中<code>云API秘密钥</code>中找到。</p></li></ul><h2 id="Markdown图床"><a href="#Markdown图床" class="headerlink" title="Markdown图床"></a>Markdown图床</h2><h3 id="创建存储桶-Bucket-1"><a href="#创建存储桶-Bucket-1" class="headerlink" title="创建存储桶(Bucket)"></a>创建存储桶(Bucket)</h3><ul><li><strong>创建存储桶</strong> <a href="http://console.cloud.tencent.com/cos5/bucket" target="_blank" rel="noopener">COS地址</a></li></ul><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/1557315247262.png" alt></p><p>填写名称后，选择权限为<strong>公有读写</strong>。</p><h3 id="PicGo-客户端配置"><a href="#PicGo-客户端配置" class="headerlink" title="PicGo 客户端配置"></a>PicGo 客户端配置</h3><h4 id="下载-amp-安装"><a href="#下载-amp-安装" class="headerlink" title="下载&amp;安装"></a>下载&amp;安装</h4><p>​    PicGo （目前 2.0.4）是一个开源的图床工具，非常优秀。可以到 git 上下载，但下载速度太慢，所以我放了一个百度云的链接，速度快很多。</p><ul><li>git的地址：<a href="https://github.com/Molunerfinn/PicGo" target="_blank" rel="noopener">https://github.com/Molunerfinn/PicGo</a></li><li>Win版下载链接：<a href="https://pan.baidu.com/s/1sr7DKuP7p0WQ1WNBK3Zkow" target="_blank" rel="noopener">https://pan.baidu.com/s/1sr7DKuP7p0WQ1WNBK3Zkow</a>  提取码：d4cx</li></ul><h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/1557315547061.png" alt></p><ul><li><p>存储空间名：所存储图片的桶名称</p></li><li><p>存储路径：选择的地区，例如：<code>ap-guangzhou</code></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;静态博客网站的需求就是一片足够大的空间，而腾讯云COS提供免费50G的存储空间，无疑是一个不错的选择。&lt;/p&gt;
&lt;p&gt;同时也可以用腾讯云cos制作Markdown的图床，这样网站访问速度会比较快&lt;/p&gt;
    
    </summary>
    
      <category term="Hexo" scheme="http://yoursite.com/categories/Hexo/"/>
    
      <category term="Markdown" scheme="http://yoursite.com/categories/Hexo/Markdown/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="markdown" scheme="http://yoursite.com/tags/markdown/"/>
    
  </entry>
  
  <entry>
    <title>hexo双站点部署github和coding</title>
    <link href="http://yoursite.com/2019/05/08/hexo%E5%8F%8C%E7%AB%99%E7%82%B9%E9%83%A8%E7%BD%B2/"/>
    <id>http://yoursite.com/2019/05/08/hexo双站点部署/</id>
    <published>2019-05-08T14:16:02.000Z</published>
    <updated>2020-06-30T03:10:26.590Z</updated>
    
    <content type="html"><![CDATA[<p>​        这是搭建博客系列的第二篇，至于为什么要托管到<code>coding</code>上，原因大家也应该能猜到，就是<code>github</code>访问速度偏慢，体验不是很好。</p><a id="more"></a><h3 id="注册coding-net账号并创建项目"><a href="#注册coding-net账号并创建项目" class="headerlink" title="注册coding.net账号并创建项目"></a>注册<code>coding.net</code>账号并创建项目</h3><p>项目名最好跟用户名一样<br><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/QQ截图20190508182436.png" alt></p><h3 id="设置coding-SSH-KEY"><a href="#设置coding-SSH-KEY" class="headerlink" title="设置coding SSH KEY"></a>设置<code>coding SSH KEY</code></h3><p>这个就用当时设置<code>github</code>的一样就行了<br><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/QQ截图20190508182512.png" alt></p><h3 id="config-yml配置"><a href="#config-yml配置" class="headerlink" title="_config.yml配置"></a><code>_config.yml</code>配置</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: </span><br><span class="line">        github: git@github.com:code-hly/code-hly.github.io.git,master</span><br><span class="line">        coding: git@git.coding.net:huliaoyuan/huliaoyuan.git,master</span><br></pre></td></tr></table></figure><blockquote><p>替换你的项目名，注意空格，我这儿用的是<code>ssh</code>，而不是<code>https</code></p></blockquote><h3 id="部署项目到coding上"><a href="#部署项目到coding上" class="headerlink" title="部署项目到coding上"></a>部署项目到<code>coding</code>上</h3><ul><li><p>进入<code>myblog</code>根目录下，先敲如下命令。为了使用<code>hexo d</code>来部署到<code>git</code>上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>使用部署命令就能把博客同步到<code>coding</code>上面</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo deploy -g</span><br></pre></td></tr></table></figure></li></ul><h3 id="pages服务方式部署"><a href="#pages服务方式部署" class="headerlink" title="pages服务方式部署"></a><code>pages</code>服务方式部署</h3><p>​        部署博客方式有两种，第一种就是<code>pages</code>服务的方式，也推荐这种方式，因为可以绑定域名，而第二种演示的方式必须升级会员才能绑定自定义域名。<code>pages</code>方式也很简单就是在<code>source/</code>需要创建一个空白文件，至于原因，是因为<code>coding.net</code>需要这个文件来作为以静态文件部署的标志。就是说看到这个<code>Staticfile</code>就知道按照静态文件来发布。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd source/</span><br><span class="line">touch Staticfile  #名字必须是Staticfile</span><br></pre></td></tr></table></figure><h3 id="个人域名绑定"><a href="#个人域名绑定" class="headerlink" title="个人域名绑定"></a>个人域名绑定</h3><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/QQ截图20190508183145.png" alt></p><blockquote><p>注意<code>github</code>绑定需要在 <code>myblog/source</code>目录下建一 <code>CNAME</code> 文件，并写上你购买的域名，域名我是在阿里万网买的</p><p>我当时按照别人的做法写的默认和海外，这样发现只能访问<code>coding</code>上的博客而不能访问 <code>github</code> 上的博客了</p></blockquote><h3 id="coding绑定私有域名"><a href="#coding绑定私有域名" class="headerlink" title="coding绑定私有域名"></a><code>coding</code>绑定私有域名</h3><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/QQ截图20190508183721.png" alt></p><h3 id="发布博客新文章后直接部署到github和coding"><a href="#发布博客新文章后直接部署到github和coding" class="headerlink" title="发布博客新文章后直接部署到github和coding"></a>发布博客新文章后直接部署到<code>github</code>和<code>coding</code></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g -d</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​        这是搭建博客系列的第二篇，至于为什么要托管到&lt;code&gt;coding&lt;/code&gt;上，原因大家也应该能猜到，就是&lt;code&gt;github&lt;/code&gt;访问速度偏慢，体验不是很好。&lt;/p&gt;
    
    </summary>
    
      <category term="Hexo" scheme="http://yoursite.com/categories/Hexo/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hexo搭建博客教程</title>
    <link href="http://yoursite.com/2019/05/08/hexo%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/"/>
    <id>http://yoursite.com/2019/05/08/hexo搭建流程/</id>
    <published>2019-05-08T14:16:02.000Z</published>
    <updated>2020-06-30T03:25:16.906Z</updated>
    
    <content type="html"><![CDATA[<p>​        现在越来越多的人喜欢利用Github搭建静态网站，原因不外乎简单省钱。本人也利用hexo+github搭建了本博客，用于分享一些心得。在此过程中，折腾博客的各种配置以及功能占具了我一部分时间，在此详细记录下我是如何利用hexo+github搭建静态博客以及一些配置相关问题，以免过后遗忘，且当备份之用。</p><a id="more"></a><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><ul><li>下载<code>node.js</code>并安装（官网下载安装），默认会安装<code>npm</code>。</li><li>下载安装<code>git</code>（官网下载安装）</li><li>下载安装<code>hexo</code>。方法：打开<code>cmd</code> 运行<code>*npm install -g hexo</code>*（要翻墙） </li></ul><h3 id="本地搭建hexo静态博客"><a href="#本地搭建hexo静态博客" class="headerlink" title="本地搭建hexo静态博客"></a>本地搭建hexo静态博客</h3><ul><li>新建一个文件夹，如<code>MyBlog</code></li><li>进入该文件夹内，右击运行<code>git</code>，输入：<code>*hexo init</code>*（生成hexo模板，可能要翻墙）</li><li>生成完模板，运行<code>*npm install</code>*（目前貌似不用运行这一步）</li><li>最后运行：<em><code>hexo server</code></em> （运行程序，访问本地localhost:4000可以看到博客已经搭建成功）</li></ul><h3 id="创建一个新仓库"><a href="#创建一个新仓库" class="headerlink" title="创建一个新仓库"></a>创建一个新仓库</h3><p>​       新建一个名为你的<code>github用户名.github.io</code>的仓库，比如说，如果你的github用户名是code-hly(这个是我的用户名)，那么你就新建<code>code-hly.github.io</code>的仓库（必须是你的用户名，其它名称无效），将来你的网站访问地址就是 <a href="https://code-hly.github.io" target="_blank" rel="noopener">https://code-hly.github.io</a> 了，是不是很方便</p><p>​     由此可见，每一个github账户最多只能创建一个这样可以直接使用域名访问的仓库，所以访问的地址也是唯一的，方便github服务器管理。</p><h3 id="设置仓库的参数"><a href="#设置仓库的参数" class="headerlink" title="设置仓库的参数"></a>设置仓库的参数</h3><p>​       相信大多数人都知道，要想使用git命令来和github进行提交部署等操作，需要进行一些配置，大概就是下面一些命令，如不明白请自行搜索.  </p><ul><li>右键鼠标选择<code>git Base here</code>,输入如下命令：</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.email xxx@qq.com   </span><br><span class="line">git config --global user.name xxx  </span><br><span class="line">ssh-keygen -t rsa -C xxx@qq.com(邮箱地址)  <span class="comment">// 生成ssh</span></span><br></pre></td></tr></table></figure><p>​       <strong>注：<code>email</code>和<code>name</code>分别为注册<code>github</code>时的邮箱和用户名，生成ssh时会提示让你选择存储地址，可直接按enter下一步存储为默认地址。</strong></p><ul><li><p>找到<code>.ssh</code>文件夹，找到<code>id_rsa.pub</code>文件打开复制<code>SSH</code></p></li><li><p>登陆<code>github</code>，<code>settings</code>-&gt;<code>Deploy keys</code>-&gt;<code>add deploy key</code>（把复制的SSH添加进去即可）</p></li></ul><h3 id="将博客与Github关联"><a href="#将博客与Github关联" class="headerlink" title="将博客与Github关联"></a>将博客与<code>Github</code>关联</h3><ul><li>打开本地的<code>MyBlog</code>文件夹项目内的<code>_config.yml</code>配置文件，将其中的type设置为git</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: git@github.com:code-hly/code-hly.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><p><strong>repository为对应仓库的地址。注意仓库地址有两种形式。一种是<code>https</code>，一种是<code>SSH</code>。此处应该使用SSH形式的地址。</strong>   </p><ul><li>运行：<em><code>npm install hexo-deployer-git –save</code></em></li><li>运行：<em><code>hexo g</code></em>（本地生成静态文件）</li><li>运行：<em><code>hexo d</code></em>（将本地静态文件推送至Github）</li></ul><p>此时，打开浏览器，访问<em><a href="https://code-hly.github.io" target="_blank" rel="noopener">https://code-hly.github.io</a></em></p><h4 id="可能问题"><a href="#可能问题" class="headerlink" title="可能问题"></a>可能问题</h4><blockquote><p>错误提示：FATAL bad indentation of a mapping entry at line 72, column 7:</p></blockquote><p>可以hexo g 但是不可以hexo d<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">错误提示：</span><br><span class="line">You should configure deployment settings in _config.yml first!</span><br><span class="line">Available deployer plugins:</span><br><span class="line">  git</span><br><span class="line">For more help, you can check the online docs: http://hexo.io/</span><br></pre></td></tr></table></figure></p><p>天真的我，被他俩坑死了。</p><p>​        问题一：(上图)type: git这里的分号后面没有空格（纳尼，手动黑人问号脸）。所有的配置项目分号（你输入的<code>http:</code>这个分号不用）后面有参数的都要有一个空格</p><p>​        问题二：(上图)deploy和下面的那几项我从网上复制了一下，（不知为啥我的配置里没生成<code>repo</code>和<code>branch</code>）结果就因为后三个没有缩进，我又白白躺了几个小时</p><h3 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h3><p>　　因为<code>Hexo</code>个人博客是托管在<code>github</code>之上，每次访问都要使用<a href="http://penglei.top" target="_blank" rel="noopener">githubname.github.io</a>这么一个长串的域名来访问，会显得非常繁琐。这个时候我们可以购买一个域名，设置<code>DNS</code>跳转，以达到通过域名即可访问我们的个人博客，我就是在阿里的万网购买的域名。</p><h4 id="域名解析"><a href="#域名解析" class="headerlink" title="域名解析"></a>域名解析</h4><ul><li>点击对应域名的”解析”</li></ul><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/1557307304760.png" alt></p><ul><li><p>点击添加解析，记录类型选<code>A</code>或<code>CNAME</code></p><blockquote><p><code>A</code>记录的记录值就是<code>ip</code>地址，<code>github</code>(官方文档)提供了两个IP地址，<code>192.30.252.153和192.30.252.154</code>，这两个<code>IP</code>地址为<code>github</code>的服务器地址，两个都要填上，解析记录设置两个<code>@</code>，线路就默认就行了，<br><code>CNAME</code>记录值填你的<code>github</code>博客网址。如我的是<code>code-hly.github.io</code></p></blockquote></li></ul><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/1557307617094.png" alt></p><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/1557307781856.png" alt></p><p>​        这些全部设置完成后，此时你并不能要申请的域名访问你的博客。接着你需要做的是在hexo根目录的source文件夹里创建<code>CNAME</code>文件，不带任何后缀，里面添加你的域名信息，如：<code>penglei.com</code>。实践证明如果此时你填写的是<code>www.penglei.top</code> 那么以后你只能用<code>www.penglei.top</code> 访问，而如果你填写的是<code>penglei.top</code>。那么用<code>www.penglei.top</code> 和 <code>penglei.top</code> 访问都是可以的。重新清理hexo,并发布即可用新的域名访问。</p><ul><li><p>运行：<em>hexo g</em></p></li><li><p>运行：<em>hexo d</em></p></li></ul><h3 id="更新博客内容"><a href="#更新博客内容" class="headerlink" title="更新博客内容"></a>更新博客内容</h3><p>　　至此博客已经搭建完毕，域名也已经正常解析，那么剩下的问题就是更新内容了。</p><h4 id="更新文章"><a href="#更新文章" class="headerlink" title="更新文章"></a>更新文章</h4><ul><li>在<code>MyBlog</code>目录下执行：<em><code>hexo new “我的第一篇文章”</code></em>，会在<code>source-&gt;_posts</code>文件夹内生成一个.md文件。</li><li>编辑该文件（遵循Markdown规则）</li><li>修改起始字段<ul><li>title    文章的标题  </li><li>date    创建日期    （文件的创建日期 ）</li><li>updated    修改日期   （ 文件的修改日期）   </li><li>comments    是否开启评论    true  </li><li>tags    标签   </li><li>categories    分类   </li><li>permalink    url中的名字（文件名）</li></ul></li><li>编写正文内容（MakeDown）</li><li><code>hexo clean</code> 删除本地静态文件（Public目录），可不执行。</li><li><code>hexo g</code> 生成本地静态文件（Public目录）</li><li><code>hexo deploy</code> 将本地静态文件推送至github（hexo d）</li></ul><h4 id="添加菜单"><a href="#添加菜单" class="headerlink" title="添加菜单"></a>添加菜单</h4><p>进入<code>theme</code>目录，编辑<code>_config_yml</code>文件，找到<code>menu:</code>字段，在该字段下添加一个字段。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: /</span><br><span class="line">  about: /about</span><br><span class="line">  ......</span><br></pre></td></tr></table></figure><p>然后找到<code>lanhuages</code>目录，编辑<code>zh-Hans.yml</code>文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: 首页</span><br><span class="line">  about: 关于作者</span><br><span class="line">  ......</span><br></pre></td></tr></table></figure><p>更新页面显示的中文字符，最后进入<code>theme</code>目录下的<code>Source</code>目录，新增一个<code>about</code>目录，里面写一个<code>index.html</code>文件。</p><h4 id="文章内插入图片"><a href="#文章内插入图片" class="headerlink" title="文章内插入图片"></a>文章内插入图片</h4><p>在文章中写入:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![](/upload_image/1.jpg)</span><br></pre></td></tr></table></figure><p>　　然后进入<code>themes-主题名-source-upload_image</code>目录下(自己创建)，将图片放到这个目录下，就可以了。</p><p>说明：当执行<code>hexo g</code>命令时，会自动把图片复制到 <code>public</code>文件的<code>upload_image</code>目录下。</p><h3 id="个性化设置"><a href="#个性化设置" class="headerlink" title="个性化设置"></a>个性化设置</h3><h4 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h4><p>　　在根目录下的<code>_config.yml</code>文件中，可以修改标题，作者等信息。打开编辑该文件，注意：每一个值的冒号后面都有一个半角空格！</p><ul><li>未生效的写法：<code>title:nMask</code>的博客</li><li>能生效的写法：<code>title:[空格]nMask</code>的博客</li></ul><h4 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h4><p>访问<a href="http://www.zhihu.com/question/24422335" target="_blank" rel="noopener">主题列表</a>，获取主题代码。</p><p>进入themes目录，进入以下操作：</p><ul><li><p>下载主题 (以next主题为例)</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https:<span class="comment">//github.com/iissnan/hexo-theme-next.git（主题的地址）</span></span><br></pre></td></tr></table></figure></li><li><p>打开<code>__config.yml</code>文件，将<code>themes</code>修改为<code>next</code>（下载到的主题文件夹的名字）</p></li><li><p><code>hexo g</code></p></li><li><p><code>hexo d</code></p></li></ul><p>关于hexo-next主题下的一些个性化配置，参考：<a href="http://theme-next.iissnan.com/" target="_blank" rel="noopener">Next主题配置</a></p><h3 id="主题美化"><a href="#主题美化" class="headerlink" title="主题美化"></a>主题美化</h3><h4 id="文章中添加居中模块"><a href="#文章中添加居中模块" class="headerlink" title="文章中添加居中模块"></a>文章中添加居中模块</h4><p>文章Markdown中填写如下：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;blockquote <span class="class"><span class="keyword">class</span></span>=<span class="string">"blockquote-center"</span>&gt;优秀的人，不是不合群，而是他们合群的人里面没有你&lt;<span class="regexp">/blockquote&gt;</span></span><br></pre></td></tr></table></figure><h4 id="在文章底部增加版权信息"><a href="#在文章底部增加版权信息" class="headerlink" title="在文章底部增加版权信息"></a>在文章底部增加版权信息</h4><ul><li>在目录<code>next/layout/_macro/</code>下添加<code>my-copyright.swig</code>：</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&#123;% <span class="keyword">if</span> page.copyright %&#125;</span><br><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"my_post_copyright"</span>&gt;</span><br><span class="line">  &lt;script src=<span class="string">"//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">  &lt;!-- JS库 sweetalert 可修改路径 --&gt;</span><br><span class="line">  &lt;script type=<span class="string">"text/javascript"</span> src=<span class="string">"http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">  &lt;script src=<span class="string">"http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">  &lt;link rel=<span class="string">"stylesheet"</span> type=<span class="string">"text/css"</span> href=<span class="string">"http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css"</span>&gt;</span><br><span class="line">  &lt;p&gt;<span class="xml"><span class="tag">&lt;<span class="name">span</span>&gt;</span>本文标题:<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><span class="xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"&#123;&#123; url_for(page.path) &#125;&#125;"</span>&gt;</span>&#123;&#123; page.title &#125;&#125;<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><span class="xml"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span></span><br><span class="line">  &lt;p&gt;<span class="xml"><span class="tag">&lt;<span class="name">span</span>&gt;</span>文章作者:<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><span class="xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/"</span> <span class="attr">title</span>=<span class="string">"访问 &#123;&#123; theme.author &#125;&#125; 的个人博客"</span>&gt;</span>&#123;&#123; theme.author &#125;&#125;<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><span class="xml"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span></span><br><span class="line">  &lt;p&gt;<span class="xml"><span class="tag">&lt;<span class="name">span</span>&gt;</span>发布时间:<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span>&#123;&#123; page.date.format(<span class="string">"YYYY年MM月DD日 - HH:MM"</span>) &#125;&#125;&lt;<span class="regexp">/p&gt;</span></span><br><span class="line"><span class="regexp">  &lt;p&gt;&lt;span&gt;最后更新:&lt;/</span>span&gt;&#123;&#123; page.updated.format(<span class="string">"YYYY年MM月DD日 - HH:MM"</span>) &#125;&#125;&lt;<span class="regexp">/p&gt;</span></span><br><span class="line"><span class="regexp">  &lt;p&gt;&lt;span&gt;原始链接:&lt;/</span>span&gt;<span class="xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"&#123;&#123; url_for(page.path) &#125;&#125;"</span> <span class="attr">title</span>=<span class="string">"&#123;&#123; page.title &#125;&#125;"</span>&gt;</span>&#123;&#123; page.permalink &#125;&#125;<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span></span><br><span class="line">    &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"copy-path"</span>  title=<span class="string">"点击复制文章链接"</span>&gt;<span class="xml"><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"fa fa-clipboard"</span> <span class="attr">data-clipboard-text</span>=<span class="string">"&#123;&#123; page.permalink &#125;&#125;"</span>  <span class="attr">aria-label</span>=<span class="string">"复制成功！"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><span class="xml"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span></span><br><span class="line">  &lt;<span class="regexp">/p&gt;</span></span><br><span class="line"><span class="regexp">  &lt;p&gt;&lt;span&gt;许可协议:&lt;/</span>span&gt;<span class="xml"><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"fa fa-creative-commons"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span> <span class="xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">rel</span>=<span class="string">"license"</span> <span class="attr">href</span>=<span class="string">"https://creativecommons.org/licenses/by-nc-nd/4.0/"</span> <span class="attr">target</span>=<span class="string">"_blank"</span> <span class="attr">title</span>=<span class="string">"Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)"</span>&gt;</span>署名-非商业性使用-禁止演绎 4.0 国际<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span> 转载请保留原文链接及作者。&lt;<span class="regexp">/p&gt;  </span></span><br><span class="line"><span class="regexp">&lt;/</span>div&gt;</span><br><span class="line">&lt;script&gt; </span><br><span class="line">    <span class="keyword">var</span> clipboard = <span class="keyword">new</span> Clipboard(<span class="string">'.fa-clipboard'</span>);</span><br><span class="line">    clipboard.on(<span class="string">'success'</span>, $(<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">      $(<span class="string">".fa-clipboard"</span>).click(<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">        swal(&#123;   </span><br><span class="line">          title: <span class="string">""</span>,   </span><br><span class="line">          text: <span class="string">'复制成功'</span>,   </span><br><span class="line">          html: <span class="literal">false</span>,</span><br><span class="line">          timer: <span class="number">500</span>,   </span><br><span class="line">          showConfirmButton: <span class="literal">false</span></span><br><span class="line">        &#125;);</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;));  </span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure><ul><li>在目录<code>next/source/css/_common/components/post/</code>下添加<code>my-post-copyright.styl</code>：</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.my_post_copyright</span> &#123;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">85%</span>;</span><br><span class="line">  <span class="attribute">max-width</span>: <span class="number">45em</span>;</span><br><span class="line">  <span class="attribute">margin</span>: <span class="number">2.8em</span> auto <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">padding</span>: <span class="number">0.5em</span> <span class="number">1.0em</span>;</span><br><span class="line">  <span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#d3d3d3</span>;</span><br><span class="line">  <span class="attribute">font-size</span>: <span class="number">0.93rem</span>;</span><br><span class="line">  <span class="attribute">line-height</span>: <span class="number">1.6em</span>;</span><br><span class="line">  <span class="attribute">word-break</span>: break-all;</span><br><span class="line">  <span class="attribute">background</span>: rgba(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>,<span class="number">0.4</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.my_post_copyright</span> p&#123;<span class="attribute">margin</span>:<span class="number">0</span>;&#125;</span><br><span class="line"><span class="selector-class">.my_post_copyright</span> <span class="selector-tag">span</span> &#123;</span><br><span class="line">  <span class="attribute">display</span>: inline-block;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">5.2em</span>;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#b5b5b5</span>;</span><br><span class="line">  <span class="attribute">font-weight</span>: bold;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.my_post_copyright</span> <span class="selector-class">.raw</span> &#123;</span><br><span class="line">  <span class="attribute">margin-left</span>: <span class="number">1em</span>;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">5em</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.my_post_copyright</span> <span class="selector-tag">a</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#808080</span>;</span><br><span class="line">  <span class="attribute">border-bottom</span>:<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.my_post_copyright</span> <span class="selector-tag">a</span>:hover &#123;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#a3d2a3</span>;</span><br><span class="line">  <span class="attribute">text-decoration</span>: underline;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.my_post_copyright</span>:hover <span class="selector-class">.fa-clipboard</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#000</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.my_post_copyright</span> <span class="selector-class">.post-url</span>:hover &#123;</span><br><span class="line">  <span class="attribute">font-weight</span>: normal;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.my_post_copyright</span> <span class="selector-class">.copy-path</span> &#123;</span><br><span class="line">  <span class="attribute">margin-left</span>: <span class="number">1em</span>;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">1em</span>;</span><br><span class="line">  +mobile()&#123;<span class="attribute">display</span>:none;&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.my_post_copyright</span> <span class="selector-class">.copy-path</span>:hover &#123;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#808080</span>;</span><br><span class="line">  <span class="attribute">cursor</span>: pointer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>修改<code>next/layout/_macro/post.swig</code>，在代码</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">      &#123;% <span class="keyword">if</span> not is_index %&#125;</span><br><span class="line">        &#123;% include <span class="string">'wechat-subscriber.swig'</span> %&#125;</span><br><span class="line">      &#123;% endif %&#125;</span><br><span class="line">&lt;<span class="regexp">/div&gt;</span></span><br></pre></td></tr></table></figure><ul><li>之前添加增加如下代码：</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">      &#123;% <span class="keyword">if</span> not is_index %&#125;</span><br><span class="line">        &#123;% include <span class="string">'my-copyright.swig'</span> %&#125;</span><br><span class="line">      &#123;% endif %&#125;</span><br><span class="line">&lt;<span class="regexp">/div&gt;</span></span><br></pre></td></tr></table></figure><ul><li>修改<code>next/source/css/_common/components/post/post.styl</code>文件，在最后一行增加代码：</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@import <span class="string">"my-post-copyright"</span></span><br></pre></td></tr></table></figure><ul><li>如果要在该博文下面增加版权信息的显示，需要在 Markdown 中增加<code>copyright: true</code>的设置，类似：</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: </span><br><span class="line">date: </span><br><span class="line">tags: </span><br><span class="line">categories: </span><br><span class="line">copyright: <span class="literal">true</span></span><br><span class="line">---</span><br></pre></td></tr></table></figure><h4 id="自定义hexo-new生成md文件的选项"><a href="#自定义hexo-new生成md文件的选项" class="headerlink" title="自定义hexo new生成md文件的选项"></a>自定义hexo new生成md文件的选项</h4><p>在<code>/scaffolds/post.md</code>文件中添加：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">tags:</span><br><span class="line">categories: </span><br><span class="line">copyright: <span class="literal">true</span></span><br><span class="line">permalink: <span class="number">01</span></span><br><span class="line">top: <span class="number">0</span></span><br><span class="line">password:</span><br><span class="line">---</span><br></pre></td></tr></table></figure><h4 id="隐藏网页底部powered-By-Hexo-强力驱动"><a href="#隐藏网页底部powered-By-Hexo-强力驱动" class="headerlink" title="隐藏网页底部powered By Hexo / 强力驱动"></a>隐藏网页底部powered By Hexo / 强力驱动</h4><p>打开<code>themes/next/layout/_partials/footer.swig</code>,使用””隐藏之间的代码即可，或者直接删除。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--</span><br><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"powered-by"</span>&gt;</span><br><span class="line">  &#123;&#123; __(<span class="string">'footer.powered'</span>, <span class="string">'&lt;a class="theme-link" rel="external nofollow" href="https://hexo.io"&gt;Hexo&lt;/a&gt;'</span>) &#125;&#125;</span><br><span class="line">&lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&lt;div class="theme-info"&gt;</span></span><br><span class="line"><span class="regexp">  &#123;&#123; __('footer.theme') &#125;&#125; -</span></span><br><span class="line"><span class="regexp">  &lt;a class="theme-link" rel="external nofollow" href="https:/</span><span class="regexp">/github.com/ii</span>ssnan/hexo-theme-next<span class="string">"&gt;</span></span><br><span class="line"><span class="string">    NexT.&#123;&#123; theme.scheme &#125;&#125;</span></span><br><span class="line"><span class="string">  &lt;/a&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br><span class="line"><span class="string">--&gt;</span></span><br></pre></td></tr></table></figure><h4 id="文章加密访问"><a href="#文章加密访问" class="headerlink" title="文章加密访问"></a>文章加密访问</h4><p>打开<code>themes-&gt;next-&gt;layout-&gt;_partials-&gt;head.swig</code>文件,在<code>meta</code>标签后面插入这样一段代码：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;script&gt;</span><br><span class="line">    (<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="string">'&#123;&#123; page.password &#125;&#125;'</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span> (prompt(<span class="string">'请输入文章密码'</span>) !== <span class="string">'&#123;&#123; page.password &#125;&#125;'</span>)&#123;</span><br><span class="line">                alert(<span class="string">'密码错误！'</span>);</span><br><span class="line">                history.back();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)();</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure><p>然后文章中添加：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">password: nmask</span><br></pre></td></tr></table></figure><p>如果<code>password</code>后面为空，则表示不用密码。</p><h4 id="博文置顶"><a href="#博文置顶" class="headerlink" title="博文置顶"></a>博文置顶</h4><p>修改 <code>hero-generator-index</code> 插件，把文件：<code>node_modules/hexo-generator-index/lib/generator.js</code> 内的代码替换为：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">'use strict'</span>;</span><br><span class="line"><span class="keyword">var</span> pagination = <span class="built_in">require</span>(<span class="string">'hexo-pagination'</span>);</span><br><span class="line"><span class="built_in">module</span>.exports = <span class="function"><span class="keyword">function</span>(<span class="params">locals</span>)</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> config = <span class="keyword">this</span>.config;</span><br><span class="line">  <span class="keyword">var</span> posts = locals.posts;</span><br><span class="line">    posts.data = posts.data.sort(<span class="function"><span class="keyword">function</span>(<span class="params">a, b</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(a.top &amp;&amp; b.top) &#123; <span class="comment">// 两篇文章top都有定义</span></span><br><span class="line">            <span class="keyword">if</span>(a.top == b.top) <span class="keyword">return</span> b.date - a.date; <span class="comment">// 若top值一样则按照文章日期降序排</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">return</span> b.top - a.top; <span class="comment">// 否则按照top值降序排</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(a.top &amp;&amp; !b.top) &#123; <span class="comment">// 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233）</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(!a.top &amp;&amp; b.top) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> b.date - a.date; <span class="comment">// 都没定义按照文章日期降序排</span></span><br><span class="line">    &#125;);</span><br><span class="line">  <span class="keyword">var</span> paginationDir = config.pagination_dir || <span class="string">'page'</span>;</span><br><span class="line">  <span class="keyword">return</span> pagination(<span class="string">''</span>, posts, &#123;</span><br><span class="line">    perPage: config.index_generator.per_page,</span><br><span class="line">    layout: [<span class="string">'index'</span>, <span class="string">'archive'</span>],</span><br><span class="line">    format: paginationDir + <span class="string">'/%d/'</span>,</span><br><span class="line">    data: &#123;</span><br><span class="line">      __index: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>在文章中添加 <code>top</code> 值，数值越大文章越靠前，如:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">......</span><br><span class="line">copyright: <span class="literal">true</span></span><br><span class="line">top: <span class="number">100</span></span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>默认不设置则为0，数值相同时按时间排序。</p><h4 id="添加顶部加载条"><a href="#添加顶部加载条" class="headerlink" title="添加顶部加载条"></a>添加顶部加载条</h4><p>打开<code>/themes/next/layout/_partials/head.swig</code>文件，在<code>maximum-scale=1”/&gt;</code>后添加如下代码:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;script src=<span class="string">"//cdn.bootcss.com/pace/1.0.2/pace.min.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">&lt;link href=<span class="string">"//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css"</span> rel=<span class="string">"stylesheet"</span>&gt;</span><br></pre></td></tr></table></figure><p>但是，默认的是粉色的，要改变颜色可以在<code>/themes/next/layout/_partials/head.swig</code>文件中添加如下代码（接在刚才link的后面）</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;style&gt;</span><br><span class="line">    .pace .pace-progress &#123;</span><br><span class="line">        background: #1E92FB; /*进度条颜色*/</span><br><span class="line">        height: <span class="number">3</span>px;</span><br><span class="line">    &#125;</span><br><span class="line">    .pace .pace-progress-inner &#123;</span><br><span class="line">         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/</span><br><span class="line">    &#125;</span><br><span class="line">    .pace .pace-activity &#123;</span><br><span class="line">        border-top-color: #1E92FB;    /*上边框颜色*/</span><br><span class="line">        border-left-color: #1E92FB;    /*左边框颜色*/</span><br><span class="line">    &#125;</span><br><span class="line">&lt;<span class="regexp">/style&gt;</span></span><br></pre></td></tr></table></figure><h4 id="添加热度"><a href="#添加热度" class="headerlink" title="添加热度"></a>添加热度</h4><p>next主题集成<code>leanCloud</code>，打开<code>/themes/next/layout/_macro/post.swig</code><br>在<code>”leancloud-visitors-count”&gt;</code>标签后面添加<code>℃</code>。<br>然后打开，<code>/themes/next/languages/zh-Hans.yml</code>，将<code>visitors</code>内容改为<em><code>热度</code></em>即可。</p><h4 id="主页文章添加阴影效果"><a href="#主页文章添加阴影效果" class="headerlink" title="主页文章添加阴影效果"></a>主页文章添加阴影效果</h4><p>打开<code>\themes\next\source\css_custom\custom.styl</code>,向里面加入：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 主页文章添加阴影效果</span></span><br><span class="line"> <span class="selector-class">.post</span> &#123;</span><br><span class="line">   <span class="attribute">margin-top</span>: <span class="number">60px</span>;</span><br><span class="line">   <span class="attribute">margin-bottom</span>: <span class="number">60px</span>;</span><br><span class="line">   <span class="attribute">padding</span>: <span class="number">25px</span>;</span><br><span class="line">   -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5);</span><br><span class="line">   -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h4 id="修改文章底部的那个带-号的标签"><a href="#修改文章底部的那个带-号的标签" class="headerlink" title="修改文章底部的那个带#号的标签"></a>修改文章底部的那个带#号的标签</h4><p>修改模板<code>/themes/next/layout/_macro/post.swig</code>，搜索 <code>rel=”tag”&gt;#</code>，将 <code>#</code> 换成<code>&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;</code></p><h4 id="鼠标点击小红心的设置"><a href="#鼠标点击小红心的设置" class="headerlink" title="鼠标点击小红心的设置"></a>鼠标点击小红心的设置</h4><p>将 <a href="https://github.com/Neveryu/Neveryu.github.io/blob/master/js/src/love.js" target="_blank" rel="noopener">love.js</a> 文件添加到 <code>\themes\next\source\js\src</code> 文件目录下。<br>找到 <code>\themes\next\layout_layout.swing</code> 文件， 在文件的后面， 标签之前 添加以下代码：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 页面点击小红心 --&gt;</span><br><span class="line">&lt;script type=<span class="string">"text/javascript"</span> src=<span class="string">"/js/src/love.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h4 id="背景的设置"><a href="#背景的设置" class="headerlink" title="背景的设置"></a>背景的设置</h4><p>将 <a href="https://github.com/Neveryu/Neveryu.github.io/blob/master/js/src/particle.js" target="_blank" rel="noopener">particle.js</a> 文件添加到 <code>\themes\next\source\js\src</code> 文件目录下。<br>找到 <code>\themes\next\layout_layout.swing</code> 文件， 在文件的后面，标签之前 添加以下代码：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 背景动画 --&gt;</span><br><span class="line">&lt;script type=<span class="string">"text/javascript"</span> src=<span class="string">"/js/src/particle.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h4 id="修改文章内链接文本样式"><a href="#修改文章内链接文本样式" class="headerlink" title="修改文章内链接文本样式"></a>修改文章内链接文本样式</h4><p>将链接文本设置为蓝色，鼠标划过时文字颜色加深，并显示下划线。<br>找到文件 <code>themes\next\source\css\_custom\custom.styl</code> ，添加如下 <code>css</code> 样式：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.post-body</span> <span class="selector-tag">p</span> <span class="selector-tag">a</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#0593d3</span>;</span><br><span class="line">  <span class="attribute">border-bottom</span>: none;</span><br><span class="line">  &amp;:hover &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#0477ab</span>;</span><br><span class="line">    <span class="attribute">text-decoration</span>: underline;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="博文压缩"><a href="#博文压缩" class="headerlink" title="博文压缩"></a>博文压缩</h4><p>在站点的根目录下执行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> npm install gulp -g</span><br><span class="line"><span class="meta">$</span> npm install gulp-minify-css gulp-uglify gulp-htmlmin gulp-htmlclean gulp --save</span><br></pre></td></tr></table></figure><p>在博客根目录下新建 <code>gulpfile.js</code> ，并填入以下内容：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> gulp = <span class="built_in">require</span>(<span class="string">'gulp'</span>);</span><br><span class="line"><span class="keyword">var</span> minifycss = <span class="built_in">require</span>(<span class="string">'gulp-minify-css'</span>);</span><br><span class="line"><span class="keyword">var</span> uglify = <span class="built_in">require</span>(<span class="string">'gulp-uglify'</span>);</span><br><span class="line"><span class="keyword">var</span> htmlmin = <span class="built_in">require</span>(<span class="string">'gulp-htmlmin'</span>);</span><br><span class="line"><span class="keyword">var</span> htmlclean = <span class="built_in">require</span>(<span class="string">'gulp-htmlclean'</span>);</span><br><span class="line"><span class="comment">// 压缩 public 目录 css</span></span><br><span class="line">gulp.task(<span class="string">'minify-css'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> gulp.src(<span class="string">'./public/**/*.css'</span>)</span><br><span class="line">        .pipe(minifycss())</span><br><span class="line">        .pipe(gulp.dest(<span class="string">'./public'</span>));</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 压缩 public 目录 html</span></span><br><span class="line">gulp.task(<span class="string">'minify-html'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> gulp.src(<span class="string">'./public/**/*.html'</span>)</span><br><span class="line">    .pipe(htmlclean())</span><br><span class="line">    .pipe(htmlmin(&#123;</span><br><span class="line">         removeComments: <span class="literal">true</span>,</span><br><span class="line">         minifyJS: <span class="literal">true</span>,</span><br><span class="line">         minifyCSS: <span class="literal">true</span>,</span><br><span class="line">         minifyURLs: <span class="literal">true</span>,</span><br><span class="line">    &#125;))</span><br><span class="line">    .pipe(gulp.dest(<span class="string">'./public'</span>))</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 压缩 public/js 目录 js</span></span><br><span class="line">gulp.task(<span class="string">'minify-js'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> gulp.src(<span class="string">'./public/**/*.js'</span>)</span><br><span class="line">        .pipe(uglify())</span><br><span class="line">        .pipe(gulp.dest(<span class="string">'./public'</span>));</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 执行 gulp 命令时执行的任务</span></span><br><span class="line">gulp.task(<span class="string">'default'</span>, [</span><br><span class="line">    <span class="string">'minify-html'</span>,<span class="string">'minify-css'</span>,<span class="string">'minify-js'</span></span><br><span class="line">]);</span><br></pre></td></tr></table></figure><p>生成博文是执行 <code>hexo g &amp;&amp; gulp</code> 就会根据 <code>gulpfile.js</code> 中的配置，对 <code>public</code> 目录中的静态资源文件进行压缩。</p><h4 id="搜索功能"><a href="#搜索功能" class="headerlink" title="搜索功能"></a>搜索功能</h4><p>安装 <code>hexo-generator-searchdb</code>，在站点的根目录下执行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure><p>编辑 站点配置文件，新增以下内容到任意位置：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: <span class="number">10000</span></span><br></pre></td></tr></table></figure><h4 id="增加阅读排行统计页面"><a href="#增加阅读排行统计页面" class="headerlink" title="增加阅读排行统计页面"></a>增加阅读排行统计页面</h4><p>首先我们可以使用<code>leancloud</code>来统计页面阅读数量，以及储存这些信息，然后通过<code>leancloud</code>提供的<code>api</code>编写<code>js</code>脚本来获取阅读数量信息，并展示在页面上。<br>首先新建一个<code>page</code>页面，<code>hexo new page “”</code>,然后编辑此<code>.md</code>文件，写下：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&lt;script src=<span class="string">"https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">&lt;script&gt;AV.initialize(<span class="string">""</span>, <span class="string">""</span>);<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span> <span class="comment">//需要写上leancloud的key</span></span><br><span class="line"></span><br><span class="line">&lt;script type=<span class="string">"text/javascript"</span>&gt;</span><br><span class="line">  <span class="keyword">var</span> time=<span class="number">0</span></span><br><span class="line">  <span class="keyword">var</span> title=<span class="string">""</span></span><br><span class="line">  <span class="keyword">var</span> url=<span class="string">""</span></span><br><span class="line">  <span class="keyword">var</span> query = <span class="keyword">new</span> AV.Query(<span class="string">'Counter'</span>);<span class="comment">//表名</span></span><br><span class="line">  query.notEqualTo(<span class="string">'id'</span>,<span class="number">0</span>); <span class="comment">//id不为0的结果</span></span><br><span class="line">  query.descending(<span class="string">'time'</span>); <span class="comment">//结果按阅读次数降序排序</span></span><br><span class="line">  query.limit(<span class="number">20</span>);  <span class="comment">//最终只返回10条结果</span></span><br><span class="line">  query.find().then(<span class="function"><span class="keyword">function</span> (<span class="params">todo</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> i=<span class="number">0</span>;i&lt;<span class="number">10</span>;i++)&#123; </span><br><span class="line">      <span class="keyword">var</span> result=todo[i].attributes;</span><br><span class="line">      time=result.time;  <span class="comment">//阅读次数</span></span><br><span class="line">      title=result.title; <span class="comment">//文章标题</span></span><br><span class="line">      url=result.url;     <span class="comment">//文章url</span></span><br><span class="line">      <span class="keyword">var</span> content=<span class="string">"&lt;p&gt;"</span>+<span class="string">"&lt;font color='#0477ab'&gt;"</span>+<span class="string">"【阅读次数:"</span>+time+<span class="string">"】"</span>+<span class="string">"&lt;a href='"</span>+<span class="string">"http://thief.one"</span>+url+<span class="string">"'&gt;"</span>+title+<span class="string">"&lt;/font&gt;"</span>+<span class="string">"&lt;/a&gt;"</span>+<span class="string">"&lt;/p&gt;"</span>;</span><br><span class="line">      <span class="comment">// document.write("&lt;a href='"+"http://thief.one/"+url+"'&gt;"+title+"&lt;/a&gt;"+"    Readtimes:"+time+"&lt;br&gt;");</span></span><br><span class="line">      <span class="built_in">document</span>.getElementById(<span class="string">"heheda"</span>).innerHTML+=content</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;, <span class="function"><span class="keyword">function</span> (<span class="params">error</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">"error"</span>);</span><br><span class="line">  &#125;);</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure><p>最终的效果查看：<a href="http://thief.one/count" target="_blank" rel="noopener">http://thief.one/count</a></p><h4 id="多说替换成来必力评论"><a href="#多说替换成来必力评论" class="headerlink" title="多说替换成来必力评论"></a>多说替换成来必力评论</h4><p>更新于@2017年5月18日<br>多说已经宣布下线了，因此我找了个来必力评论系统来替换，以下是替换的教程，教程内容来自：<a href="https://blog.smoker.cc/web/add-comments-livere-for-hexo-theme-next.html" target="_blank" rel="noopener">https://blog.smoker.cc/web/add-comments-livere-for-hexo-theme-next.html</a></p><p>来必力评价<br>优点：界面美观<br>缺点：不支持数据导入，加载慢</p><p>首先在 <code>_config.yml</code> 文件中添加如下配置：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">livere_uid: your uid</span><br></pre></td></tr></table></figure><p>其中 <code>livere_uid</code> 即注册来必力获取到的 <code>uid</code>。<br>在 <code>layout/_scripts/third-party/comments/</code> 目录中添加 <code>livere.swig</code>，文件内容如下：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;% <span class="keyword">if</span> not (theme.duoshuo and theme.duoshuo.shortname) and not theme.duoshuo_shortname and not theme.disqus_shortname and not theme.hypercomments_id and not theme.gentie_productKey %&#125;</span><br><span class="line">  &#123;% <span class="keyword">if</span> theme.livere_uid %&#125;</span><br><span class="line">    &lt;script type=<span class="string">"text/javascript"</span>&gt;</span><br><span class="line">      (<span class="function"><span class="keyword">function</span>(<span class="params">d, s</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">var</span> j, e = d.getElementsByTagName(s)[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">typeof</span> LivereTower === <span class="string">'function'</span>) &#123; <span class="keyword">return</span>; &#125;</span><br><span class="line">        j = d.createElement(s);</span><br><span class="line">        j.src = <span class="string">'https://cdn-city.livere.com/js/embed.dist.js'</span>;</span><br><span class="line">        j.async = <span class="literal">true</span>;</span><br><span class="line">        e.parentNode.insertBefore(j, e);</span><br><span class="line">      &#125;)(<span class="built_in">document</span>, <span class="string">'script'</span>);</span><br><span class="line">    &lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">  &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure><p>​        优先使用其他评论插件，如果其他评论插件没有开启，且<code>LiveRe</code>评论插件配置开启了，则使用<code>LiveRe</code>。其中脚本代码为上一步管理页面中获取到的。在<code>layout/_scripts/third-party/comments.swig</code>文件中追加：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% include <span class="string">'./comments/livere.swig'</span> %&#125;</span><br></pre></td></tr></table></figure><p>引入 <code>LiveRe</code> 评论插件。<br>最后，在 <code>layout/_partials/comments.swig</code> 文件中条件最后追加<code>LiveRe</code>插件是否引用的判断逻辑：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% elseif theme.livere_uid %&#125;</span><br><span class="line">      &lt;div id=<span class="string">"lv-container"</span> data-id=<span class="string">"city"</span> data-uid=<span class="string">"&#123;&#123; theme.livere_uid &#125;&#125;"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>最后打开博客瞧瞧吧！</p><h4 id="多说替换成网易云跟贴"><a href="#多说替换成网易云跟贴" class="headerlink" title="多说替换成网易云跟贴"></a>多说替换成网易云跟贴</h4><p>最好的方法就是更新next主题，因为最新版本的主题已经支持这几种评论。<br>如果不想更新主题，则往下看：</p><p>网易云跟贴评价：<br>性能稳定，功能中规中矩，支持数据导入</p><p>首先在 <code>_config.yml</code> 文件中添加如下配置：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gentie_productKey: #your-gentie-product-key</span><br></pre></td></tr></table></figure><p>其中 <code>gentie_productKey</code> 即注册网易云跟贴获取到的<code>key</code>。<br>在 <code>layout/_scripts/third-party/comments/</code> 目录中添加 <code>gentie.swig</code>，文件内容如下：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;% <span class="keyword">if</span> not (theme.duoshuo and theme.duoshuo.shortname) and not theme.duoshuo_shortname and not theme.disqus_shortname and not theme.hypercomments_id %&#125;</span><br><span class="line"></span><br><span class="line">  &#123;% <span class="keyword">if</span> theme.gentie_productKey %&#125;</span><br><span class="line">    &#123;% <span class="keyword">set</span> gentie_productKey = theme.gentie_productKey %&#125;</span><br><span class="line">    &lt;script&gt;</span><br><span class="line">      var cloudTieConfig = &#123;</span><br><span class="line">        url: <span class="built_in">document</span>.location.href, </span><br><span class="line">        sourceId: <span class="string">""</span>,</span><br><span class="line">        productKey: <span class="string">"&#123;&#123;gentie_productKey&#125;&#125;"</span>,</span><br><span class="line">        target: <span class="string">"cloud-tie-wrapper"</span></span><br><span class="line">      &#125;;</span><br><span class="line">    &lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">    &lt;script src="https:/</span><span class="regexp">/img1.ws.126.net/</span>f2e/tie/yun/sdk/loader.js<span class="string">"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="string">  &#123;% endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure><p>在<code>layout/_scripts/third-party/comments.swig</code>文件中追加：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% include <span class="string">'./comments/gentie.swig'</span> %&#125;</span><br></pre></td></tr></table></figure><p>最后，在 <code>layout/_partials/comments.swig</code> 文件中条件最后追加网易云跟帖插件引用的判断逻辑：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% elseif theme.gentie_productKey %&#125;</span><br><span class="line">      &lt;div id=<span class="string">"cloud-tie-wrapper"</span> <span class="class"><span class="keyword">class</span></span>=<span class="string">"cloud-tie-wrapper"</span>&gt;</span><br><span class="line">      &lt;<span class="regexp">/div&gt;</span></span><br></pre></td></tr></table></figure><h3 id="报错解决"><a href="#报错解决" class="headerlink" title="报错解决"></a>报错解决</h3><h4 id="（一）Deployer-not-found-git"><a href="#（一）Deployer-not-found-git" class="headerlink" title="（一）Deployer not found: git"></a>（一）Deployer not found: git</h4><p>当编辑<code>__config.yml</code>文件，将<code>type: git</code>设置完成后，运行<code>hexo g</code> 报错：<em><code>git not found</code></em><br>解决方案：可以在<code>MyBlog</code>目录下运行: <em><code>npm install hexo-deployer-git –save</code></em>。</p><h4 id="（二）permission-denied"><a href="#（二）permission-denied" class="headerlink" title="（二）permission denied"></a>（二）permission denied</h4><p>当执行: <code>hexo  deploy</code> 报错时，把<code>__config.yml</code>中的<code>github</code>连接形式从<code>ssh</code>改成<code>http</code>。</p><h4 id="（三）当在themes目录下载主题时，报错。"><a href="#（三）当在themes目录下载主题时，报错。" class="headerlink" title="（三）当在themes目录下载主题时，报错。"></a>（三）当在themes目录下载主题时，报错。</h4><p>将该目录只读属性取消。</p><h4 id="（四）genrnate-报错"><a href="#（四）genrnate-报错" class="headerlink" title="（四）genrnate 报错"></a>（四）genrnate 报错</h4><p>检查<code>_config.yml</code>配置中，键值对冒号后面是否已经预留了一个半角空格。</p><h4 id="（五）ERROR-Plugin-load-failed-hexo-generator-feed"><a href="#（五）ERROR-Plugin-load-failed-hexo-generator-feed" class="headerlink" title="（五）ERROR Plugin load failed: hexo-generator-feed"></a>（五）ERROR Plugin load failed: hexo-generator-feed</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-feed</span><br><span class="line">npm install hexo-generator-feed --save</span><br></pre></td></tr></table></figure><h4 id="（六）fatal-The-remote-end-hung-up-unexpectedly"><a href="#（六）fatal-The-remote-end-hung-up-unexpectedly" class="headerlink" title="（六）fatal: The remote end hung up unexpectedly"></a>（六）fatal: The remote end hung up unexpectedly</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> git config https.postBuffer 524288000</span><br><span class="line"><span class="meta">$</span> git config http.postBuffer 524288000</span><br><span class="line"><span class="meta">$</span> git config ssh.postBuffer 524288000</span><br></pre></td></tr></table></figure><h4 id="（七）hero-d推送的内容有问题"><a href="#（七）hero-d推送的内容有问题" class="headerlink" title="（七）hero d推送的内容有问题"></a>（七）hero d推送的内容有问题</h4><p>　　首先检查下<code>.deploy_git</code>文件夹下的<code>.git</code>文件是否存在，此<code>.git</code>文件指定了<code>hexo d</code>时推送public文件夹，而不是所有的内容。如果此<code>.git</code>文件不存在，则会出现推送内容错误。<br>　　用<code>npm  install hexo-deployer-git  –save</code>生成的<code>.deploy_git</code>不包含.git文件，因此正确的做法是<code>.deploy_git</code>文件夹也需要备份，然后再用<code>npm install  hexo-deployer-git –save</code>更新一下其内容即可。<br>　　如果已经出现这个错误，则删除<code>.deploy_git</code>，重新<code>hexo d</code>。</p><h4 id="（八）hexo-s报错"><a href="#（八）hexo-s报错" class="headerlink" title="（八）hexo s报错"></a>（八）hexo s报错</h4><p>在新版本的mac上，安装运行<code>hexo</code>会报此错误，但不影响使用。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; <span class="attr">Error</span>: Cannot find <span class="built_in">module</span></span><br></pre></td></tr></table></figure><p>解决方案：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo --no-optional</span><br></pre></td></tr></table></figure><h3 id="Local-Search错误"><a href="#Local-Search错误" class="headerlink" title="Local Search错误"></a>Local Search错误</h3><p>　　最近发现Local Search搜索出来的连接有错误，到不是说连接不对，而是当我在/aaa/目录下搜索一个页面时，跳转到了/aaa/正确的连接/，这样明显是正确的，应该是跟目录+跳转的目录。<br>　　网上搜索了下，没有类似的案例，那么自己动手修改吧，打开<code>node_modules/hexo-generator-searchdb/templates</code>下的<code>xml.ejs</code>文件：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;url&gt;<span class="xml"><span class="tag">&lt;<span class="name">%-</span> ("<span class="attr">..</span>/<span class="attr">..</span>/<span class="attr">..</span>/<span class="attr">..</span>/<span class="attr">..</span>/<span class="attr">..</span>/<span class="attr">..</span>/<span class="attr">..</span>/"+<span class="attr">post.path</span>) %&gt;</span><span class="tag">&lt;/<span class="name">url</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>说明：将这个文件的两处url都改成这样就可以了。</p><h3 id="异地同步博客内容"><a href="#异地同步博客内容" class="headerlink" title="异地同步博客内容"></a>异地同步博客内容</h3><p>　　现在电脑已经很普及了，因为一般来说我们都是公司一台电脑，家里一台电脑，那么如何将两台电脑上博客的内容同步内，即两台电脑上都可以编辑更新博客？<br>要解决这个问题，首先我们要清楚我们博客文件的组成：</p><ul><li>node_modules</li><li>public</li><li>scaffolds</li><li>source</li><li>themes</li><li>_config_yml</li><li>db.json</li><li>package.json</li><li>.deploy_git</li></ul><p>　　以上为利用hexo生成的博客全部内容，那么当我们执行hexo d时，正真被推送到github上的又有哪些内容呢？<br>　　我们可以看下github上的<code>code-hly.github.io</code>项目，发现里面只有<code>Public</code>目录下的内容。也就是说，我们博客上呈现的内容，其实就是<code>public</code>下的文件内容。那么这个Pulic目录是怎么生成的呢？<br>　　一开始<code>hexo  init</code>的时候是没有<code>public</code>目录的，而当我们运行<code>hexo g</code>命令时，<code>public`</code>目录被生成了，换句话说hexo  g<code>命令就是用来生成博客文件的（会根据</code>_config.yml<code>，</code>source<code>目录文件以及</code>themes<code>目录下文件生成）。同样当我们运行</code>hexo  clean<code>命令时，</code>public<code>目录被删除了。　　好了，既然我们知道了决定博客显示内容的只有一个</code>Public<code>目录，而</code>public<code>目录又是可以动态生成的，那么其实我们只要在不同电脑上同步可以生成</code>Public`目录的文件即可。</p><p>以下文件以及目录是必须要同步的：</p><ul><li>source</li><li>themes</li><li>_config.yml</li><li>db.json</li><li>package.json</li><li>.deploy_git</li></ul><p>　　同步的方式有很多种，可以每次更新后都备份到一个地址。我采用<code>github</code>去备份，也就是新建一个项目用来存放以上文件，每次更新后推送到github上，用作备份同步。<br>　　同步完必须的文件后，怎么再其他电脑上也可以更新博客呢？<br>　　前提假设我们现在配置了一台新电脑，里面没有安装任何有关博客的东西，那么我们开始吧：</p><ul><li>下载<code>node.js</code>并安装（官网下载安装），默认会安装npm。</li><li>下载安装<code>git</code>（官网下载安装）</li><li>下载安装<code>hexo</code>。方法：打开<code>cmd</code> 运行<em><code>npm install -g hexo</code></em>（要翻墙） </li><li>新建一个文件夹，如<code>MyBlog</code></li><li>进入该文件夹内，右击运行<code>git</code>，输入：<em><code>hexo init</code></em>（生成hexo模板，可能要翻墙)</li></ul><p>　　我们重复了一开始搭建博客的步骤，重新生成了一个新的模板，这个模板中包含了hexo生成的一些文件。</p><ul><li><code>git clone</code> 我们备份的项目，生成一个文件夹，如：<code>MyBlogData</code></li><li>将<code>MyBlog</code>里面的<code>node_modules</code>、<code>scaffolds</code>文件夹复制到<code>MyBlogData</code>里面。</li></ul><p>　　做完这些，从表面上看，两台电脑上MyBlogData目录下的文件应该都是一样的了。那么我们运行hexo g<br>hexo d试试，如果会报错，则往下看。</p><ul><li>这是因为<code>.deploy_git</code>没有同步，在<code>MyBlogData</code>目录内运行:<em><code>npm install hexo-deployer-git –save</code></em>后再次推送即可</li></ul><p>　　总结流程：当我们每次更新<code>MyBlog</code>内容后，先利用<code>hexo</code>将<code>public</code>推送到<code>github</code>，然后再将其余必须同步的文件利用<code>git</code>推送到<code>github</code>。</p><h3 id="SEO优化"><a href="#SEO优化" class="headerlink" title="SEO优化"></a>SEO优化</h3><p>seo优化对于网站是否能被搜索引擎快速收录有很大帮助，因此适当做一些seo还是有必要的，以下内容参考：<a href="https://lancelot_lewis.coding.me/2016/08/16/blog/Hexo-NexT-SEO/" target="_blank" rel="noopener">https://lancelot_lewis.coding.me/2016/08/16/blog/Hexo-NexT-SEO/</a></p><h4 id="添加sitemap文件"><a href="#添加sitemap文件" class="headerlink" title="添加sitemap文件"></a>添加sitemap文件</h4><p>安装以下2个插件，然后重启<code>hexo</code>后，网站根目录（source）下会生成<code>sitemap.xml</code>与<code>baidusitemap.xml</code>文件，搜索引擎在爬取时会参照文件中的url去收录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-sitemap --save-dev</span><br><span class="line">hexo d -g</span><br><span class="line">npm install hexo-generator-baidu-sitemap --save-dev</span><br><span class="line">hexo d -g</span><br></pre></td></tr></table></figure><h4 id="添加robots-txt"><a href="#添加robots-txt" class="headerlink" title="添加robots.txt"></a>添加robots.txt</h4><p>新建<code>robots.txt</code>文件，添加以下文件内容，把<code>robots.txt</code>放在<code>hexo</code>站点的<code>source</code>文件下。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">User-agent: * Allow: <span class="regexp">/</span></span><br><span class="line"><span class="regexp">Allow: /</span>archives/</span><br><span class="line">Disallow: <span class="regexp">/vendors/</span></span><br><span class="line">Disallow: <span class="regexp">/js/</span></span><br><span class="line">Disallow: <span class="regexp">/css/</span></span><br><span class="line">Disallow: <span class="regexp">/fonts/</span></span><br><span class="line">Disallow: <span class="regexp">/vendors/</span></span><br><span class="line">Disallow: <span class="regexp">/fancybox/</span></span><br><span class="line"></span><br><span class="line">Sitemap: http:<span class="comment">//thief.one/sitemap.xml</span></span><br><span class="line">Sitemap: http:<span class="comment">//thief.one/baidusitemap.xml</span></span><br></pre></td></tr></table></figure><h4 id="首页title的优化"><a href="#首页title的优化" class="headerlink" title="首页title的优化"></a>首页title的优化</h4><p>更改<code>index.swig</code>文件，文件路径是<code>your-hexo-site\themes\next\layout</code>，将下面代码</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% block title %&#125;  &#123;&#123; config.title &#125;&#125;  &#123;% endblock %&#125;</span><br></pre></td></tr></table></figure><p>改成</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% block title %&#125;  &#123;&#123; config.title &#125;&#125; - &#123;&#123; theme.description &#125;&#125;  &#123;% endblock</span><br></pre></td></tr></table></figure><p>观察首页title就是标题+描述了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​        现在越来越多的人喜欢利用Github搭建静态网站，原因不外乎简单省钱。本人也利用hexo+github搭建了本博客，用于分享一些心得。在此过程中，折腾博客的各种配置以及功能占具了我一部分时间，在此详细记录下我是如何利用hexo+github搭建静态博客以及一些配置相关问题，以免过后遗忘，且当备份之用。&lt;/p&gt;
    
    </summary>
    
      <category term="Hexo" scheme="http://yoursite.com/categories/Hexo/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>ceph部署难题</title>
    <link href="http://yoursite.com/2019/05/08/ceph-%E9%83%A8%E7%BD%B2%E9%9A%BE%E9%A2%98%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2019/05/08/ceph-部署难题（一）/</id>
    <published>2019-05-08T14:16:02.000Z</published>
    <updated>2019-05-09T01:26:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>本章总结了搭建ceph集群过程中遇到的各种问题，以及相应的原理过程</p><a id="more"></a><h3 id="Q1-环境预准备"><a href="#Q1-环境预准备" class="headerlink" title="Q1. 环境预准备**"></a>Q1. 环境预准备**</h3><p>​        绝大多数MON创建的失败都是由于防火墙没有关导致的，亦或是SeLinux没关闭导致的。一定一定一定要关闭每个每个每个节点的防火墙(执行一次就好，没安装报错就忽视)：</p><p><strong>CentOS</strong></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'s/SELINUX=.*/SELINUX=disabled/'</span> /etc/selinux/config</span><br><span class="line">setenforce <span class="number">0</span></span><br><span class="line">systemctl stop firewalld </span><br><span class="line">systemctl disable firewalld</span><br><span class="line"># iptables -F</span><br><span class="line">service iptables stop</span><br></pre></td></tr></table></figure><h3 id="Q2-清理环境"><a href="#Q2-清理环境" class="headerlink" title="Q2. 清理环境"></a><strong>Q2. 清理环境</strong></h3><p>​        MON部署不上的第二大问题就是在旧的节点部署MON，或者在这个节点部署MON失败了，然后重新<code>new</code>再<code>mon create-initial</code>，请查看要部署MON的节点上的<code>/var/lib/ceph/mon/</code>目录下是否为空，如果不为空，说明已经在这个目录部署过MON，再次部署会检测子目录下的<code>done</code>文件，由于有了这个文件，就不会再建立新的MON数据库，并且不会覆盖之，导致了部署时的各种异常，这里就不赘述了，直接给出万能清理大法：</p><p><strong>对于任何需要新部署MON的节点，请到这个节点下执行如下指令，确保环境已经清理干净：</strong></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ps aux|grep ceph |awk <span class="string">'&#123;print $2&#125;'</span>|xargs kill <span class="number">-9</span></span><br><span class="line">ps -ef|grep ceph</span><br><span class="line">#确保此时所有ceph进程都已经关闭！！！如果没有关闭，多执行几次。</span><br><span class="line">rm -rf /<span class="keyword">var</span>/lib/ceph/mon<span class="comment">/*</span></span><br><span class="line"><span class="comment">rm -rf /var/lib/ceph/bootstrap-mds/*</span></span><br><span class="line"><span class="comment">rm -rf /var/lib/ceph/bootstrap-osd/*</span></span><br><span class="line"><span class="comment">rm -rf /var/lib/ceph/bootstrap-rgw/*</span></span><br><span class="line"><span class="comment">rm -rf /etc/ceph/*</span></span><br><span class="line"><span class="comment">rm -rf /var/run/ceph/*</span></span><br></pre></td></tr></table></figure><p>请直接复制粘贴，遇到过好些个自己打错打漏删了目录的。</p><h3 id="Q3-部署前最后的确认"><a href="#Q3-部署前最后的确认" class="headerlink" title="Q3. 部署前最后的确认"></a><strong>Q3. 部署前最后的确认</strong></h3><p>这里介绍的都是个案，不过还是需要提一下：</p><ul><li>确保每个节点的<code>hostname</code>都设置正确，并且添加至<code>/etc/hosts</code>文件中，然后同步到所有节点下。克隆出来的虚拟机或者批量建的虚拟机有可能发生此情形。</li><li>确保以下目录在各个节点都存在：</li><li><code>/var/lib/ceph/</code></li><li><code>/var/lib/ceph/mon/</code></li><li><code>/var/lib/ceph/osd/</code></li><li><code>/etc/ceph/</code></li><li><code>/var/run/ceph/</code></li><li>上面的目录，如果Ceph版本大于等于<code>jewel</code>,请确认权限均为<code>ceph:ceph</code>，如果是<code>root:root</code>，请自行<code>chown</code>。</li></ul><h3 id="Q4-安装Ceph"><a href="#Q4-安装Ceph" class="headerlink" title="Q4. 安装Ceph"></a><strong>Q4. 安装Ceph</strong></h3><p>​        官网指导方法是使用<code>ceph-deploy install nodeX</code>,但是因为是国外的源，速度慢得令人发指，所以我们换到阿里的源，并且使用<code>yum install</code>的方式安装，没差啦其实，这样反而还快点，毕竟多个节点一起装。</p><p><strong>很多安装失败的都是因为没有添加epel源请在每个存储节点都执行以下指令，来安装Ceph:</strong></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">yum clean all</span><br><span class="line">rm -rf /etc/yum.repos.d<span class="comment">/*.repo</span></span><br><span class="line"><span class="comment">wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span></span><br><span class="line"><span class="comment">wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span></span><br><span class="line"><span class="comment">sed -i '/aliyuncs/d' /etc/yum.repos.d/CentOS-Base.repo</span></span><br><span class="line"><span class="comment">sed -i '/aliyuncs/d' /etc/yum.repos.d/epel.repo</span></span><br><span class="line"><span class="comment">sed -i 's/$releasever/7.2.1511/g' /etc/yum.repos.d/CentOS-Base.repo</span></span><br><span class="line"><span class="comment">echo "</span></span><br><span class="line"><span class="comment">[ceph]</span></span><br><span class="line"><span class="comment">name=ceph</span></span><br><span class="line"><span class="comment">baseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el7/x86_64/</span></span><br><span class="line"><span class="comment">gpgcheck=0</span></span><br><span class="line"><span class="comment">[ceph-noarch]</span></span><br><span class="line"><span class="comment">name=cephnoarch</span></span><br><span class="line"><span class="comment">baseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el7/noarch/</span></span><br><span class="line"><span class="comment">gpgcheck=0</span></span><br><span class="line"><span class="comment">" &gt; /etc/yum.repos.d/ceph.repo</span></span><br><span class="line"><span class="comment">yum install ceph ceph-radosgw -y</span></span><br></pre></td></tr></table></figure><p>这里是安装的<code>hammer</code>版本的Ceph，如果需要安装<code>jewel</code>版本的，请执行：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'s/hammer/jewel/'</span> /etc/yum.repos.d/ceph.repo</span><br><span class="line">yum install ceph ceph-radosgw -y</span><br></pre></td></tr></table></figure><p>如果安装了<code>jewel</code>版本的Ceph，想要换回<code>hammer</code>版本的Ceph，可以执行下面的指令：</p><p><strong>卸载Ceph客户端</strong></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa |grep <span class="string">`ceph -v |awk '&#123;print $3&#125;'`</span> |xargs rpm -e --nodeps</span><br></pre></td></tr></table></figure><p><strong>更改ceph.repo里面的Ceph版本</strong></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'s/jewel/hammer/'</span> /etc/yum.repos.d/ceph.repo</span><br><span class="line">yum install ceph ceph-radosgw -y</span><br></pre></td></tr></table></figure><h3 id="Q5-ceph-deploy"><a href="#Q5-ceph-deploy" class="headerlink" title="Q5. ceph-deploy"></a><strong>Q5. ceph-deploy</strong></h3><p>这里我要开启话唠模式：</p><p><strong>① Ceph-deploy 是什么？</strong></p><p>​        Ceph-deploy是Ceph官方给出的用于<strong>部署Ceph</strong>的一个工具，这个工具几乎全部是Python写的脚本，其代码位于<code>/usr/lib/python2.7/site-packages/ceph_deploy</code>目录下(<code>1.5.36</code>版本)。最主要的功能就是用几个简单的指令部署好一个集群，而不是手动部署操碎了心，敲错一个地方就可能失败。所以对于新人来说，或者说以我的经验，接触Ceph少于一个月的，又或者说，集群规模不上PB的，都没有必要手动部署，Ceph-deploy完全足够了。</p><p><strong>② Ceph-deploy怎么装?</strong></p><p>​        这个包在ceph的源里面：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install ceph-deploy -y</span><br></pre></td></tr></table></figure><p><strong>③Ceph-deploy装在哪？</strong></p><p>​        既然Ceph-deploy只是个部署Ceph的脚本工具而已，那么这个工具随便装在哪个节点都可以，<strong>并不需要单独为了装这个工具再搞个节点</strong>，我一般习惯放在第一个节点，以后好找部署目录。</p><p><strong>④Ceph-deploy怎么用？</strong></p><p>​        详细的指令暂时不介绍，下面会有，在安装好后，需要在这个节点新建一个目录，用作<code>部署目录</code>，这里是强烈建议建一个单独的目录的，比如我习惯在集群的第一个节点下建一个<code>/root/cluster</code>目录，为了以后好找。<strong>Ceph-deploy的所有的指令都需要在这个目录下执行</strong>。包括<code>new,mon,osd</code>等等一切ceph-deploy的指令都需要在这个部署目录下执行！最后一遍，所有的<code>ceph-deploy</code>的指令都要在部署目录下执行！否则就会报下面的错：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ceph_deploy][ERROR ] ConfigError: Cannot load config: [Errno <span class="number">2</span>] No such file or directory: <span class="string">'ceph.conf'</span>; has ceph-deploy <span class="keyword">new</span> been run <span class="keyword">in</span> <span class="keyword">this</span> directory?</span><br></pre></td></tr></table></figure><p><strong>⑤ Ceph-deploy怎么部署集群?</strong></p><p>​        我们暂且把<strong>部署目录</strong>所在的节点叫做<strong>部署节点</strong>。Ceph-deploy通过SSH到各个节点，然后再在各个节点执行本机的Ceph指令来创建MON或者OSD等。所以在部署之前，你需要从<code>部署节点ssh-copy-id</code>到各个集群节点，使其可以免秘钥登陆。</p><p><strong>⑥Ceph-deploy部署的日志在哪里?</strong></p><p>​        就在部署目录下面的<code>ceph-deploy-ceph.log</code>文件，部署过程中产生的所有的日志都会保存在里面，比如你大半年前敲的创建OSD的指令。在哪个目录下执行ceph-deploy指令，就会在这个目录下生成log，如果你跑到别的目录下执行，就会在执行目录里生成log再记下第四点的错。当然，这个LOG最有用的地方还是里面记录的部署指令，你可以通过<code>cat ceph-deploy-ceph.log |grep &quot;Running command&quot;</code>查看到创建一个集群所需的所有指令，这对你手动建立集群或者创建秘钥等等等等有着很大的帮助！！！</p><p><strong>⑦ Ceph-deploy版本</strong></p><p>​        写这段时的最新的版本号为<code>1.5.36</code>，下载链接为ceph-deploy-1.5.36-0.noarch.rpm， 之前的<code>1.5.35</code>里面有点bug在这个版本被修复了，如果使用<code>1.5.25</code>部署遇到了问题，可以更新至这个版本，会绕过一些坑。更新到<code>1.5.36</code>之后，腰也不酸了,退了不疼了，Ceph也能部署上了。</p><h3 id="Q6-ceph-deploy-new-做了什么"><a href="#Q6-ceph-deploy-new-做了什么" class="headerlink" title="Q6. ceph-deploy new 做了什么"></a><strong>Q6. ceph-deploy new 做了什么</strong></h3><p>​        <strong>进入部署目录</strong>，执行<code>ceph-deploy new node1 node2 node3</code>，会生成两个文件（第三个是<code>ceph-deploy-ceph.log</code>，忽视之）:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@blog cluster]# ls</span><br><span class="line">ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring</span><br></pre></td></tr></table></figure><p>​        <code>new</code>后面跟的是你即将部署MON的节点的<code>hostname</code>，推荐三个就够了，需要是奇数个MON节点。不要因为只有两个节点就搞两个MON，两个节点请用一个MON，因为两个MON挂掉一个，集群也就挂了，和一个MON挂掉一个效果是一样的。生成的<code>ceph.conf</code>默认情况下长成这样：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@blog cluster]# cat ceph.conf </span><br><span class="line">[global]</span><br><span class="line">fsid = <span class="number">13</span>b5d863<span class="number">-75</span>aa<span class="number">-479</span>d<span class="number">-84</span>ba<span class="number">-9e5</span>edd881ec9</span><br><span class="line">mon_initial_members = blog</span><br><span class="line">mon_host = <span class="number">1.2</span><span class="number">.3</span><span class="number">.4</span></span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br></pre></td></tr></table></figure><p>​        会调用<code>uuidgen</code>生成一个<code>fsid</code>，用作集群的唯一ID，再将<code>new</code>后面的主机加入到<code>mon_initial_members</code>和<code>mon_host</code>里面，剩下的三行大家都是一样的，默认开启CephX认证。下面有一节会专门介绍这个，需要注意的是，<strong>部署的时候，千万不要动这三行</strong> 下面会有一节介绍之。还有一个文件<code>ceph.mon.keyring</code>：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@blog cluster]# cat ceph.mon.keyring </span><br><span class="line">[mon.]</span><br><span class="line">key = AQB1yWRYAAAAABAAhMoAcadfCdy9VtAaY79+Sw==</span><br><span class="line">caps mon = allow *</span><br></pre></td></tr></table></figure><p>​        除了<code>key</code>的内容不一样，剩下的都会是一样的。因为是开启了CephX认证了，所以MON直接的通讯是需要一个秘钥的，<code>key</code>的内容就是秘钥。是不是对Ceph里面的明文认证感到吃惊，有总比没有强。如果，你再次执行<code>new</code>，会生成新的<code>ceph.conf</code>和新的<code>ceph.mon.keyring</code>，并将之前的这两个文件给覆盖掉，新旧文件唯一不同的就是<code>fsid</code>和<code>key</code>的内容，但是对Ceph来说，这就是两个集群了。这里说一下我个人非常非常非常反感的一个问题，有的朋友喜欢在<code>/etc/ceph/</code>目录下面执行ceph-deploy的命令，这么做和在<strong>部署目录</strong>下面做一般是没有差别的，因为这两个目录下面都有<code>ceph.conf</code>和<code>ceph.client.admin.keyring</code>，但是我还是强烈推荐创建独立的<strong>部署目录</strong>，因为<code>/etc/ceph</code>目录是Ceph节点的运行目录，为了体现各自的功能性，也为了安全性，<strong>请不要在</strong><code>**/etc/ceph**</code><strong>目录下部署集群！！！</strong></p><h3 id="Q7-为ceph-deploy添加参数"><a href="#Q7-为ceph-deploy添加参数" class="headerlink" title="Q7. 为ceph-deploy添加参数"></a><strong>Q7. 为ceph-deploy添加参数</strong></h3><p>​        Ceph-deploy的log还是很有看头的，查看<code>ceph-deploy new blog</code>(blog是我的一台主机)的log：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@blog cluster]# ceph-deploy new blog</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: <span class="regexp">/root/</span>.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (<span class="number">1.5</span><span class="number">.36</span>): <span class="regexp">/usr/</span>bin/ceph-deploy <span class="keyword">new</span> blog</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : <span class="xml"><span class="tag">&lt;<span class="name">function</span> <span class="attr">new</span> <span class="attr">at</span> <span class="attr">0x288e2a8</span>&gt;</span></span></span><br><span class="line"><span class="xml">[ceph_deploy.cli][INFO  ]  verbose                       : False</span></span><br><span class="line"><span class="xml">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span></span><br><span class="line"><span class="xml">[ceph_deploy.cli][INFO  ]  quiet                         : False</span></span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x28eccf8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ssh_copykey                   : True</span><br><span class="line">[ceph_deploy.cli][INFO  ]  mon                           : ['blog']</span><br><span class="line">[ceph_deploy.cli][INFO  ]  public_network                : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster_network               : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]   fsid                          : None</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating new cluster named ceph</span><br></pre></td></tr></table></figure><p>​        可以看到有很多的参数被列出来了，比如：<code>mon : [&#39;blog&#39;]</code>，也有很多参数是False或者None， 这些参数能否被设置呢? 因为这里我们可以看到有<code>fsid : None</code> 这个参数，难道集群的<code>fsid</code>可以被指定吗？抱着这些疑惑，我就去看完了ceph-deploy的所有代码，答案是：可以设置。所有上面的参数都可以使用参数的形式进行设置，只需要在前面加上两个<code>--</code>，比如对于<code>fsid</code>可以执行：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy  <span class="keyword">new</span> blog --fsid xx-xx-xx-xxxx</span><br></pre></td></tr></table></figure><p>​        如果想要查看每个执行可指定的参数，可以<code>-h</code>：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@blog cluster]# ceph-deploy new -h</span><br><span class="line">usage: ceph-deploy <span class="keyword">new</span> [-h] [--no-ssh-copykey] [--fsid FSID]</span><br><span class="line">                      [--cluster-network CLUSTER_NETWORK]</span><br><span class="line">                      [--public-network PUBLIC_NETWORK]</span><br><span class="line">                      MON [MON ...]</span><br><span class="line">...</span><br><span class="line">optional <span class="built_in">arguments</span>:</span><br><span class="line">  -h, --help            show <span class="keyword">this</span> help message and exit</span><br><span class="line">  --no-ssh-copykey      <span class="keyword">do</span> not attempt to copy SSH keys</span><br><span class="line">  --fsid FSID           provide an alternate FSID <span class="keyword">for</span> ceph.conf generation</span><br><span class="line">  --cluster-network CLUSTER_NETWORK</span><br><span class="line">                        specify the (internal) cluster network</span><br><span class="line">  --public-network PUBLIC_NETWORK</span><br><span class="line">                        specify the public network <span class="keyword">for</span> a cluster</span><br></pre></td></tr></table></figure><p>​        这里就可以看到可以指定<code>--cluster-network</code>，<code>--public-network</code>，等等，如果<code>optional arguments</code>里面没有介绍这个参数，可以直接使用<code>--xxarg</code>的方式指定，比如<code>--overwrite-conf</code>，<code>--verbose</code>等等，能不能设置这些参数，自己动手试一下就知道了。需要注意的是，参数的位置根据指令而异，比如<code>--overwrite-conf</code>参数是跟在<code>ceph-deploy</code>后面的，而<code>--public-network</code>是跟在<code>new</code>后面的：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy --overwrite-conf --verbose <span class="keyword">new</span> blog --fsid a-a-a-a</span><br><span class="line">[root@blog cluster]# cat ceph.conf |grep fsid</span><br><span class="line">fsid = a-a-a-a</span><br></pre></td></tr></table></figure><h3 id="Q8-Public-VS-Cluster"><a href="#Q8-Public-VS-Cluster" class="headerlink" title="Q8. Public VS Cluster"></a><strong>Q8. Public VS Cluster</strong></h3><p>​        如果非要在刚刚生成的ceph.conf里面添加什么的话，那么可能就要加public_network或者cluster_network了。那么这两个配置项有什么用呢？这里简单得介绍下Ceph的Public(外网或者叫公网或者前端网)和Cluster(内网或者叫集群网或者叫后端网)这两个网络，在Ceph中，存在以下三种主要的网络通讯关系：</p><ul><li>client-&gt; mon =&gt;public : 也就是客户端获取集群状态，或者叫客户端与MON通讯走的网络，是走的外网。</li><li>client-&gt; osd =&gt; public : 也就是客户端向OSD直接写入数据走的也是外网。</li><li>osd<-> osd =&gt; cluster ：也就是OSD之间的数据克隆，恢复走的是内网，客户端写第一份数据时通过外网写，对于三副本剩下的两个副本OSD之间通过内网完成数据复制。当OSD挂掉之后产生的recover,走的也是内网。</-></li></ul><p>通常，我们会将外网配置为千兆网，而内网配置成万兆网，这是有一定原因的：</p><ul><li>客户端可能由成百上千的计算节点组成，外网配成万兆成本太高。</li><li>存储节点一般只有几个到几十个节点，配置了万兆内网可以大大加快故障恢复速度，而且剩余的两副本写速度会大大加快，万兆网的性价比极高。举个例子，集群坏掉一个OSD千兆需要一小时，那么万兆网只需要五六分钟，一定程度上增加了集群的安全性。</li></ul><p>借用官网的这张图来说明集群的网络走势：再假设你的节点有两个网段172.23.0.1和3.3.4.1，还记得我们上一节<code>ceph-deploy new</code>的时候是可以指定<code>public_network</code>和<code>cluster_network</code>的吗！如果不指定这两个参数，那么ceph-deploy怎么知道用哪个IP作为这个节点的<code>mon_host</code>的IP呢，其实他是随便选的，如果它选了172网段但是你想使用3.3网段作为这个节点的<code>mon_host</code>的IP，那么只需要指定<code>--public-network 172.23.0.0/24</code> 就可以了，其中的<code>/24</code>就相当于一个掩码，表示前面的IP的前24位，也就是<code>172.23.0.XXX</code>，只要你的主机上有一个处于这个范围内的IP，那么就会选择这个IP作为公网IP。类似的，<code>/16</code>表示范围：<code>172.23.XXX.XXX</code>。 如果想指定内网IP，那么只要指定<code>--cluster-network 3.3.4.1/24</code>就可以了。</p><p>​        <strong>一般情况下，会在new生成的ceph.conf文件里加入public_network配置项以指定公网IP。当然你的MON主机上需要有至少一个IP在公网范围内。</strong>除了在生成的<code>ceph.conf</code>文件中加入公网IP的方式，我们还可以使用参数的方式来指定公网IP：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 cluster]# ceph-deploy new ceph-1 --public-network 172.23.0.0/24</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (<span class="number">1.5</span><span class="number">.36</span>): <span class="regexp">/usr/</span>bin/ceph-deploy <span class="keyword">new</span> ceph<span class="number">-1</span> --public-network <span class="number">172.23</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span></span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">...</span><br><span class="line">[ceph_deploy.cli][INFO  ]  public_network                : <span class="number">172.23</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span></span><br><span class="line">...</span><br><span class="line">[ceph<span class="number">-1</span>][DEBUG ] IP addresses found: [u<span class="string">'172.23.0.101'</span>, u<span class="string">'10.0.2.15'</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph<span class="number">-1</span></span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph<span class="number">-1</span> at <span class="number">172.23</span><span class="number">.0</span><span class="number">.101</span></span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor initial members are [<span class="string">'ceph-1'</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor addrs are [u<span class="string">'172.23.0.101'</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...</span><br><span class="line">[root@ceph-1 cluster]# cat ceph.conf </span><br><span class="line">[global]</span><br><span class="line">fsid = d2a2bccc-b215<span class="number">-4</span>f3e<span class="number">-922</span>b-cf6019068e76</span><br><span class="line">public_network = <span class="number">172.23</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span></span><br><span class="line">mon_initial_members = ceph<span class="number">-1</span></span><br><span class="line">mon_host = <span class="number">172.23</span><span class="number">.0</span><span class="number">.101</span></span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br></pre></td></tr></table></figure><p>​        查看部署log可以发现参数配置已经生效，而这个节点有两个IP，<code>public_nwtwork</code>这个参数限定了公网IP的搜索范围，生成的ceph.conf文件内也包含了<code>public_network</code>这个参数。</p><h3 id="Q9-参数是下划线还是空格分隔"><a href="#Q9-参数是下划线还是空格分隔" class="headerlink" title="Q9. 参数是下划线还是空格分隔"></a><strong>Q9. 参数是下划线还是空格分隔</strong></h3><p>​        这里只是简单的提一下这个小困惑，对于以下的两个参数书写方式，哪种会有问题呢：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public_network = <span class="number">172.23</span><span class="number">.0</span><span class="number">.1</span>/<span class="number">24</span></span><br><span class="line">public network = <span class="number">172.23</span><span class="number">.0</span><span class="number">.1</span>/<span class="number">24</span></span><br><span class="line">osd_journal_size = <span class="number">128</span></span><br><span class="line">osd journal size = <span class="number">128</span></span><br></pre></td></tr></table></figure><p>​        这两种参数的书写方式其实都是正确的，说到底是因为底层调用的是Python的<code>argparse</code>模块。这两种方式都是等效的，所以不需要担心。</p><h3 id="Q10-ceph-deploy-mon-create-initial如何一次性通过"><a href="#Q10-ceph-deploy-mon-create-initial如何一次性通过" class="headerlink" title="Q10. ceph-deploy mon create-initial如何一次性通过"></a><strong>Q10. ceph-deploy mon create-initial如何一次性通过</strong></h3><p>​        这一步坑哭了多少迫切加入Ceph世界的新人，看到的最多的就是5s，10s，10s, 15s，20s。。。然后报了错。再执行，再报错。所以这里给出以下的预检清单，如果被报错失败所烦恼，请认真执行各个子项，尤其是失败后要执行清理环境：</p><ol><li>请确保所有节点都安装了Ceph。</li><li>请确保所有节点的防火墙等都关闭了。参考<strong>环境预准备</strong>一节</li><li>请前往各个MON节点清理干净，不论你是否相信这个节点是干净的。参考<strong>清理环境</strong>一节。</li><li>请确保各个MON节点下存在以下目录，并且对于Jewel版本及之后的请确保目录权限为<code>ceph:ceph</code>。参考<strong>部署前最后的确认</strong>一节。</li><li>请在<code>ceph-deploy new</code>生成的<code>ceph.conf</code>内添加<code>public_network</code>配置项，参考<strong>Public VS Cluster</strong>一节。</li></ol><p>这些总结来之不易，我帮过上百个人解决过部署问题和集群故障。我相信在<strong>认真确认</strong>过之后是肯定可以通过的(反正前三点如果有问题一般是不会建好MON的，为什么不认真确认下呢)，我遇到过绝大多数都是因为防火墙没关，或者手动删除了一些目录，或者没有修改权限导致的问题。</p><p>​        相对来说，新环境只要关了防火墙就可以一次性通过，旧环境或者失败的环境只要清理环境就可以通过了。</p><p><strong>Q11. mon create-initial 做了什么</strong></p><p>简单介绍下流程：</p><ul><li><p>ceph-deploy读取配置文件中的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mon_initial_members</span><br></pre></td></tr></table></figure><p>的各个主机，然后依次SSH前往各个主机：</p><ol><li>将<strong>部署目录</strong>下的ceph.conf推送到新节点的<code>/etc/ceph/</code>目录下。</li><li>创建<code>/var/lib/ceph/mon/$cluster-$hostname/</code>目录。</li><li>检查MON目录下是否有<code>done</code>文件，如果有则直接跳到第6步。</li><li>将<code>ceph.mon.keyring</code>拷贝到新节点，并利用该秘钥在MON目录下建立MON数据库。</li><li>在MON目录下建立done文件，防止重新建立MON。</li><li>启动MON进程。</li><li>查看<code>/var/run/ceph/$cluster-mon.$hostname.asok</code>SOCKET文件，这个是由MON进程启动后生成的，输出MON状态。</li></ol></li><li><p>在所有的MON都建立好后，再次前往各个主机，查看所有主机是否运行并且到达法定人群(quorum)。如果有没到到的，直接结束报错。如果都到达了，执行下一步。</p></li><li><p>调用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">auth get-or-create</span><br></pre></td></tr></table></figure><p>方法创建(如果不存在)或者拉取(已经存在)MON节点上的以下几个keyring到</p><p>部署目录</p><p>中：</p><ul><li><code>ceph.bootstrap-mds.keyring</code></li><li><code>ceph.bootstrap-osd.keyring</code></li><li><code>ceph.bootstrap-rgw.keyring</code></li><li><code>ceph.client.admin.keyring</code></li></ul></li><li><p>指令结束。</p></li></ul><h3 id="Q12-mon-create-initial-为什么会失败"><a href="#Q12-mon-create-initial-为什么会失败" class="headerlink" title="Q12. mon create-initial 为什么会失败"></a><strong>Q12. mon create-initial 为什么会失败</strong></h3><p>​        我不喜欢讲怎么做，我愿意花很大的篇幅介绍为什么会造成各种各样的问题，如果知道了原因，你自然知道该怎么做，所以才会理解Ceph，而不是机械的去敲指令。</p><p>综合上面的所有小节，我来总结下这一步失败的基本上所有可能的原因：</p><ul><li>所谓MON的quorum，相当于多个MON形成的一个群体，它们之间需要通过网络发送数据包来通讯达成某种协议，如果打开了防火墙，会阻断数据交流。所以不能构成群体，一直等待(5s-&gt;10s-&gt;10s-&gt;15s-&gt;20s)其他MON的数据包，既然被阻断了这样的等待是没有意义的，等了30s还没有正常，就可以直接<code>ctrl+z</code>去检查了。</li><li>我在配置文件里面添加了<code>pubilc_network</code>，但是有个主机的所有IP都不在公网IP段内，那么这个MON是建不好的，因为没有IP用作MON使用，<code>public_network</code>相当于一个<strong>过滤器</strong>。</li><li>搭好了一台虚拟机后，直接克隆了两台，没有修改主机名，导致socket文件路径名识别错误，报了异常，不过这很少发生。</li><li>如果在旧的MON节点上再次部署新的MON，再又没有清理环境，之前的MON数据库会保留着<code>done</code>文件，MON数据库里面还是记录着之前fsid，keyring等等，和新集群是两套完全不同的，所以这个节点的MON自然到达不了MON群体。</li><li>即使你单单删除了<code>/var/lib/ceph/mon</code>下的东西，而没有清理那些keyring，也有可能会因为收集了旧集群的秘钥而发生稀奇古怪的问题。</li><li>对于Jewel，你一不小心删除了<code>/var/lib/ceph/mon</code>目录，或者其他的OSD目录或者<code>/var/run/ceph</code>目录，然后又重建了目录，依然部署不上，是因为Jewel的所有Ceph指定都是运行在<code>ceph:ceph</code>用户下的，自然不能在root权限目录下建立任何文件，修改权限即可。</li><li>Ceph生成MON数据库是依照主机的<code>hostname</code>来命名至目录<code>/var/lib/ceph/mon/${cluster}-${hostname}</code>的，而检测SOCKET文件则是用<code>ceph.conf</code>里面的<code>mon_initial_members</code>里面的名字来检测的 ，如果<code>mon_initial_members</code>里面的名字和真是的主机名不一致，就会报错。</li></ul><p>​      一旦你运行了<code>ceph-deploy mon create-initial</code>指令，并且失败了，有极大的可能性已经在某些节点建立好了MON的数据库，再次执行可能会因为旧的环境导致再次失败，所以如果失败了，执行一下第二节中的<code>清理环境</code>即可。清理完毕后，再执行<code>ceph-deploy mon create-initial</code>。</p><h3 id="Q13-ceph-s-的全称以及报错原因"><a href="#Q13-ceph-s-的全称以及报错原因" class="headerlink" title="Q13. ceph -s 的全称以及报错原因**"></a>Q13. ceph -s 的全称以及报错原因**</h3><p>​        开开心心过了<code>mon create-initial</code>，这个时候执行<code>ceph -s</code>，如果你恰好在monitor节点执行，那就会显示正常的信息，但是如果你在别的节点执行<code>ceph -s</code>，很有可能会报下面的错，但是有的节点又不会，所以这里花一点篇幅介绍<code>ceph -s</code>到底是怎么工作的。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@root cluster]# ceph -s</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-17</span> <span class="number">13</span>:<span class="number">47</span>:<span class="number">34.190226</span> <span class="number">7</span>f446ccde700 <span class="number">-1</span> auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin: (<span class="number">2</span>) No such file or directory</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-17</span> <span class="number">13</span>:<span class="number">47</span>:<span class="number">34.190393</span> <span class="number">7</span>f446ccde700 <span class="number">-1</span> monclient(hunting): ERROR: missing keyring, cannot use cephx <span class="keyword">for</span> authentication</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-17</span> <span class="number">13</span>:<span class="number">47</span>:<span class="number">34.190443</span> <span class="number">7</span>f446ccde700  <span class="number">0</span> librados: client.admin initialization error (<span class="number">2</span>) No such file or directory</span><br></pre></td></tr></table></figure><p>​        首先，如果你要执行<code>ceph</code>开头的任何指令，你当然要安装好Ceph客户端！（<code>yum install ceph</code>）而<code>ceph -s</code>的全称是：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ceph \</span><br><span class="line">--name client.admin \</span><br><span class="line">--keyring /etc/ceph/ceph.client.admin.keyring \</span><br><span class="line">--conf /etc/ceph/ceph.conf</span><br><span class="line">--cluster ceph \</span><br><span class="line">-s</span><br></pre></td></tr></table></figure><p>​        上面两个参数很好理解，Ceph内部自身使用<code>CephX</code>进行认证，和普通的认证没什么区别，同样需要用户名和密码进行认证，那么这里默认的用户名就叫做<code>client.admin</code>，而默认的秘钥保存位置就位于以下几个位置任一：</p><ul><li>/etc/ceph/ceph.client.admin.keyring</li><li>/etc/ceph/ceph.keyring</li><li>/etc/ceph/keyring</li><li>/etc/ceph/keyring.bin</li></ul><p>一般我们选择第一个，因为秘钥的命名规则采用<code>/etc/ceph/$cluster.$name.keyring</code>也就是集群名加上用户名再加上keyring的后缀组成。所以在我们执行<code>ceph -s</code>的时候，默认使用的是<code>client.admin</code>用户，同时会去那四个默认位置搜索该用户的秘钥，如果和集群保存的认证信息一致，那么就会显示出集群的状态。如果在那四个位置下面无法找到秘钥文件，就会报上面的<code>unable to find a keyring</code>这样的错误，解决方法后面再说。如果这个位置下面的秘钥文件保存了错误的秘钥值，就会报下面的错误：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-17</span> <span class="number">15</span>:<span class="number">59</span>:<span class="number">07.625018</span> <span class="number">7</span>f8757577700  <span class="number">0</span> librados: client.admin authentication error (<span class="number">1</span>) Operation not permitted</span><br><span class="line"><span class="built_in">Error</span> connecting to cluster: PermissionError</span><br></pre></td></tr></table></figure><p>​        翻译过来很简单，就是认证不通过，就好比你使用了错误的密码，去登陆系统不通过一样。这可能是由于这个节点保存了旧的集群的秘钥信息导致的。</p><p>​        那么正确的秘钥信息保存在哪里呢？还记得部署目录吗，在<code>mon create-initial</code>正确通过后，就会自动收集所有的秘钥，并保存在部署目录下面，眼疾手快的把部署目录的<code>ceph.client.admin.keyring</code>拷贝到<code>/etc/ceph</code>下面就会发现<code>ceph -s</code>正常显示了，不过，这不是<strong>授权</strong>的正确的姿势。</p><p>如果我们想要给一个节点admin权限，也就是执行所有Ceph指令的权限，我们可以前往部署目录，然后调用下面的指令：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy admin xxNode</span><br></pre></td></tr></table></figure><p>​        这样就会把部署目录下的<code>ceph.client.admin.keyring</code>和<code>ceph.conf</code>拷贝到xxNode的<code>/etc/ceph</code>目录下，并覆盖掉原先的秘钥文件，虽然实际上也就是scp了这两个文件，但是管理Ceph遵循一定的规则是一个很好的习惯。所以，想要得到<code>ceph -s</code>的正确输出，你需要确认在<code>/etc/ceph</code>目录下有<code>ceph.conf</code>和<code>ceph.client.admin.keyring</code>这两个文件，并且和集群认证信息相同即可。如果认证失败，可以前往部署目录<strong>授权</strong>该节点。</p><h3 id="Q14-ceph-s-卡住了"><a href="#Q14-ceph-s-卡住了" class="headerlink" title="Q14. ceph -s 卡住了"></a><strong>Q14. ceph -s 卡住了</strong></h3><p>简单介绍下<code>ceph -s</code>的流程:</p><ul><li>每当你敲下一个Ceph指令时，相当于建立了一个Ceph的客户端进程去连接集群。</li><li>连接集群需要知道MON的IP地址，这个地址从<code>/etc/ceph/ceph.conf</code>里面的<code>mon_host</code>读取。</li><li>有了IP客户端就拿着自己用户名和秘钥向MON进行认证，认证通过执行指令返回输出。</li></ul><p>如果你只有一个MON，然后这个MON挂掉了，那么执行指令会返回：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@blog ceph]# ceph -s</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-19</span> <span class="number">17</span>:<span class="number">49</span>:<span class="number">45.437748</span> <span class="number">7</span>f02f44e1700  <span class="number">0</span> -- :<span class="regexp">/1314350745 &gt;&gt; 139.224.0.251:6789/</span><span class="number">0</span> pipe(<span class="number">0x7f02f0063e80</span> sd=<span class="number">3</span> :<span class="number">0</span> s=<span class="number">1</span> pgs=<span class="number">0</span> cs=<span class="number">0</span> l=<span class="number">1</span> c=<span class="number">0x7f02f005c4f0</span>).fault</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-19</span> <span class="number">17</span>:<span class="number">49</span>:<span class="number">48.442946</span> <span class="number">7</span>f02f43e0700  <span class="number">0</span> -- :<span class="regexp">/1314350745 &gt;&gt; 139.224.0.251:6789/</span><span class="number">0</span> pipe(<span class="number">0x7f02e4000c80</span> sd=<span class="number">3</span> :<span class="number">0</span> s=<span class="number">1</span> pgs=<span class="number">0</span> cs=<span class="number">0</span> l=<span class="number">1</span> c=<span class="number">0x7f02e4001f90</span>).fault</span><br></pre></td></tr></table></figure><p><strong>Tips: MON的端口号为6789，所以一般看到IP:6789时，就可以判断这个IP的MON可能挂了，或者MON的防火墙开开了。</strong>上面的报错还好处理， 前往MON节点，检查<code>ceph-mon</code>进程是否正常运行，正确启动MON进程就可以了。</p><p>​        如果你有两个MON，挂了一个，指令会返回和上面一样的信息，所以，两个MON只能坏一个，一般MON个数都是<strong>奇数个</strong>。如果你有三个MON，挂了一个，那么会返回下面信息，集群还是会有输出的：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@st001 ~]# ceph -s</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-19</span> <span class="number">17</span>:<span class="number">59</span>:<span class="number">40.753370</span> <span class="number">7</span>f72ac31c700  <span class="number">0</span> -- :<span class="regexp">/4173548806 &gt;&gt; 10.8.0.101:6789/</span><span class="number">0</span> pipe(<span class="number">0x7f72a805e9d0</span> sd=<span class="number">3</span> :<span class="number">0</span> s=<span class="number">1</span> pgs=<span class="number">0</span> cs=<span class="number">0</span> l=<span class="number">1</span> c=<span class="number">0x7f72a805fce0</span>).fault</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-19</span> <span class="number">17</span>:<span class="number">59</span>:<span class="number">49.754198</span> <span class="number">7</span>f72ac21b700  <span class="number">0</span> -- <span class="number">10.8</span><span class="number">.0</span><span class="number">.101</span>:<span class="number">0</span>/<span class="number">4173548806</span> &gt;&gt; <span class="number">10.8</span><span class="number">.0</span><span class="number">.101</span>:<span class="number">6789</span>/<span class="number">0</span> pipe(<span class="number">0x7f729c000b90</span> sd=<span class="number">4</span> :<span class="number">0</span> s=<span class="number">1</span> pgs=<span class="number">0</span> cs=<span class="number">0</span> l=<span class="number">1</span> c=<span class="number">0x7f729c0041e0</span>).fault</span><br><span class="line">    cluster <span class="number">810</span>eaecb<span class="number">-2</span>b15<span class="number">-4</span>a97<span class="number">-84</span>ad<span class="number">-7340e6</span>cbe969</span><br><span class="line">    health HEALTH_WARN</span><br><span class="line">            <span class="number">1</span> mons down, quorum <span class="number">1</span>,<span class="number">2</span> st002,st003</span><br><span class="line">    monmap e1: <span class="number">3</span> mons at &#123;st001=<span class="number">10.8</span><span class="number">.0</span><span class="number">.101</span>:<span class="number">6789</span>/<span class="number">0</span>,st002=<span class="number">10.8</span><span class="number">.0</span><span class="number">.102</span>:<span class="number">6789</span>/<span class="number">0</span>,st003=<span class="number">10.8</span><span class="number">.0</span><span class="number">.103</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">18</span>, quorum <span class="number">1</span>,<span class="number">2</span> st002,st003</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>​        客户端会去连挂掉的MON，如果过一秒钟左右连不上，就会连接剩下的MON，剩下的还有两个在运行，就连到了运行中的MON，一切输出照旧，就是多了那个连不上的MON报错输出。</p><p>​        而<code>ceph -s</code>卡住有一种可能是：对于有三个MON的集群，挂掉了两个MON之后 ，手动去<code>/etc/ceph/ceph.conf</code>里面把挂掉的MON的IP给删除了， 只留下一个，这时候<code>ceph -s</code>的指令就会一直卡在那里，查看MON的log可以发现，那个活着的MON一直处于<code>probing</code>状态，这样的MON是不会给客户端返回信息的，所以会卡在那里。有一点需要知道的是，MON的删除比较复杂，不能仅仅通过修改配置文件里面的IP值修改MON，所以，这里正确的做法就是，将删除的IP加回去，然后<code>ceph -s</code>就会报出6789之类的错误，然后再去对应的IP的MON去启动MON服务。</p><p>​        那么一个集群能坏多少MON呢 ，简单的计算法法就是：</p><p>​        <strong>(mon个数 -1 )/ 2 取整数位</strong></p><p>​        也就是说三个能坏一个，两个和一个不能坏，四个坏一个，五个坏两个等等等。当你坏的MON个数大于可以坏的个数，那么所有的指令是不能返回的。</p><h3 id="Q15-Monitor-clock-skew-detected"><a href="#Q15-Monitor-clock-skew-detected" class="headerlink" title="Q15. Monitor clock skew detected"></a><strong>Q15. Monitor clock skew detected</strong></h3><p>​        如果你部署了多个monitor，比如三个MON，而这三个MON的时间不是严格相同的，那么就会报这个错，而Ceph需要MON节点之间的时间差在0.05秒之内，所以一般会选择配置一个内部的NTP server。剩余节点指向该Server节点。</p><p>​        千万一定不要小看了时间对其这个问题，如果各个节点时间不对其的话，有可能会导致某些OSD无法启动，而校准后，OSD立马就启动成功了，亦或导致OSD异常挂掉等等一系列的奇怪现象，十分不利于故障排查。</p><p>​        然而，简单的增加<code>mon_clock_drift_allowed</code>的时间偏移大小，是治标不治本的方法，并且OSD节点的时间偏移并不会报告在<code>ceph -s</code>里面，所以根本的节点方法还是配置NTP，具体方法请参考我之前写的配置NTP一段，这里就不重复了。</p><h3 id="Q16-CephX是什么，以及CephX的开启与关闭"><a href="#Q16-CephX是什么，以及CephX的开启与关闭" class="headerlink" title="Q16. CephX是什么，以及CephX的开启与关闭"></a><strong>Q16. CephX是什么，以及CephX的开启与关闭</strong></h3><p>在默认生成的<code>ceph.conf</code>里面有三行包含CephX的配置：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br></pre></td></tr></table></figure><p>Ceph提供认证功能，想要连接集群，是需要提供用户名和密码的，这三个配置的值只有两种：</p><ul><li><code>cephx</code>: 开启CephX，即需要提供用户名和密码才能连接集群。</li><li><code>none</code>: 关闭CephX，即不需要提供，任何人都可以连接集群。</li></ul><p><strong>注意</strong>：如果关闭了CephX，那么任何一个客户端只要拥有了MON的IP和集群的fsid，就可以连接到集群中，然后执行所有的Ceph的指令，这是相当危险的，所以对于一个非局域网的集群，是需要开启的。</p><p>之所以写这一节，是因为见过好几个在部署集群时就关闭了CephX而遇到了奇怪的现象的情形，他们一般的操作步骤是：</p><ol><li><code>ceph-deploy new node1 node2 node3</code></li><li>将生成的<code>ceph.conf</code>中的三个<code>cephx</code>改成了<code>none</code></li><li><code>ceph-deploy mon create-initial</code> 这一步报错如下:</li></ol><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[ceph_deploy.mon][INFO  ] mon.blog monitor has reached quorum!</span><br><span class="line">[ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum</span><br><span class="line">[ceph_deploy.mon][INFO  ] Running gatherkeys...</span><br><span class="line">.......</span><br><span class="line">[blog][DEBUG ] fetch remote file</span><br><span class="line">[ceph_deploy.gatherkeys][WARNIN] No mon key found <span class="keyword">in</span> host: blog</span><br><span class="line">[ceph_deploy.gatherkeys][ERROR ] Failed to connect to host:blog</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpyGDe4r</span><br><span class="line">[ceph_deploy][ERROR ] RuntimeError: Failed to connect any mon</span><br></pre></td></tr></table></figure><p>​        先介绍下这里报错的原因，在Ceph中，除了需要使用Ceph的普通用户之外，Ceph的基本组件：MON，OSD，MDS再到RGW等都可以看做一个用户，而在使用<code>ceph-deploy</code>部署的时候，会默认为这些用户生成秘钥文件，在<code>ceph-deploy new</code>的时候，除了生成了<code>ceph.conf</code>，还生成了<code>ceph.mon.keyring</code>，顾名思义这个就是为MON用户生成的秘钥文件。查看该文件的内容可以看到如下内容：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[mon.]</span><br><span class="line">key = AQCUXIRYAAAAABAAOi6Cxnvm+zFzd5gi+hrt+A==</span><br><span class="line">caps mon = allow *</span><br></pre></td></tr></table></figure><p>一个秘钥文件一般由三部分组成：</p><ul><li><code>[mon.]</code> ： 也就是用户名，在方括号里面的就是用户名，这里为<code>mon.</code>，注意是有个<strong>点号</strong>的。</li><li><code>key = AQCUXIRYAAAAABAAOi6Cxnvm+zFzd5gi+hrt+A==</code> : 顾名思义，这就是<code>mon.</code>用户的密码。</li><li><code>caps</code> ： 后面的就是权限，这里可以简单理解成，该用户可以对所有的MON进行所有操作。</li></ul><p>也就是说，Ceph中的Monitor也会像一个用户一样，拥有自己的用户名和秘钥以及操作MON的权限。简单理解了CephX之后，我们再来看上面修改了<code>none</code>之后报的错。</p><p>​        在<code>ceph-deploy mon create-initial</code>执行的时候，它会去读取<code>ceph.conf</code>里面的<code>auth_cluster_required</code>配置，当被修改为<code>none</code>之后， 就不会在创建MON的时候，为其生成对应的<code>keyring</code>，但是有一点要注意的是，尽管没有为MON生成秘钥文件，但是，MON是正确生成的，这时候执行<code>ceph -s</code>是可以得到集群状态的，说明MON已经正确建立。但是在所有的MON建立成功之后，<code>mon create-initial</code>指令内部会执行<code>gatherkeys</code>指令，这个指令会首先去MON的目录下面查找<code>/var/lib/ceph/mon/ceph-$HOSTNAME/keyring</code>文件，由于关闭了CephX，在创建MON的时候不会为其生成该文件，所以<code>gatherkeys</code>指令报错：<code>No mon key found in host: blog</code>。这里只要清理下MON环境然后开启CephX重新部署MON就可以通过了。所以在我们<strong>部署集群</strong>的时候，<strong>强烈建议开启CephX</strong>，这样除了可以正确通过<code>mon create-initial</code>，还可以在后续的添加OSD时，为每个OSD生成对应的秘钥。在<strong>集群部署完毕后</strong>，可以关闭CephX认证，具体方法如下：</p><ul><li>修改部署目录内<code>ceph.conf</code>的<code>cephx-&gt;none</code>,将配置推送到所有节点。</li><li>重启所有的MON和OSD。如果只重启MON，过一段时间(几个小时)，所有的OSD就会挂掉。。。</li></ul><p>在<code>ceph-deploy mon create-initial</code>正确通过之后，我们可以在部署目录下面看到多出了几个文件，都是以<code>keyring</code>结尾：</p><ul><li><code>ceph.client.admin.keyring</code>： 这个是超级用户<code>client.admin</code>的秘钥文件，查看其对应的权限，可以发现全部都是<code>allow *</code>，所以有了这个秘钥之后，相当于有了Linux系统的<code>root</code>用户，可以为所欲为了。</li><li><code>ceph.bootstrap-osd.keyring</code>: 类似的还有两个<code>mds</code>和<code>rgw</code>，<code>bootstrap</code>的意思是引导，查看其权限<code>mon = &quot;allow profile bootstrap-osd&quot;</code>，简单解释就是，这个用户可以用于创建OSD(or MDS or RGW)用户。也就是说，后续的OSD的用户的生成是由该用户引导生成的。</li></ul><p>最后再说一点，对于秘钥文件，其实我们只需要提供<code>key= xxxxxxxx</code>和用户名<code>[xxx]</code>就好了，不需要提供权限部分，因为权限已经在Ceph集群中保存了，秘钥文件说了不算的。具体权限可以通过<code>ceph auth list</code>来查看。</p><h3 id="Q17-–overwrite-conf参数"><a href="#Q17-–overwrite-conf参数" class="headerlink" title="Q17. –overwrite-conf参数"></a><strong>Q17. –overwrite-conf参数</strong></h3><p>这是个经常会遇到的问题，修改配置文件内的某些参数后，再执行<code>ceph-deploy</code>指令，会报如下的错误：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[blog][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists <span class="keyword">with</span> different content; use --overwrite-conf to overwrite</span><br><span class="line">[ceph_deploy][ERROR ] GenericError: Failed to create <span class="number">1</span> monitors</span><br></pre></td></tr></table></figure><p>​        报错信息提示得很明确，部署目录内的<code>ceph.conf</code>和集群的配置文件<code>/etc/ceph/ceph.conf</code>内容不一致，使用<code>--overwrite-conf</code>参数来覆盖集群的配置文件，也就是用部署目录的<code>ceph.conf</code>覆盖之。使用<code>ceph-deploy --overwrite-conf xxxCMD</code>来达到这一效果，当然，你也可以直接<code>cp</code>覆盖之。但是这不是一个好习惯。</p><p>正确的修改集群配置文件的姿势应该是：</p><ul><li>修改<strong>部署目录下的<code>ceph.conf</code></strong>。</li><li><code>ceph-deploy --overwrite-conf config push NodeA NodeB ... NodeZ</code>将部署目录下的配置文件推送到各个节点。</li><li><strong>强烈建议使用上面的方法</strong></li></ul><p>有的朋友可能喜欢直接去某个节点下的<code>/etc/ceph/ceph.conf</code>去改配置文件，这样有很多坏处：</p><ul><li>过了一周你可能忘了你改过这个节点的配置文件。</li><li>这个节点的配置和集群其他节点的配置不一样，会带来一些安全隐患。</li><li>如果再来一个不知情的同事，他使用了正确的姿势推送配置文件，你改过的参数很容易被覆盖掉。</li></ul><p>所以，从一开始，大家都使用同样的方式去修改集群的配置，是一个很好的习惯，对集群对同事有利无害。</p><p>​        如果你觉得可以接受这种推送配置的方式，但是又不喜欢每次都敲<code>--overwrite-conf</code>这么长的参数，你可以修改<code>~/.cephdeploy.conf</code>这个文件，增加一行<code>overwrite_conf = true</code>：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy configuration file</span><br><span class="line">[ceph-deploy-global]</span><br><span class="line"># Overrides for some of ceph-deploy's global flags, like verbosity or cluster</span><br><span class="line"># name</span><br><span class="line">overwrite_conf = <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>​        打开文件你就会发现，这个是<code>ceph-deploy</code>的配置文件，里面的配置项是对<code>ceph-deploy</code>生效的，在加了那一行之后，我们再去执行<code>ceph-deploy</code>的任何指令，都会默认带上了<code>--overwrite-conf</code>参数，这样就可以不打这个参数还能覆盖节点的配置文件。好处是少打了一些参数，坏处是你可能会不知不觉就覆盖了配置文件，各中利弊自行取舍。</p><p>​        <code>~/.cephdeploy.conf</code>这个文件的用处是很大的，可以为不同的<code>ceph-deploy xxxCMD</code>添加参数，刚刚添加在<code>[ceph-deploy-global]</code>下的参数对全局都会生效，如果你希望只对<code>xxxCMD</code>比如<code>new</code>，<code>osd</code>，<code>mon</code>指定对应的参数，可以添加<code>[ceph-deploy-xxxCMD]</code>域，同时在对应的域下添加对应的参数。</p><p>​        比如给<code>ceph-deploy osd 添加参数--zap-disk</code>，可以在<code>~/.cephdeploy.conf</code>中添加：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ceph-deploy-osd]</span><br><span class="line">zap_disk = <span class="literal">true</span></span><br></pre></td></tr></table></figure><h3 id="Q18-PG卡在creating状态"><a href="#Q18-PG卡在creating状态" class="headerlink" title="Q18. PG卡在creating状态"></a><strong>Q18. PG卡在creating状态</strong></h3><p>​        这时候，Monitor已经建好了，可以执行<code>ceph -s</code>的指令了，然而我们看到集群的健康状态却是：<code>health HEALTH_ERR</code>。之所以是ERROR状态，是因为目前还没有建立OSD，PG处于creating状态，在建好了OSD之后，自然会解决这一问题。然而我要说的重点是<code>creating</code>这个状态的几个产生原因。</p><p>​        <code>creating</code>字面意思很好理解，正在创建，那么怎么理解PG正在创建呢？ 用最简单的方式解释PG就是： <strong>PG等于目录</strong>。如果我们使用磁盘做OSD的话，那么这个OSD上的PG就相当于，在这个磁盘上建立的目录。那么现在的问题就可以简化成，我们尚未添加任何磁盘，那么需要落盘的目录无处可建，所以就会长时间处于<code>creating</code>状态。在添加了一些OSD后，PG就可以建立了。</p><p>​        还有一种可能的原因是，刚入门的同学在配置文件中加了<code>osd_crush_update_on_start = false</code> 参数，这个参数的具体意义会有专门的小节介绍，这个参数的默认值是<code>true</code>，在使用这个参数后不论创建多少OSD，PG都依旧卡在<code>creating</code>状态。原因是所添加的OSD均不在默认的<code>root=default</code>根节点下。CRUSH在<code>default</code>下无法找到OSD，所以效果就和没有创建OSD一样，再解释就过于深入了，这里只简单介绍下解决方法：</p><ul><li>将部署目录里的<code>ceph.conf</code>的<code>osd_crush_update_on_start = false</code>去掉，或者将false改为true。</li><li>将配置文件推送到各个节点。</li><li>重启所有的OSD。</li></ul><p>这样OSD在启动时，就会自动加到对应的主机名下的host下方，并将主机名加到<code>default</code>下方。这样CRUSH就可以找到OSD了。当然，对于新入门的同学，一点建议就是，不知道意义的参数都不用加上，Ceph有自己一套默认参数，而这些参数不用修改就可以正常运行集群。如果添加了某些参数，最好知道其作用再使用。</p><h3 id="Q19-osd-crush-update-on-start-参数的使用和注意点"><a href="#Q19-osd-crush-update-on-start-参数的使用和注意点" class="headerlink" title="Q19. osd_crush_update_on_start 参数的使用和注意点"></a><strong>Q19. osd_crush_update_on_start 参数的使用和注意点</strong></h3><p>​        这是一个很有趣的参数，使用得当会省去很多事情，使用不当可能会造成灾难(亲身体验)。这个参数在<code>ceph --show-config</code>中并不能查询到，所以这并不是Ceph进程的一个配置项。实际上，这个配置相当于一个启动配置项。也就是说在OSD启动的时候会加载这个参数。由于Jewel将OSD的启动方式做了修改，所以针对Hammer及其之前和Jewel两种启动方式，分别在下面的两个文件使用到了这个参数，实际上，加载的方式还是一样的，只是启动文件有所变化：</p><ul><li>Hammer 及其之前 : <code>0.94.9 -&gt; /etc/init.d/ceph -&gt; line 370 -&gt; get_conf update_crush &quot;&quot; &quot;osd crush update on start&quot;</code></li><li>Jewel : <code>10.2.3 -&gt;/usr/lib/ceph/ceph-osd-prestart.sh -&gt; line 23 -&gt; update=&quot;$(ceph-conf --cluster=${cluster:-ceph} --name=osd.$id --lookup osd_crush_update_on_start || :)&quot;</code></li></ul><p>在OSD启动的时候，都会去配置文件中读取<code>osd_crush_update_on_start</code>。然后启动脚本根据是否存在以及配置值来决定是否将该OSD按照一定的方式(CRUSH位置，OSD的ID，OSD的weight)将这个OSD添加到CRUSH中。</p><p>​        简单点说，如果这个值为false，那么OSD在启动的时候不会去修改你的CRUSH树，也就是说OSD不会自动填加到对应的主机名下再自己添加到<code>root=default</code>下。</p><p>​        如果这个值为true，或者不添加该配置项(也就是说，默认为true)，OSD在启动时(任何一次启动)都会将自己添加到CRUSH树下。默认的位置为：<code>/usr/bin/ceph-crush-location -&gt; line 86 -&gt; host=$(hostname -s) root=default</code>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本章总结了搭建ceph集群过程中遇到的各种问题，以及相应的原理过程&lt;/p&gt;
    
    </summary>
    
      <category term="Ceph" scheme="http://yoursite.com/categories/Ceph/"/>
    
    
      <category term="ceph" scheme="http://yoursite.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>ceph-deploy部署ceph12集群</title>
    <link href="http://yoursite.com/2019/05/07/ceph-12%E9%83%A8%E7%BD%B2/"/>
    <id>http://yoursite.com/2019/05/07/ceph-12部署/</id>
    <published>2019-05-07T09:11:02.000Z</published>
    <updated>2019-05-28T08:13:04.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-环境"><a href="#1-环境" class="headerlink" title="1. 环境"></a>1. 环境</h1><h2 id="1-1-硬件"><a href="#1-1-硬件" class="headerlink" title="1.1 硬件"></a>1.1 硬件</h2><p>4台 Linux虚拟机： server0, server1, server2, server3<br> 每台有两块磁盘 ： /dev/vdb, /dev/vdc<br> 每台有两块网卡 ：eth0, ens9</p><h2 id="1-2-软件"><a href="#1-2-软件" class="headerlink" title="1.2 软件"></a>1.2 软件</h2><p>linux版本： CentOS 7.2.1511<br> 内核版本 ：  3.10.0-327.el7.x86_64<br> ceph版本：  12.2.12<br> ceph-deploy版本： 2.0.0</p><a id="more"></a><h1 id="2-准备工作-所有server"><a href="#2-准备工作-所有server" class="headerlink" title="2. 准备工作(所有server)"></a>2. 准备工作(所有server)</h1><h2 id="2-1-配置静态IP"><a href="#2-1-配置静态IP" class="headerlink" title="2.1 配置静态IP"></a>2.1 配置静态IP</h2><p>每台server有两个interface, 分别配置在如下两个网段：</p><ul><li>192.168.122.0/24</li><li>192.168.100.0/24</li></ul><p>具体如下表：</p><table><thead><tr><th>Server</th><th>Interface</th><th>IPADDR</th></tr></thead><tbody><tr><td>server0</td><td>eth0</td><td>192.168.122.160</td></tr><tr><td>server0</td><td>ens9</td><td>192.168.100.160</td></tr><tr><td>server1</td><td>eth0</td><td>192.168.122.161</td></tr><tr><td>server1</td><td>ens9</td><td>192.168.100.161</td></tr><tr><td>server2</td><td>eth0</td><td>192.168.122.162</td></tr><tr><td>server2</td><td>ens9</td><td>192.168.100.162</td></tr><tr><td>server3</td><td>eth0</td><td>192.168.122.163</td></tr><tr><td>server3</td><td>ens9</td><td>192.168.100.163</td></tr></tbody></table><h2 id="2-2-生成ssh-key"><a href="#2-2-生成ssh-key" class="headerlink" title="2.2 生成ssh key"></a>2.2 生成ssh key</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ssh-keygen</span><br></pre></td></tr></table></figure><h2 id="2-3-配置主机名解析"><a href="#2-3-配置主机名解析" class="headerlink" title="2.3 配置主机名解析"></a>2.3 配置主机名解析</h2><p>把如下内容追加到/etc/hosts:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">192.168.100.160 server0</span><br><span class="line">192.168.100.161 server1</span><br><span class="line">192.168.100.162 server2</span><br><span class="line">192.168.100.163 server3</span><br></pre></td></tr></table></figure><h2 id="2-4-配置ntp"><a href="#2-4-配置ntp" class="headerlink" title="2.4 配置ntp"></a>2.4 配置ntp</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># yum install  -y  ntp ntpdate ntp-doc</span><br><span class="line"># vim /etc/ntp.conf  （一般不需要修改）</span><br><span class="line"># systemctl start ntpd.service</span><br><span class="line"># systemctl enable ntpd.service</span><br></pre></td></tr></table></figure><h2 id="2-5-关闭防火墙"><a href="#2-5-关闭防火墙" class="headerlink" title="2.5 关闭防火墙"></a>2.5 关闭防火墙</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># systemctl stop firewalld</span><br><span class="line"># systemctl disable firewalld</span><br></pre></td></tr></table></figure><h2 id="2-6-安装yum源epel"><a href="#2-6-安装yum源epel" class="headerlink" title="2.6 安装yum源epel"></a>2.6 安装yum源epel</h2><p>为了方便yum安装一些常用的软件包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span><br></pre></td></tr></table></figure><h1 id="3-安装ceph软件包"><a href="#3-安装ceph软件包" class="headerlink" title="3. 安装ceph软件包"></a>3. 安装ceph软件包</h1><h2 id="3-1-添加yum源-所有server"><a href="#3-1-添加yum源-所有server" class="headerlink" title="3.1 添加yum源(所有server)"></a>3.1 添加yum源(所有server)</h2><p>在<strong>所有server</strong>上添加ceph.repo，内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># cat /etc/yum.repos.d/ceph.repo </span><br><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">priority=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">priority=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/SRPMS</span><br><span class="line">enabled=0</span><br><span class="line">priority=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br></pre></td></tr></table></figure><p><strong>[Ceph]</strong>： ceph软件包的yum源，所有server都需要添加。<br><strong>[Ceph-noarch]</strong>：ceph-deploy的yum源。admin server (见3.2节)  需要安装ceph-deploy，所以它需要这个yum源。admin  server控制其他server的时候，也需要被控server添加这个yum源。最终，所有server都需要添加。<br><strong>[ceph-source]</strong>： admin server控制其他server的时候，也需要被控server添加这个yum源。所以，所有server都需要添加。</p><h2 id="3-2-选择admin-server"><a href="#3-2-选择admin-server" class="headerlink" title="3.2 选择admin server"></a>3.2 选择admin server</h2><p>选择server0作为admin server。官网上建议admin server使用一个单独的user来进行ceph-deploy操作，这里避免麻烦，还用root账户。<br> admin server需要免密登录所有server（包括自己），所以在admin server上配置免密登录（其他server不必配置）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># ssh-copy-id root@server0</span><br><span class="line"># ssh-copy-id root@server1</span><br><span class="line"># ssh-copy-id root@server2</span><br><span class="line"># ssh-copy-id root@server3</span><br></pre></td></tr></table></figure><p>测试一下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># for i in &#123;0..3&#125; ; do ssh server$i hostname ; done </span><br><span class="line">server0</span><br><span class="line">server1</span><br><span class="line">server2</span><br><span class="line">server3</span><br></pre></td></tr></table></figure><h2 id="3-3-安装ceph-deploy-在admin-server上"><a href="#3-3-安装ceph-deploy-在admin-server上" class="headerlink" title="3.3 安装ceph-deploy(在admin server上)"></a>3.3 安装ceph-deploy(在admin server上)</h2><p>在3.1节已经添加了ceph-deploy的yum源，这里直接通过yum安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># yum -y install ceph-deploy</span><br></pre></td></tr></table></figure><p>然后测试一下，发现报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy --version</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/bin/ceph-deploy&quot;, line 18, in &lt;module&gt;</span><br><span class="line">    from ceph_deploy.cli import main</span><br><span class="line">  File &quot;/usr/lib/python2.7/site-packages/ceph_deploy/cli.py&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    import pkg_resources</span><br><span class="line">ImportError: No module named pkg_resources</span><br></pre></td></tr></table></figure><p>原因是缺python-setuptools，安装它即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># yum install python-setuptools</span><br><span class="line"></span><br><span class="line"># ceph-deploy --version</span><br><span class="line">2.0.0</span><br></pre></td></tr></table></figure><h2 id="3-4-安装ceph包-在admin-server上执行"><a href="#3-4-安装ceph包-在admin-server上执行" class="headerlink" title="3.4 安装ceph包(在admin server上执行)"></a>3.4 安装ceph包(在admin server上执行)</h2><p>这一步的目标是：admin server通过远程控制在所有server上安装ceph包。它需要在所有server上添加yum源：[Ceph], [Ceph-noarch]和[ceph-source]，见3.1节。</p><p>另外注意：在所有server上安装deltarpm (yum install -y deltarpm)， 否则会报如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[server0][DEBUG ] Delta RPMs disabled because /usr/bin/applydeltarpm not installed.</span><br><span class="line">[server0][WARNIN] No data was received after 300 seconds, disconnecting...</span><br><span class="line">[server0][INFO  ] Running command: ceph --version</span><br><span class="line">[server0][ERROR ] Traceback (most recent call last):</span><br><span class="line">[server0][ERROR ]   File &quot;/usr/lib/python2.7/site-packages/ceph_deploy</span><br></pre></td></tr></table></figure><p>下面就是安装了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy install --release=luminous server0 server1 server2 server3</span><br></pre></td></tr></table></figure><p>成功之后，每台server都安装了ceph包，在任意sever上检查：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># ceph -v</span><br><span class="line">ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable)</span><br><span class="line"></span><br><span class="line"># ceph -v</span><br><span class="line">ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable)</span><br><span class="line">[root@server1 ~]# rpm -qa | grep ceph</span><br><span class="line">ceph-common-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-mds-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-release-1-1.el7.noarch</span><br><span class="line">libcephfs2-12.2.5-0.el7.x86_64</span><br><span class="line">python-cephfs-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-base-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-mon-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-osd-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-mgr-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-radosgw-12.2.5-0.el7.x86_64</span><br><span class="line">ceph-selinux-12.2.5-0.el7.x86_64</span><br></pre></td></tr></table></figure><h1 id="4-部署ceph集群（在admin-server上执行）"><a href="#4-部署ceph集群（在admin-server上执行）" class="headerlink" title="4. 部署ceph集群（在admin server上执行）"></a>4. 部署ceph集群（在admin server上执行）</h1><p>为了演示，我们</p><ol><li>创建一个集群：1 mon + 1 mgr。这个是initial monitor。</li><li>添加 osd</li><li>添加 2 mon + 2 mgr</li><li>创建一个mds</li></ol><p>实际上，我们完全可以在第1步中直接创建 3 mon + 3 mgr的集群 (3个都是initial monitor)，然后添加osd就行了。这里分作1和3两步，是为了演示添加mon和mgr。</p><p>另外，ceph-deploy在部署集群的过程中，会产生一些文件(log，keyring，ceph.conf等)，所以，我们在一个新目录下执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># mkdir test-ceph-deploy</span><br><span class="line"># cd test-ceph-deploy/</span><br></pre></td></tr></table></figure><p>若部署出现错误，需要重头开始：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy purge server0 server1 server2 server3</span><br><span class="line">ceph-deploy purgedata server0 server1 server2 server3</span><br><span class="line">ceph-deploy forgetkeys</span><br><span class="line">rm ceph.*</span><br></pre></td></tr></table></figure><h2 id="4-1-创建集群：1-mon-1-mgr"><a href="#4-1-创建集群：1-mon-1-mgr" class="headerlink" title="4.1 创建集群：1 mon + 1 mgr"></a>4.1 创建集群：1 mon + 1 mgr</h2><p><strong>A. 以server2为initial monitor创建集群</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy new server2</span><br></pre></td></tr></table></figure><p>这里指定server2作为initial monitor。这一步完成之后，在当前目录下会产生如下文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph.conf               </span><br><span class="line">ceph.mon.keyring        </span><br><span class="line">ceph-deploy-ceph.log</span><br></pre></td></tr></table></figure><p>ceph.conf是ceph的配置文件。它将会被分发到所有server的/etc/ceph/目录下。在后续的ceph运维中，若需要做某些配置，可以在所有server上修改/etc/ceph/ceph.conf。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># cat ceph.conf </span><br><span class="line">[global]</span><br><span class="line">fsid = 744f59b7-c403-48e6-a1c6-2c74901a4d0b</span><br><span class="line">mon_initial_members = server2</span><br><span class="line">mon_host = 192.168.100.162</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br></pre></td></tr></table></figure><p>ceph.mon.keyring是monitor的keyring，它定义了monitor的key，以及monitor有什么权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># cat ceph.mon.keyring </span><br><span class="line">[mon.]</span><br><span class="line">key = AQDf7O9aAAAAABAAX4qmBiNsPhvK43wnpNCtLA==</span><br><span class="line">caps mon = allow *</span><br></pre></td></tr></table></figure><p><strong>B. 配置ceph网络</strong></p><p>ceph集群使用两个网络：public network和cluster network。前者用于服务client；后者用于集群内部通信，例如osd之间迁移数据。另外，两个网络上都有heartbeat。</p><p>注意：若只有一个网络，也可以部署ceph。这个网络同时担任public network和cluster network。<strong>这种情况下，跳过本小节</strong>。</p><p>我们有两个网络（见第2.1节），所以在ceph.conf中，增加如下两行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># vim ceph.conf</span><br><span class="line">......</span><br><span class="line">public network  = 192.168.100.0/24</span><br><span class="line">cluster network = 192.168.122.0/24</span><br></pre></td></tr></table></figure><p>注意以下两点：</p><ul><li>在2.3节，我们配置主机名解析的时候，把主机名解析为public  network的地址。这是因为，ceph-deploy是作为client (见下文D小节：client.admin,  client.bootstrap-mds,client.bootstrap-mgr,client.bootstrap-osd,client.bootstrap-rgw)来操作集群的，ceph集群通过public  network服务于client。</li><li>monitor是运行于public network上的。这也很容易理解，ceph的client都需要访问monitor，若monitor运行于cluster network上，client无法访问。</li></ul><p><strong>C. 部署initial monitor</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy mon create server2</span><br></pre></td></tr></table></figure><p>这时候，server2上，monitor已经运行起来了。可以到server2上检查。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 ~]# ps -ef | grep ceph</span><br><span class="line">ceph       18240       1  1 14:24 ?        00:00:00 /usr/bin/ceph-mon -f --cluster ceph --id server2 --setuser ceph --setgroup ceph</span><br></pre></td></tr></table></figure><p>如前文B小节所述，monitor运行于public network之上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 ~]# netstat -anpl | grep 6789 | grep LISTEN</span><br><span class="line">tcp        0      0 192.168.100.162:6789    0.0.0.0:*               LISTEN      18240/ceph-mon</span><br></pre></td></tr></table></figure><p><strong>D. 创建ceph keyring</strong></p><p>经过前一步，server2上的monitor已经运行起来了。但这时候ceph -s失败，因为ceph -s是admin的命令，我们还没有admin的权限信息呢。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> ceph -c ceph.conf -s </span><br><span class="line">2018-05-07 14:25:46.127163 7f76e1834700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory</span><br><span class="line">2018-05-07 14:25:46.127199 7f76e1834700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication</span><br><span class="line">2018-05-07 14:25:46.127201 7f76e1834700  0 librados: client.admin initialization error (2) No such file or directory</span><br></pre></td></tr></table></figure><p>下面使用gatherkeys来创建各个角色（包括admin）的权限信息。gatherkeys 依次对角色 admin,  bootstrap-mds, bootstrap-mgr, bootstrap-osd,  bootstrap-rgw作如下操作（问题：为什么没有bootstrap-rbd？）：</p><ol><li>使用 ceph auth get 来获取角色的key和权限；</li><li>若不存在，则使用auth get-or-create {角色} {权限}来创建角色的key和权限；</li><li>把角色的key保存到 {角色}.keyring文件；</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy gatherkeys server2</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.0): /usr/bin/ceph-deploy gatherkeys server2</span><br><span class="line">......</span><br><span class="line">[server2][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-server2/keyring auth get client.admin</span><br><span class="line">[server2][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-server2/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *</span><br><span class="line">[server2][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-server2/keyring auth get client.bootstrap-mds</span><br><span class="line">[server2][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-server2/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds</span><br><span class="line">[server2][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-server2/keyring auth get client.bootstrap-mgr</span><br><span class="line">[server2][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-server2/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr</span><br><span class="line">[server2][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-server2/keyring auth get client.bootstrap-osd</span><br><span class="line">[server2][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-server2/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd</span><br><span class="line">[server2][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-server2/keyring auth get client.bootstrap-rgw</span><br><span class="line">[server2][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-server2/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mgr.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] keyring &apos;ceph.mon.keyring&apos; already exists</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpCHsfbU</span><br></pre></td></tr></table></figure><p>创建之后，各个角色的key和权限就存在于集群中了。某个角色（例如admin）要对集群的某个组件（例如osd）进行读写操作时，要提供自己的key；集群根据它的key找到它的权限，然后鉴定它是否能够对这个组件进行读写操作。</p><p>上面gatherkeys在生成各个角色的key+权限的同时，把角色的key保存成keyring文件，供各个角色读写集群组件时使用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># ll</span><br><span class="line">total 120</span><br><span class="line">-rw-------. 1 root root    71 May  7 14:28 ceph.bootstrap-mds.keyring</span><br><span class="line">-rw-------. 1 root root    71 May  7 14:28 ceph.bootstrap-mgr.keyring</span><br><span class="line">-rw-------. 1 root root    71 May  7 14:28 ceph.bootstrap-osd.keyring</span><br><span class="line">-rw-------. 1 root root    71 May  7 14:28 ceph.bootstrap-rgw.keyring</span><br><span class="line">-rw-------. 1 root root    63 May  7 14:28 ceph.client.admin.keyring</span><br><span class="line"></span><br><span class="line"># cat ceph.client.admin.keyring </span><br><span class="line">[client.admin]</span><br><span class="line">    key = AQD88e9aAwlyBRAAVwhjmH8GGVmG+JFYs/3MAA==</span><br><span class="line"></span><br><span class="line"># cat ceph.bootstrap-osd.keyring </span><br><span class="line">[client.bootstrap-osd]</span><br><span class="line">    key = AQD+8e9aFC9+LxAApTnB/DImy5ZjoRbQhYoiVA==</span><br></pre></td></tr></table></figure><p>现在就可以执行ceph的admin命令了（admin的key保存在ceph.client.admin.keyring文件里，通过–keyring提供）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># ceph --keyring ceph.client.admin.keyring -c ceph.conf -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     744f59b7-c403-48e6-a1c6-2c74901a4d0b</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum server2</span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 bytes</span><br><span class="line">    usage:   0 kB used, 0 kB / 0 kB avail</span><br><span class="line">    pgs:     </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ceph --keyring ceph.client.admin.keyring -c ceph.conf auth get client.admin</span><br><span class="line">exported keyring for client.admin</span><br><span class="line">[client.admin]</span><br><span class="line">    key = AQD88e9aAwlyBRAAVwhjmH8GGVmG+JFYs/3MAA==</span><br><span class="line">    caps mds = &quot;allow *&quot;</span><br><span class="line">    caps mgr = &quot;allow *&quot;</span><br><span class="line">    caps mon = &quot;allow *&quot;</span><br><span class="line">    caps osd = &quot;allow *&quot;</span><br></pre></td></tr></table></figure><p><strong>E. 分发keyring</strong></p><p>如前所示，我们执行admin的命令，要提供admin的key（–keyring  ceph.client.admin.keyring）以及配置文件(-c  ceph.conf)。在后续的运维中，我们经常需要在某个server上执行admin命令。每次都提供这些参数比较麻烦。实际上，ceph会默认地从/etc/ceph/中找keyring和ceph.conf。因此，我们可以把ceph.client.admin.keyring和ceph.conf放到每个server的/etc/ceph/。ceph-deploy可以帮我做这些：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy admin server0 server1 server2 server3</span><br></pre></td></tr></table></figure><p>检查每个server，发现/etc/ceph/下都多了ceph.client.admin.keyring和ceph.conf这两个文件。现在就不用提供那些参数了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ceph -s</span><br><span class="line"># ceph auth get client.admin</span><br></pre></td></tr></table></figure><p><strong>F. 创建mgr</strong></p><p>从ceph 12（luminous）开始，需要为每个monitor创建一个mgr（其功能待研究，之前的版本都没有这个组件）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy mgr create server2</span><br><span class="line"></span><br><span class="line"># ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     744f59b7-c403-48e6-a1c6-2c74901a4d0b</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum server2</span><br><span class="line">    mgr: server2(active)    ----------------------新加的mgr</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 bytes</span><br><span class="line">    usage:   0 kB used, 0 kB / 0 kB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure><h2 id="4-2-添加OSD"><a href="#4-2-添加OSD" class="headerlink" title="4.2 添加OSD"></a>4.2 添加OSD</h2><p>ceph-deploy osd create通过调用ceph-volume来创建OSD。使用bluestore时(默认)，需要指定3个device：</p><table><thead><tr><th>device</th><th>如何指定</th><th>说明</th></tr></thead><tbody><tr><td>block</td><td>–data</td><td>主要存储，必选。可以是磁盘，分区或者lv</td></tr><tr><td>block.db</td><td>–block-db</td><td>可选。若不指定，则对应内容存储于block。可以是分区或者lv</td></tr><tr><td>block.wal</td><td>–block-wal</td><td>可选。若不指定，则对应内容存储于block。可以是分区或者lv</td></tr></tbody></table><p>注意： </p><ol><li>不可以使用磁盘作为block.db或者block.wal，否则会报错：blkid could not detect a PARTUUID for device； </li><li>若使用磁盘或者分区作block，则ceph-volume会在其上创建lv来使用。若使用分区作block.db或block.wal，则直接使用分区而不创建lv。</li></ol><p>在使用磁盘之前，我们先把磁盘清空。若已经创建了volume group，需要先删掉（vgremove），然后通过ceph-deploy的disk zap进行清空。ceph-deploy disk zap会发生如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy disk zap server0 /dev/vdb</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.0): /usr/bin/ceph-deploy disk zap </span><br><span class="line">[ceph_deploy.osd][DEBUG ] zapping /dev/vdb on server0</span><br><span class="line">......</span><br><span class="line">[server0][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy][ERROR ] Traceback (most recent call last):</span><br><span class="line">[ceph_deploy][ERROR ]   File &quot;/usr/lib/python2.7/site-packages/ceph_deploy/util/decorators.py&quot;, line 69, in newfunc</span><br><span class="line">[ceph_deploy][ERROR ]     return f(*a, **kw)</span><br><span class="line">[ceph_deploy][ERROR ]   File &quot;/usr/lib/python2.7/site-packages/ceph_deploy/cli.py&quot;, line 164, in _main</span><br><span class="line">[ceph_deploy][ERROR ]     return args.func(args)</span><br><span class="line">[ceph_deploy][ERROR ]   File &quot;/usr/lib/python2.7/site-packages/ceph_deploy/osd.py&quot;, line 438, in disk</span><br><span class="line">[ceph_deploy][ERROR ]     disk_zap(args)</span><br></pre></td></tr></table></figure><p>修改ceph-deploy的 osd.py的disk_zap函数，即可成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># vim /usr/lib/python2.7/site-packages/ceph_deploy/osd.py</span><br><span class="line">ceph_volume_executable = system.executable_path(distro.conn, &apos;ceph-volume&apos;)</span><br><span class="line">#if args.debug:</span><br><span class="line">if False:</span><br></pre></td></tr></table></figure><p><strong>A. 添加osd.0（磁盘作block，无block.db，无block.wal）</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy osd create server0 --data /dev/vdb</span><br><span class="line"></span><br><span class="line"># ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     744f59b7-c403-48e6-a1c6-2c74901a4d0b</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum server2</span><br><span class="line">    mgr: server2(active)</span><br><span class="line">    osd: 1 osds: 1 up, 1 in</span><br><span class="line"></span><br><span class="line"># mount | grep ceph</span><br><span class="line">tmpfs on /var/lib/ceph/osd/ceph-0 type tmpfs (rw,relatime,seclabel)</span><br><span class="line"></span><br><span class="line"># ll /var/lib/ceph/osd/ceph-0  </span><br><span class="line">total 48</span><br><span class="line">-rw-r--r--. 1 ceph ceph 189 May  7 15:19 activate.monmap</span><br><span class="line">lrwxrwxrwx. 1 ceph ceph  93 May  7 15:19 block -&gt; /dev/ceph-012c2043-33ef-4219-af69-34c7ed389d41/osd-block-5beb22d5-891c-4d6e-affe-87eb4bc083b2</span><br><span class="line">-rw-r--r--. 1 ceph ceph   2 May  7 15:19 bluefs</span><br><span class="line">-rw-r--r--. 1 ceph ceph  37 May  7 15:19 ceph_fsid</span><br><span class="line">-rw-r--r--. 1 ceph ceph  37 May  7 15:19 fsid</span><br><span class="line">-rw-------. 1 ceph ceph  55 May  7 15:19 keyring</span><br><span class="line">-rw-r--r--. 1 ceph ceph   8 May  7 15:19 kv_backend</span><br><span class="line">-rw-r--r--. 1 ceph ceph  21 May  7 15:19 magic</span><br><span class="line">-rw-r--r--. 1 ceph ceph   4 May  7 15:19 mkfs_done</span><br><span class="line">-rw-r--r--. 1 ceph ceph  41 May  7 15:19 osd_key</span><br><span class="line">-rw-r--r--. 1 ceph ceph   6 May  7 15:19 ready</span><br><span class="line">-rw-r--r--. 1 ceph ceph  10 May  7 15:19 type</span><br><span class="line">-rw-r--r--. 1 ceph ceph   2 May  7 15:19 whoami</span><br></pre></td></tr></table></figure><p>可见： </p><ol><li>使用磁盘vdb创建lv供block使用；  </li><li>osd是mount到tmpfs的（bluefs, ceph_fsid, fsid, keyring等等都存于集群中）；</li></ol><p><strong>B. 添加osd.1（分区作block，分区作block.db，无block.wal）</strong></p><p>把server0的vdc分成两个分区（分区过程省略，注意，要是有gpt分区格式）：vdc1作block.db，vdc2作block。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy osd create server0 --data /dev/vdc2 --block-db /dev/vdc1</span><br><span class="line"></span><br><span class="line"># ll  /var/lib/ceph/osd/ceph-1 </span><br><span class="line">total 52</span><br><span class="line">-rw-r--r--. 1 ceph ceph 189 May  7 15:25 activate.monmap</span><br><span class="line">lrwxrwxrwx. 1 ceph ceph  93 May  7 15:25 block -&gt; /dev/ceph-ae408599-db16-4028-914d-4006594c5cd8/osd-block-1edeced4-e5e9-45ac-a5d3-ddd238d720d4</span><br><span class="line">lrwxrwxrwx. 1 root root   9 May  7 15:25 block.db -&gt; /dev/vdc1</span><br><span class="line">-rw-r--r--. 1 ceph ceph   2 May  7 15:25 bluefs</span><br><span class="line">-rw-r--r--. 1 ceph ceph  37 May  7 15:25 ceph_fsid</span><br><span class="line">-rw-r--r--. 1 ceph ceph  37 May  7 15:25 fsid</span><br><span class="line">-rw-------. 1 ceph ceph  55 May  7 15:25 keyring</span><br><span class="line">-rw-r--r--. 1 ceph ceph   8 May  7 15:25 kv_backend</span><br><span class="line">-rw-r--r--. 1 ceph ceph  21 May  7 15:25 magic</span><br><span class="line">-rw-r--r--. 1 ceph ceph   4 May  7 15:25 mkfs_done</span><br><span class="line">-rw-r--r--. 1 ceph ceph  41 May  7 15:25 osd_key</span><br><span class="line">-rw-r--r--. 1 ceph ceph  10 May  7 15:25 path_block.db</span><br><span class="line">-rw-r--r--. 1 ceph ceph   6 May  7 15:25 ready</span><br><span class="line">-rw-r--r--. 1 ceph ceph  10 May  7 15:25 type</span><br><span class="line">-rw-r--r--. 1 ceph ceph   2 May  7 15:25 whoami</span><br></pre></td></tr></table></figure><p>可见，使用分区vdc2创建lv供block使用； block.db直接使用vdc1;</p><p><strong>C. 添加osd.2（分区作block，分区作block.db，分区作block.wal）</strong></p><p>把serve1的vdb分成3个分区：vdb3作block，vdb2作block.db，vdb1作block-wal：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy osd create server1 --data /dev/vdb3 --block-db /dev/vdb2 --block-wal /dev/vdb1</span><br></pre></td></tr></table></figure><p>到server1上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># ll /var/lib/ceph/osd/ceph-2 </span><br><span class="line">total 56</span><br><span class="line">-rw-r--r--. 1 ceph ceph 189 May  7 15:34 activate.monmap</span><br><span class="line">lrwxrwxrwx. 1 ceph ceph  93 May  7 15:34 block -&gt; /dev/ceph-c2f66dc2-076b-46cd-a1cd-e3ef9511a38a/osd-block-7bf0f953-2feb-4064-8d19-873495cae7f5</span><br><span class="line">lrwxrwxrwx. 1 root root   9 May  7 15:34 block.db -&gt; /dev/vdb2</span><br><span class="line">lrwxrwxrwx. 1 root root   9 May  7 15:34 block.wal -&gt; /dev/vdb1</span><br><span class="line">-rw-r--r--. 1 ceph ceph   2 May  7 15:34 bluefs</span><br><span class="line">-rw-r--r--. 1 ceph ceph  37 May  7 15:34 ceph_fsid</span><br><span class="line">-rw-r--r--. 1 ceph ceph  37 May  7 15:34 fsid</span><br><span class="line">-rw-------. 1 ceph ceph  55 May  7 15:34 keyring</span><br><span class="line">-rw-r--r--. 1 ceph ceph   8 May  7 15:34 kv_backend</span><br><span class="line">-rw-r--r--. 1 ceph ceph  21 May  7 15:34 magic</span><br><span class="line">-rw-r--r--. 1 ceph ceph   4 May  7 15:34 mkfs_done</span><br><span class="line">-rw-r--r--. 1 ceph ceph  41 May  7 15:34 osd_key</span><br><span class="line">-rw-r--r--. 1 ceph ceph  10 May  7 15:34 path_block.db</span><br><span class="line">-rw-r--r--. 1 ceph ceph  10 May  7 15:34 path_block.wal</span><br><span class="line">-rw-r--r--. 1 ceph ceph   6 May  7 15:34 ready</span><br><span class="line">-rw-r--r--. 1 ceph ceph  10 May  7 15:34 type</span><br><span class="line">-rw-r--r--. 1 ceph ceph   2 May  7 15:34 whoami</span><br></pre></td></tr></table></figure><p><strong>D. 添加osd.3（lv作block，lv作block.db，lv作block.wal）</strong></p><p>首先，在server1上，使用vdc创建出3个lv</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># pvcreate /dev/vdc</span><br><span class="line">  Physical volume &quot;/dev/vdc&quot; successfully created</span><br><span class="line"></span><br><span class="line"># vgcreate myvg /dev/vdc   </span><br><span class="line">  Volume group &quot;myvg&quot; successfully created</span><br><span class="line"></span><br><span class="line"># lvcreate -n block-lv -L 30G myvg  </span><br><span class="line">  Logical volume &quot;block-lv&quot; created.</span><br><span class="line"></span><br><span class="line"># lvcreate -n db-lv -L 10G myvg</span><br><span class="line">  Logical volume &quot;db-lv&quot; created.</span><br><span class="line"></span><br><span class="line"># lvcreate -n wal-lv -L 10G myvg</span><br><span class="line">  Logical volume &quot;wal-lv&quot; created.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ls /dev/myvg/</span><br><span class="line">block-lv  db-lv  wal-lv</span><br></pre></td></tr></table></figure><p>然后，在admin server上创建osd.3:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy osd create server1 --data myvg/block-lv --block-db myvg/db-lv  --block-wal myvg/wal-lv</span><br></pre></td></tr></table></figure><p>到server1上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># ll /var/lib/ceph/osd/ceph-3  </span><br><span class="line">total 56</span><br><span class="line">-rw-r--r--. 1 ceph ceph 189 May  7 15:47 activate.monmap</span><br><span class="line">lrwxrwxrwx. 1 ceph ceph  18 May  7 15:47 block -&gt; /dev/myvg/block-lv</span><br><span class="line">lrwxrwxrwx. 1 root root  15 May  7 15:47 block.db -&gt; /dev/myvg/db-lv</span><br><span class="line">lrwxrwxrwx. 1 root root  16 May  7 15:47 block.wal -&gt; /dev/myvg/wal-lv</span><br><span class="line">-rw-r--r--. 1 ceph ceph   2 May  7 15:47 bluefs</span><br><span class="line">-rw-r--r--. 1 ceph ceph  37 May  7 15:47 ceph_fsid</span><br><span class="line">-rw-r--r--. 1 ceph ceph  37 May  7 15:47 fsid</span><br><span class="line">-rw-------. 1 ceph ceph  55 May  7 15:47 keyring</span><br><span class="line">-rw-r--r--. 1 ceph ceph   8 May  7 15:47 kv_backend</span><br><span class="line">-rw-r--r--. 1 ceph ceph  21 May  7 15:47 magic</span><br><span class="line">-rw-r--r--. 1 ceph ceph   4 May  7 15:47 mkfs_done</span><br><span class="line">-rw-r--r--. 1 ceph ceph  41 May  7 15:47 osd_key</span><br><span class="line">-rw-r--r--. 1 ceph ceph  16 May  7 15:47 path_block.db</span><br><span class="line">-rw-r--r--. 1 ceph ceph  17 May  7 15:47 path_block.wal</span><br><span class="line">-rw-r--r--. 1 ceph ceph   6 May  7 15:47 ready</span><br><span class="line">-rw-r--r--. 1 ceph ceph  10 May  7 15:47 type</span><br><span class="line">-rw-r--r--. 1 ceph ceph   2 May  7 15:47 whoami</span><br></pre></td></tr></table></figure><p>注意： lv应写作 myvg/xx-lv，而不是/dev/myvg/xx-lv。否则会报错。</p><p><strong>E. 添加其他osd</strong></p><p>为了方便，block, block.db和block.wal都使用分区。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy osd create server3 --data /dev/vdb3  --block-db /dev/vdb2  --block-wal /dev/vdb1</span><br><span class="line"># ceph-deploy osd create server3 --data /dev/vdc3  --block-db /dev/vdc2  --block-wal /dev/vdc1</span><br><span class="line"># ceph-deploy osd create server2 --data /dev/vdb3  --block-db /dev/vdb2  --block-wal /dev/vdb1</span><br><span class="line"># ceph-deploy osd create server2 --data /dev/vdc3  --block-db /dev/vdc2  --block-wal /dev/vdc1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     744f59b7-c403-48e6-a1c6-2c74901a4d0b</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum server2</span><br><span class="line">    mgr: server2(active)</span><br><span class="line">    osd: 8 osds: 8 up, 8 in    &lt;-------- 8 个 osd</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 bytes</span><br><span class="line">    usage:   8226 MB used, 339 GB / 347 GB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure><h2 id="4-3-添加-2-mon-2-mgr"><a href="#4-3-添加-2-mon-2-mgr" class="headerlink" title="4.3 添加 2 mon + 2 mgr"></a>4.3 添加 2 mon + 2 mgr</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy mon add server0</span><br><span class="line"># ceph-deploy mgr create server0</span><br><span class="line"># ceph-deploy mon add server1</span><br><span class="line"># ceph-deploy mgr create server1</span><br></pre></td></tr></table></figure><p>注意：貌似新版的ceph-deploy一次只能增加一个mon.</p><p>现在集群就有3个mon和3个mgr了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     744f59b7-c403-48e6-a1c6-2c74901a4d0b</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum server0,server1,server2</span><br><span class="line">    mgr: server2(active), standbys: server0, server1</span><br><span class="line">    osd: 8 osds: 8 up, 8 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 bytes</span><br><span class="line">    usage:   8230 MB used, 339 GB / 347 GB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure><h2 id="4-4-创建一个mds"><a href="#4-4-创建一个mds" class="headerlink" title="4.4 创建一个mds"></a>4.4 创建一个mds</h2><p>为了支持cephfs，我们在server2上创建一个mds：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy mds create server2</span><br></pre></td></tr></table></figure><p>成功之后，到server2上可以看见mds进程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 ~]# ps -ef | grep ceph-mds</span><br><span class="line">ceph       19995       1  0 16:35 ?        00:00:00 /usr/bin/ceph-mds -f --cluster ceph --id server2 --setuser ceph --setgroup ceph</span><br></pre></td></tr></table></figure><p>但这个时候，mds并没有active，如下，我们通过ceph -s看不到mds服务。直到创建ceph filesystem的时候，mds才进入active状态（见6.1节）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     744f59b7-c403-48e6-a1c6-2c74901a4d0b</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            too few PGs per OSD (12 &lt; min 30)</span><br><span class="line"></span><br><span class="line">  services:    -----------------&gt; 看不到mds</span><br><span class="line">    mon: 3 daemons, quorum server0,server1,server2</span><br><span class="line">    mgr: server2(active), standbys: server0, server1</span><br><span class="line">    osd: 8 osds: 8 up, 8 in</span><br><span class="line">    rgw: 2 daemons active</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   4 pools, 32 pgs</span><br><span class="line">    objects: 187 objects, 1113 bytes</span><br><span class="line">    usage:   8239 MB used, 339 GB / 347 GB avail</span><br><span class="line">    pgs:     32 active+clean</span><br></pre></td></tr></table></figure><p>至此，ceph集群就完全部署起来了。下面，我们为ceph集群增加一些client。</p><h1 id="5-增加rgw-（在admin-server上操作）"><a href="#5-增加rgw-（在admin-server上操作）" class="headerlink" title="5. 增加rgw （在admin server上操作）"></a>5. 增加rgw （在admin server上操作）</h1><p>我们可以使用ceph集群之外的server来部署rgw。部署之前，需要保证默认端口（7480）没有被防火墙禁止。并且需要安装ceph-radosgw包机器依赖：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy install --rgw &lt;client-node&gt; [&lt;client-node&gt; ...]</span><br></pre></td></tr></table></figure><p>为了方便起见，我们复用集群内的server1和server3来部署rgw。由于ceph-radosgw已经安装（见3.4节），并且防火墙已经被停掉（见2.5节），所以，直接部署即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ceph-deploy rgw create server1 server3</span><br></pre></td></tr></table></figure><p>成功之后，在server1和server3上rgw进程就运行起来了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@server1 ~]# ps -ef | grep ceph</span><br><span class="line">......</span><br><span class="line">ceph       15884       1  2 16:23 ?        00:00:00 /usr/bin/radosgw -f --cluster ceph --name client.rgw.server1 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@server3 ~]# ps -ef | grep ceph</span><br><span class="line">......</span><br><span class="line">ceph       14107       1  2 16:23 ?        00:00:00 /usr/bin/radosgw -f --cluster ceph --name client.rgw.server3 --setuser ceph --setgroup ceph</span><br></pre></td></tr></table></figure><p>并且我们可以通过http来访问：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># curl server1:7480</span><br><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;</span><br><span class="line">&lt;Owner&gt;</span><br><span class="line">    &lt;ID&gt;anonymous&lt;/ID&gt;</span><br><span class="line">    &lt;DisplayName&gt;&lt;/DisplayName&gt;</span><br><span class="line">&lt;/Owner&gt;</span><br><span class="line">&lt;Buckets&gt;&lt;/Buckets&gt;</span><br><span class="line">&lt;/ListAllMyBucketsResult&gt;</span><br></pre></td></tr></table></figure><h1 id="6-增加cephfs"><a href="#6-增加cephfs" class="headerlink" title="6. 增加cephfs"></a>6. 增加cephfs</h1><p>ceph filesystem需要mds（我们在4.4节已经部署）。并且，有两种方式来挂载：ceph fuse和ceph kernel driver。在这一节，我们:</p><ol><li>创建一个ceph filesystem</li><li>通过ceph fuse挂载</li><li>通过ceph kernel driver挂载</li></ol><h2 id="6-1-创建ceph-filesystem（在集群内任意server上）"><a href="#6-1-创建ceph-filesystem（在集群内任意server上）" class="headerlink" title="6.1 创建ceph filesystem（在集群内任意server上）"></a>6.1 创建ceph filesystem（在集群内任意server上）</h2><p><strong>A. 创建所需的pool</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># ceph osd pool create cephfs_data 80</span><br><span class="line">pool &apos;cephfs_data&apos; created</span><br><span class="line"># ceph osd pool create cephfs_metadata 40</span><br><span class="line">pool &apos;cephfs_metadata&apos; created</span><br></pre></td></tr></table></figure><p><strong>B. 创建filesystem</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ceph fs new mycephfs cephfs_metadata cephfs_data</span><br><span class="line">new fs with metadata pool 6 and data pool 5</span><br></pre></td></tr></table></figure><p>如第4.4节所示，在没有创建filesystem之前，mds没有active。现在mds就进入active状态了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     744f59b7-c403-48e6-a1c6-2c74901a4d0b</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum server0,server1,server2</span><br><span class="line">    mgr: server2(active), standbys: server0, server1</span><br><span class="line">    mds: mycephfs-1/1/1 up  &#123;0=server2=up:active&#125;  ---------&gt;mds已经active</span><br><span class="line">    osd: 8 osds: 8 up, 8 in</span><br><span class="line">    rgw: 2 daemons active</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   6 pools, 152 pgs</span><br><span class="line">    objects: 208 objects, 3359 bytes</span><br><span class="line">    usage:   8248 MB used, 339 GB / 347 GB avail</span><br><span class="line">    pgs:     152 active+clean</span><br></pre></td></tr></table></figure><h2 id="6-2-通过ceph-fuse挂载（在server2上）"><a href="#6-2-通过ceph-fuse挂载（在server2上）" class="headerlink" title="6.2 通过ceph fuse挂载（在server2上）"></a>6.2 通过ceph fuse挂载（在server2上）</h2><p>和rgw一样，原则上我们在ceph集群之外的某台server上挂载ceph filesystem。但为了方便起见，我们还是在server2上挂载。</p><p>首先，要在server2上安装ceph-fuse（<strong>若使用ceph集群外的server，也只需这一个包</strong>）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># yum install -y ceph-fuse.x86_64</span><br></pre></td></tr></table></figure><p>然后，创建一个挂载点，就可以挂载了。注意，ceph-fuse挂载使用的是admin的权限，所以，通过-k选项传入admin的key。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># mkdir /mnt/cephfs</span><br><span class="line"></span><br><span class="line"># ceph-fuse -k /etc/ceph/ceph.client.admin.keyring -m server0:6789 /mnt/cephfs</span><br><span class="line">2018-05-07 17:27:07.205147 7f501e11f040 -1 init, newargv = 0x7f502968cb80 newargc=9</span><br><span class="line">ceph-fuse[20080]: starting ceph client</span><br><span class="line">ceph-fuse[20080]: starting fuse</span><br></pre></td></tr></table></figure><p>这时候，一个全新的ceph filesystem就可以使用了。注意：这时cephfs_data是空的，但cephfs_metadata不空：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># ceph df</span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE     AVAIL     RAW USED     %RAW USED </span><br><span class="line">    347G      339G        8248M          2.32 </span><br><span class="line">POOLS:</span><br><span class="line">    NAME                    ID     USED     %USED     MAX AVAIL     OBJECTS </span><br><span class="line">    ......</span><br><span class="line">    cephfs_data             5         0         0          106G           0 </span><br><span class="line">    cephfs_metadata         6      2624         0          106G          21</span><br></pre></td></tr></table></figure><p>往里拷贝一些东西，就会发现cephfs_data也不空了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># cp /boot/vmlinuz-3.10.0-327.el7.x86_64  /mnt/cephfs/</span><br><span class="line"></span><br><span class="line"># ls /mnt/cephfs/</span><br><span class="line">vmlinuz-3.10.0-327.el7.x86_64</span><br><span class="line"></span><br><span class="line"># ceph df</span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE     AVAIL     RAW USED     %RAW USED </span><br><span class="line">    347G      339G        8263M          2.32 </span><br><span class="line">POOLS:</span><br><span class="line">    NAME                    ID     USED      %USED     MAX AVAIL     OBJECTS </span><br><span class="line">    ......</span><br><span class="line">    cephfs_data             5      5035k         0          106G           2 </span><br><span class="line">    cephfs_metadata         6       7541         0          106G          21</span><br></pre></td></tr></table></figure><h2 id="6-3-通过ceph-kernel-driver挂载"><a href="#6-3-通过ceph-kernel-driver挂载" class="headerlink" title="6.3 通过ceph kernel driver挂载"></a>6.3 通过ceph kernel driver挂载</h2><p>首先，我们尝试在server3上测试ceph kernel driver挂载。</p><p>和ceph-fuse一样，通过ceph kernel driver挂载也需要admin的权限。不同的是，不需要admin的keyring文件，而是直接需要admin的key：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@server3 ~]# mkdir /mnt/kcephfs</span><br><span class="line">[root@server3 ~]# mount -t ceph server0:6789:/ /mnt/kcephfs/  -o name=admin,secret=AQD88e9aAwlyBRAAVwhjmH8GGVmG+JFYs/3MAA==</span><br></pre></td></tr></table></figure><p>这个命令卡了一段时间后，报出如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount error 5 = Input/output error</span><br></pre></td></tr></table></figure><p>在/var/log/messages中，有如下错误信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@server3 ~]# tail /var/log/messages</span><br><span class="line">May  7 17:41:29 server3 kernel: libceph: mon0 192.168.100.160:6789 feature set mismatch, my 103b84a842aca &lt; server&apos;s 40103b84a842aca, missing 400000000000000</span><br><span class="line">May  7 17:41:29 server3 kernel: libceph: mon0 192.168.100.160:6789 missing required protocol features</span><br></pre></td></tr></table></figure><p>就是说：ceph集群需要的feature set，我们的ceph kernel driver没能够全部提供，缺失的是400000000000000。</p><p>从<a href="http://cephnotes.ksperis.com/blog/2014/01/21/feature-set-mismatch-error-on-ceph-kernel-client" target="_blank" rel="noopener">CephNotes</a>里，我们可以看到，缺失有些feature，可以通过两种办法解决：</p><ol><li>升级内核  （从客户端入手解决）</li><li>对集群做某些设置 （从server端入手解决）</li></ol><p>例如：</p><ul><li>missing 2040000 (CEPH_FEATURE_CRUSH_TUNABLES 和CEPH_FEATURE_CRUSH_TUNABLES2 )： <pre><code>把客户server（cephfs挂载机）的内核升到3.9（或以上） ； 把tunables设置为legacy : ceph osd crush tunables legacy；</code></pre></li><li>missing 40000000 (CEPH_FEATURE_OSDHASHPSPOOL)： <pre><code>把客户server（rbd客户机？）的内核升到3.9（或以上） ； ceph osd pool set rbd hashpspool false</code></pre></li><li>missing 800000000 (CEPH_FEATURE_OSD_CACHEPOOL)： <pre><code>把客户server的内核升到3.14（或以上） ； 删除cache pool并reload monitors；</code></pre></li></ul><p>悲剧的是，我们缺失的400000000000000 (CEPH_FEATURE_NEW_OSDOPREPLY_ENCODING)，无法通过设置集群来解决，也就是说必须升级内核（到4.5以上）。</p><p>参考：<a href="http://cephnotes.ksperis.com/blog/2014/01/21/feature-set-mismatch-error-on-ceph-kernel-client" target="_blank" rel="noopener">http://cephnotes.ksperis.com/blog/2014/01/21/feature-set-mismatch-error-on-ceph-kernel-client</a></p><p>刚好，我有个开发server（devbuild：192.168.100.150），已经编译安装过内核  4.14.39。去试试。如4.1.B节所述，monitor运行于public  network上（192.168.100.0/24），devbuild能够访问这个网络。这就足够了。<strong>注意：这个server不需安装任何ceph包</strong>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># ping 192.168.100.160</span><br><span class="line">PING 192.168.100.160 (192.168.100.160) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.100.160: icmp_seq=1 ttl=64 time=4.12 ms</span><br><span class="line">64 bytes from 192.168.100.160: icmp_seq=2 ttl=64 time=0.557 ms</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">[root@devbuild ~]# uname -a</span><br><span class="line">Linux devbuild 4.14.39.hyg.20180503  ......</span><br><span class="line"></span><br><span class="line">[root@devbuild ~]# mkdir /mnt/kcephfs</span><br><span class="line"></span><br><span class="line">[root@devbuild ~]# mount -t ceph 192.168.100.162:6789:/ /mnt/kcephfs/  -o name=admin,secret=AQD88e9aAwlyBRAAVwhjmH8GGVmG+JFYs/3MAA==</span><br><span class="line"></span><br><span class="line">[root@devbuild ~]# ls /mnt/kcephfs/</span><br><span class="line">vmlinuz-3.10.0-327.el7.x86_64</span><br></pre></td></tr></table></figure><p>已经mount成功，并且能看到第6.2节拷贝过去的文件。再测试拷贝一个文件，可见读写正常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@devbuild ~]# cp linux-4.14.39.tar.xz /mnt/kcephfs/</span><br><span class="line"></span><br><span class="line">[root@devbuild ~]# ll /mnt/kcephfs/</span><br><span class="line">total 103560</span><br><span class="line">-rw-r--r-- 1 root root 100888428 May  7 18:27 linux-4.14.39.tar.xz</span><br><span class="line">-rwxr-xr-x 1 root root   5156528 May  7 17:30 vmlinuz-3.10.0-327.el7.x86_64</span><br></pre></td></tr></table></figure><p>另外，根据官方文档，命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># mount -t ceph 192.168.100.162:6789:/ /mnt/kcephfs/  -o name=admin,secret=AQD88e9aAwlyBRAAVwhjmH8GGVmG+JFYs/3MAA==</span><br></pre></td></tr></table></figure><p>也可以换做：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># cat admin.secret </span><br><span class="line">AQD88e9aAwlyBRAAVwhjmH8GGVmG+JFYs/3MAA==</span><br><span class="line"></span><br><span class="line"># mount -t ceph 192.168.100.162:6789:/ /mnt/kcephfs/  -o name=admin,secretfile=admin.secret</span><br></pre></td></tr></table></figure><p>但是，我这样尝试，一直报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@devbuild ~]# cat admin.secret</span><br><span class="line">AQD88e9aAwlyBRAAVwhjmH8GGVmG+JFYs/3MAA==</span><br><span class="line">[root@devbuild ~]# mount -t ceph 192.168.100.162:6789:/ /mnt/kcephfs/  -o name=admin,secretfile=admin.secret</span><br><span class="line">mount: wrong fs type, bad option, bad superblock on 192.168.100.162:6789:/,</span><br><span class="line">       missing codepage or helper program, or other error</span><br><span class="line"></span><br><span class="line">       In some cases useful info is found in syslog - try</span><br><span class="line">       dmesg | tail or so.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@devbuild ~]# dmesg | tail   </span><br><span class="line">[   66.850589] random: 7 urandom warning(s) missed due to ratelimiting</span><br><span class="line">[  140.953833] Key type dns_resolver registered</span><br><span class="line">[  141.096210] Key type ceph registered</span><br><span class="line">[  141.097950] libceph: loaded (mon/osd proto 15/24)</span><br><span class="line">[  141.160712] ceph: loaded (mds proto 32)</span><br><span class="line">[  141.163762] libceph: bad option at &apos;secretfile=admin.secret&apos;</span><br></pre></td></tr></table></figure><p>原因是，我的这个server没有安装任何ceph包，所以没有/usr/sbin/mount.ceph这个文件。解决办法：<br> \1. 从别的server拷贝这个文件;<br> \2. 安装ceph-common；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># yum install -y ceph-common-12.2.5-0.el7.x86_64</span><br></pre></td></tr></table></figure><p>前文说过，devbuild server不需安装任何ceph包，但若使用secretfile的方式，还是安装这个ceph-common为好。</p><h1 id="7-增加rbd"><a href="#7-增加rbd" class="headerlink" title="7. 增加rbd"></a>7. 增加rbd</h1><h2 id="7-1-准备rbd-pool"><a href="#7-1-准备rbd-pool" class="headerlink" title="7.1 准备rbd pool"></a>7.1 准备rbd pool</h2><p>在集群内的任意server上创建一个pool，并init：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># ceph osd pool create rbd_pool 100 100</span><br><span class="line">pool &apos;rbd_pool&apos; created</span><br><span class="line"></span><br><span class="line"># rbd pool init rbd_pool</span><br></pre></td></tr></table></figure><h2 id="7-2-创建块设备"><a href="#7-2-创建块设备" class="headerlink" title="7.2 创建块设备"></a>7.2 创建块设备</h2><p>首先我们尝试在集群内的一个server（server0）上创建块设备。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># rbd create foo --size 4096 --image-feature layering -m 192.168.100.160  -K admin.secret -p rbd_pool</span><br><span class="line"># rbd map foo --name client.admin -m 192.168.100.160 -K admin.secret -p rbd_pool</span><br><span class="line"></span><br><span class="line">rbd: sysfs write failed</span><br><span class="line">rbd: error opening default pool &apos;rbd&apos;</span><br><span class="line">Ensure that the default pool has been created or specify an alternate pool name.</span><br><span class="line">In some cases useful info is found in syslog - try &quot;dmesg | tail&quot;.</span><br><span class="line">rbd: map failed: (5) Input/output error</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># dmesg | tail </span><br><span class="line">......</span><br><span class="line">[527394.031762] libceph: mon0 192.168.100.160:6789 feature set mismatch, my 102b84a842a42 &lt; server&apos;s 40102b84a842a42, missing 400000000000000</span><br><span class="line">[527394.034677] libceph: mon0 192.168.100.160:6789 missing required protocol features</span><br></pre></td></tr></table></figure><p>和6.3节遇到的问题一样，内核版本低，缺feature 400000000000000。还是到devbuild这个server上创建吧，要求：</p><ul><li>devbuild server能够访问monitor (public network)；</li><li>安装ceph-common；</li><li>admin.secret；</li></ul><p>如6.3节所述，都已满足。可以创建块设备了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># rbd create foo --size 4096 --image-feature layering -m 192.168.100.160  -K admin.secret -p rbd_pool</span><br><span class="line"></span><br><span class="line"># rbd map foo --name client.admin -m 192.168.100.160  -K admin.secret -p rbd_pool</span><br></pre></td></tr></table></figure><p>选项-K admin.secret也可以换成-k ceph.client.admin.keyring。成功之后：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># ll /dev/rbd0</span><br><span class="line">brw-rw----. 1 root disk 251, 0 May 10 16:31 /dev/rbd0</span><br><span class="line"></span><br><span class="line"># ls /dev/rbd/</span><br><span class="line">rbd_pool</span><br><span class="line"></span><br><span class="line"># ls /dev/rbd/rbd_pool</span><br><span class="line">foo</span><br></pre></td></tr></table></figure><p>这时候，我们可以使用/dev/rbd0了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># mkfs.ext4  /dev/rbd0 </span><br><span class="line"># mount  /dev/rbd0 /mnt/rbd/</span><br></pre></td></tr></table></figure><h1 id="8-小结"><a href="#8-小结" class="headerlink" title="8. 小结"></a>8. 小结</h1><p>本文实践了使用ceph-deploy安装部署ceph集群的过程，给集群添加了三种类型的客户端，并且解决了一些部署中常见的问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-环境&quot;&gt;&lt;a href=&quot;#1-环境&quot; class=&quot;headerlink&quot; title=&quot;1. 环境&quot;&gt;&lt;/a&gt;1. 环境&lt;/h1&gt;&lt;h2 id=&quot;1-1-硬件&quot;&gt;&lt;a href=&quot;#1-1-硬件&quot; class=&quot;headerlink&quot; title=&quot;1.1 硬件&quot;&gt;&lt;/a&gt;1.1 硬件&lt;/h2&gt;&lt;p&gt;4台 Linux虚拟机： server0, server1, server2, server3&lt;br&gt; 每台有两块磁盘 ： /dev/vdb, /dev/vdc&lt;br&gt; 每台有两块网卡 ：eth0, ens9&lt;/p&gt;
&lt;h2 id=&quot;1-2-软件&quot;&gt;&lt;a href=&quot;#1-2-软件&quot; class=&quot;headerlink&quot; title=&quot;1.2 软件&quot;&gt;&lt;/a&gt;1.2 软件&lt;/h2&gt;&lt;p&gt;linux版本： CentOS 7.2.1511&lt;br&gt; 内核版本 ：  3.10.0-327.el7.x86_64&lt;br&gt; ceph版本：  12.2.12&lt;br&gt; ceph-deploy版本： 2.0.0&lt;/p&gt;
    
    </summary>
    
      <category term="Ceph" scheme="http://yoursite.com/categories/Ceph/"/>
    
    
      <category term="ceph" scheme="http://yoursite.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph存储引擎之FileStore</title>
    <link href="http://yoursite.com/2019/05/07/ceph-FileStore%E8%AF%A6%E8%A7%A3/"/>
    <id>http://yoursite.com/2019/05/07/ceph-FileStore详解/</id>
    <published>2019-05-07T09:11:02.000Z</published>
    <updated>2019-05-07T09:25:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>​        Ceph作为一个高可用和强一致性的软件定义存储实现，去使用它非常重要的就是了解其内部的IO路径和存储实现。这篇文章主要介绍在IO路径中最底层的ObjectStore的实现之一FileStore。</p><a id="more"></a><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/图片1.png" alt></p><h2 id="ObjectStore"><a href="#ObjectStore" class="headerlink" title="ObjectStore"></a>ObjectStore</h2><p>​        ObjectStore是Ceph OSD中最重要的概念之一，它封装了所有对底层存储的IO操作。从上图中可以看到所有IO请求在Clieng端发出，在Message层统一解析后会被OSD层分发到各个PG，每个PG都拥有一个队列，一个线程池会对每个队列进行处理。</p><p>​        当一个在PG队列里的IO被提出后，该IO请求会被根据类型和相关附带参数进行处理。如果是读请求会通过ObjectStore提供的API获得相应的内容，如果是写请求也会利用ObjectStore提供的事务API将所有写操作组合成一个原子事务提交给ObjectStore。ObjectStore通过接口对上层提供不同的隔离级别，目前PG层只采用了Serializable级别,保证读写的顺序性。</p><p>​        ObjectStore主要接口分为三部分，第一部分是Object的读写操作，类似于POSIX的部分接口，第二部分是Object的属性(xattr)读写操作，这类操作的特征是kv对并且与某一个Object关联。第三部分是关联Object的kv操作(在Ceph中称为omap)，这个其实与第二部分非常类似，但是在实现上可能会有所变化。</p><p>​        目前ObjectStore的主要实现是FileStore，也就是利用文件系统的POSIX接口实现ObjectStore  API。每个Object在FileStore层会被看成是一个文件，Object的属性(xattr)会利用文件的xattr属性存取，因为有些文件系统(如Ext4)对xattr的长度有限制，因此超出长度的Metadata会被存储在DBObjectMap里。而Object的omap则直接利用DBObjectMap实现。因此，可以看出xattr和omap操作是互通的，在用户角度来说，前者可以看作是受限的长度，后者更宽泛(API没有对这些做出硬性要求)。</p><h2 id="FileJournal"><a href="#FileJournal" class="headerlink" title="FileJournal"></a>FileJournal</h2><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/图片2.png" alt></p><p>为了缩小写事务的处理时间，提高写事务的处理能力并且实现事务的原子性，FileStore引入了FileJournal，所有写事务在被FileJournal处理以后都会立即返回(上图中的第二步)。FileJournal类似于数据库的writeahead日志，使用O_DIRECT和O_DSYNC每次同步写入到journal文件，完成后该事务会被塞到FileStore的op  queue。事务通常有若干个写操作组成，当在中间过程进程crash时，journal会OSD  recover提供了完备的输入。FileStore会存在多个thread从op  queue里获取op，然后真正apply到文件系统上对应的Object(Buffer  IO)。当FileStore将事务落到disk上之后，后续的该Object的读请求才会继续(上图中的第五步)。当FileStore完成一个op后，对应的Journal可以丢弃这部分日志。</p><p>​        实际上并不是所有的文件系统都按照这个顺序，一般来说如Ceph推荐的Ext4和XFS文件系统会先写入Journal，然后再写入Filesystem，而COW(Copy  on Write)文件系统如Btrfs和ZFS会同时写入Journal和FileSystem。</p><h2 id="DBObjectMap"><a href="#DBObjectMap" class="headerlink" title="DBObjectMap"></a>DBObjectMap</h2><p><img src="https://blog-picture-1259089570.cos.ap-guangzhou.myqcloud.com/图片3.png" alt></p><p>​        DBObjectMap是FileStore的一部分，利用KeyValue数据库实现了ObjectStore的第三部分API，DBObjectMap主要复杂在其实现了clone操作的no-copy。因为ObjectStore提供了clone   API，提供对一个Object的完全clone(包括Object的属性和omap)。DBObjectMap对每一个Object有一个Header，每个Object联系的omap(kv   pairs)对会与该Header联系，当clone时，会产生两个新的Header，原来的Header作为这两个新Header的parent，这时候无论是原来的Object还是cloned  Object在查询或者写操作时都会查询parent的情况，并且实现copy-on-write。那么Header如何与omap(kv  pairs)联系呢，首先每个Header有一个唯一的seq，然后所有属于该Header的omap的key里面都会包含该seq，因此，利用KeyValueDB的提供的有序prefix检索来实现对omap的遍历。</p><p>​        上面提到FileStore会将每个Object作为一个文件，那么Object的一些属性会与Object  Name一起作为文件名，Object  所属的PG会作为文件目录，当一个PG内所包含的文件超过一定程度时(在目录内文件太多会造成文件系统的lookup性能损耗)，PG会被分裂成两个PG。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​        Ceph作为一个高可用和强一致性的软件定义存储实现，去使用它非常重要的就是了解其内部的IO路径和存储实现。这篇文章主要介绍在IO路径中最底层的ObjectStore的实现之一FileStore。&lt;/p&gt;
    
    </summary>
    
      <category term="Ceph" scheme="http://yoursite.com/categories/Ceph/"/>
    
    
      <category term="ceph" scheme="http://yoursite.com/tags/ceph/"/>
    
  </entry>
  
</feed>
